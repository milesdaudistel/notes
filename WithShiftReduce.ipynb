{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as wg\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  I currently do not think shift reduce parsing is worth learning for it's own sake.  It's pretty popular to teach in compiler courses, but most actual compilers I know of use recursive descent.  Yes, there are some nice properties of shift reduce parsers, like not having to mangle the grammar, and being faster than recursive descent, but the actual algorithm is much more complicated.  The overall time taken up by parsing is negligable compared to the other portions of a compiler.  So I would suggest skipping over this entirely.\n",
    "\n",
    "`Shift Reduce Parsing`\n",
    "\n",
    "There are a few techniques for parsing, but we're only going to learn about shift reduce parsing since that's the kind of parser that Bison generates.\n",
    "\n",
    "SR parsers look through the tokens 1 at a time.  An SR parser keeps a stack of `items`.  Each item either a terminal (token) or non-terminal.  At each step of parsing, we do 1 of 2 things:  \n",
    "\n",
    "1 We could take a token from the token stream and put it on our item stack.  This is called a `shift`.\n",
    "\n",
    "2 We could use a production rule to combine multiple stack items into a single item.  This is a `reduce`.  Every time we reduce, we add a new node to our parse tree.\n",
    "\n",
    "An SR parser shifts and reduces over and over until it can't shift or reduce anymore.  If there is no more input, and the only item on our stack is the start symbol, we have successfully created a parse tree.\n",
    "\n",
    "Exactly how the SR parser decides whether to shift or reduce will remain mysterious for now.  Just know that sometimes the parser could either shift or reduce, and it always chooses the 'right' one to do.\n",
    "\n",
    "Below is an SR parsing example.  In our example grammar, T and E are non-terminals, and int, *, and + are terminals (tokens).  Our example input to the parser is 'int * int + int'.  The vertical bar character | is used show where we are in our parse.  For instance, 'int * int | + int' means that our parser has looked at the 3 tokens 'int * int' and has put them on the item stack.  The parser has not yet looked at '+ int', and it has not yet reduced anything.  Whenever a shift move occurs, we put another token onto our stack, and move the | over by 1.  When we do a reduce move, some things to the left of the | will be changed, and we won't look at another token of input for that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f7dd8be76a42d2bdd810b880c26f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='slideNum', max=11, min=1), Output()), _dom_classes=('wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def slideshow(slideNum):\n",
    "    return SVG(filename='./SR-Parser/SR-Parser-Page-' + str(slideNum) + '.svg')\n",
    "\n",
    "wg.interact(slideshow, slideNum=wg.IntSlider(min=1,max=11,step=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "Reductions seem backwards.  If Production 1:  T -> int has the arrow going from T to int, why are we turning ints into Ts?\n",
    "</summary>\n",
    "\n",
    "To answer this question, I have to explain some history.  The first parsers did not use the shift reduce algorithm.  They instead used `recursive descent`.  Rather than taking tokens and combining them into non-terminals, they started at the start symbol non-terminal, and broke up the start symbol into its child terminals and non-terminals.  For example, it would start at E, and break that up into T using production 4.  Sometimes (most of the time) it would choose the wrong production.  So it would have to undo all of its work and break E up into T + E using production 5.\n",
    "\n",
    "Since recursive descent parsers came first, backus nar form has the arrows going from non-terminals to the parts that you break it up into.\n",
    "\n",
    "Recursive descent parsing is slow.  It can be sped up by using special kinds of CFGs, called LL(k) grammars.  If you use an LL(k) grammar, recursive descent is as fast as shift reduce.  However, LL(k) grammars are annoying to create.\n",
    "\n",
    "Interestingly, GCC and Clang (the two most popular C/C++ compilers) both use hand-coded recursive descent compilers.  They don't use bison, and they don't use shift reduce.  Supposedly it's easier to maintain your code if you do everything by hand.  But there are other languages like Ruby that do use parser generators like Bison.  Right now, I would recommend using a parser generator like Bison.  It's much less work.  C/C++ have to use hand-written because they're bloated from years of committees of people piling on features that were always conflicting with eachother.  It's a giant mess. \n",
    "</details>\n",
    "\n",
    "<details><summary>Why does the parse tree end with a bunch of non-terminal symbols on it?  I thought it was supposed to only have terminal symbols.</summary>\n",
    "You are correct.  After we create the parse tree, we will remove all of the nonterminal symbols.  Exactly how we do this will come after we learn how the parser decides whether to shift or reduce. \n",
    "</details>\n",
    "\n",
    "\n",
    "So how does the parser know which production to use?  Well, you know to use a production if that production can be traced back to the start symbol.\n",
    "\n",
    "The whole parser is a giant graph of shifts/reduces that can be traced back to the start symbol.  Here's a parser that can parse our E, T grammar:\n",
    "\n",
    "<img src=\"./ShiftReduceStepByStep/ShiftReduceStepByStep-Page-35.svg\" />\n",
    "\n",
    "Here's how the graph works:\n",
    "\n",
    "Remember that our parser works in steps.  At each step, the parser decides whether to shift or reduce.  It decides by looking through the stack, bottom to top.  At each item in the stack, it makes a transition in the graph.\n",
    "\n",
    "Before explaining each part of the graph, let's first go through an example.\n",
    "\n",
    "Let's say we're parsing `int * int + int`, and the parse stack is `int * | int + int`, so 'int' and '*' are on the stack.  Now the parser decides what's next:  shift the next 'int', or reduce something on the stack?\n",
    "\n",
    "The parser looks at its graph, always starting in the start state.  Then it looks at the bottom of the stack, sees an 'int', and travels to state 4.  Then it sees a '*' and travels to state 8.  Now that the parser has gone through the entire stack, the final state is 8, and the shift/reduce decision is based on state 8.\n",
    "\n",
    "Each state has a bunch of `items` in it.  An item looks like a production with a . somewhere on its right hand side.  An item says 'This is a potential production that we could reduce by.  This dot tells us how much of the production we have seen so far.'  \n",
    "\n",
    "So in the start state, the item S\\` -> .E says:  We haven't seen anything yet, because there's nothing to the left of the '.'.  If the next item on the stack is an E, we can transform that E into an S\\`.\n",
    "\n",
    "An item says to shift if there's at least 1 thing to the right of its '.'.  If there is nothing to the right of the '.', then the item says to reduce.\n",
    "\n",
    "If the parser ended in state 10, it would decide to shift.  If the parser ended in state 11, it would decide to reduce by the production `T->(E)`.\n",
    "\n",
    "But most of these states have multiple items; how do we decide for those states?  First, notice that all the items are shift items for most of these states, like State 1, State 5, and State 6.  They all tell you to shift, and unlike reducing, there's only 1 way to shift an item.\n",
    "\n",
    "Here's how we actually made this graph:\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584f8aa6f101413287199937d306376b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='slideNum', max=35, min=1), Output()), _dom_classes=('wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def slideshow(slideNum):\n",
    "    return SVG(filename='./ShiftReduceStepByStep/ShiftReduceStepByStep-Page-' + str(slideNum) + '.svg')\n",
    "\n",
    "wg.interact(slideshow, slideNum=wg.IntSlider(min=1,max=35,step=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a problem.  States 3 and 4 both have 1 shift item and 1 reduce item.  This is a `shift/reduce conflict`.  As our graph is now, the parser can't decide whether to shift or reduce.  We can alleviate this problem by using follow sets, which I don't feel like explaining properly.  Basically, we just peek at the next thing on the stack without putting it on the stack.  If the next thing in the input is a '+', we shift.  If it's not a '+', we reduce.  There are also `reduce/reduce conflicts` which are also solved with follow sets.\n",
    "\n",
    "There's also another problem:  the runtime of this algorithm.  Our parser starts at the start state at every shift/reduce decision.  We can improve this by putting things on the stack along with the state that we finished on.  So instead of `int * | int + int`, we would have `(int 4) (* 8) | int + int`.  The things on the stack say 'we shifted an int because state 4 told us to, and \\* because state 8 told us to'.  Now at the next shift/reduce decision, our parser looks at the thing at the top of the stack, (* 8), and starts in state 8.\n",
    "\n",
    "\n",
    "\n",
    "<details><summary>How do you tell where shift/reduce and reduce/reduce conflicts are just by looking at some productions?  How do you tell when a production is ambiguous?  What is the difference between a shift|reduce/reduce conflict and an ambiguity?</summary>\n",
    "\n",
    "Conflicts are a specific kind of easily resolvable ambiguity.  A conflict is where the compiler says 'I have these 2 paths and don't know which one to take' and you explicitly tell it 'just take this one'.  This 'resolves' the ambiguity without actually making the grammar unambiguous.  It's a cheat rule, a patch, however you want to think of it.\n",
    "\n",
    "Shift reduce conflicts are things like dangling elses.  They're pretty normal, and easy to resolve.  Reduce/reduce conflicts mean there is probably something wrong with your grammar.  Here's a reduce/reduce conflict in Bison:\n",
    "\n",
    "sequence:\n",
    "  %empty         { printf (\"empty sequence\\n\"); }\n",
    "| maybeword\n",
    "| sequence word  { printf (\"added word %s\\n\", $2); }\n",
    ";\n",
    "\n",
    "maybeword:\n",
    "  %empty    { printf (\"empty maybeword\\n\"); }\n",
    "| word      { printf (\"single word %s\\n\", $1); }\n",
    ";\n",
    "\n",
    "If you get a word, do you reduce by maybeword's production 2, or sequence's production 3?  A reduce/reduce conflict means that there is some redundancy in your grammar.\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If doing the whole follow set thing doesn't work, then your grammar isn't `SLR`.  Also LALR.  Also ambiguity.\n",
    "If you make sure every element in the follow set of every thing corresponds to a unique production, you might not have to worry about LALR, since I think having this 'unique element' thing would make your grammar SLR.\n",
    "\n",
    "You also need to explain how this gets turned into that really complicated table.\n",
    "\n",
    "Also instead of S\\` you should just have S, which goes to E followed by dollar sign (end of input).  I don't know why Aiken wasn't explicit on that part.\n",
    "\n",
    "Need to explain how epsilon productions work in your DFA:  remember that its an epsilon production, not an epsilon transition.  So you know to make the transitions based on what is immediately to the right of the '.'.  But For epsilon transitions, you make transitions for everything in their follow set.\n",
    "\n",
    "#Misc\n",
    "\n",
    "http://trevorjim.com/python-is-not-context-free/\n",
    "\n",
    "<details><summary>TODO</summary>\n",
    "Figure out how to concisely explain epsilon, and where.  Probably in the lexer phase.\n",
    "\n",
    "put your goats mustache example up at the very top.  Then at each stage, use that example to give them a sense of what this stage of the compiler does.  Also add these in:\n",
    "optimization:  this stage is optional.  Makes code easier for computer to execute.  'I must proceed at high velocity' becomes 'Gotta go fast'.\n",
    "code generation:  'Gotta go fast' becomes '010100100101001010010101001' (but make it accurate).  \n",
    "\n",
    "Here are some things context free grammars can't do.  If a grammar is like this, it's context sensitive.\n",
    "But we can just do some quick hacks like this and pretend that our grammar is still context free.\n",
    "It's important to keep our grammars as close to context free as we can, since they can be parsed in O(n) time.  Context sensitive takes O(n^2) time.  So technically the grammars we make here take O(n^2) time, but just for a few operations, so in practice our parser and lexer will seem like it only takes O(n) time.\n",
    "\n",
    "I feel like if you explain simply why LL(k) grammars don't matter, and more generally why you only ever need 1 token of lookahead, then all the rest of this stuff should fall into place.  Just simply explain why 1 token of lookahead should be all that is necessary.  Then, once you understand that only 1 token of lookahead is ever needed, explain why that's not entirely true, and that you USUALLY only ever need 1 token of lookahead.  GLR parsers USUALLY only look ahead 1 token.  And cut down on the definitions.\n",
    "Once you compare all of these definitions and really hone in on what this stuff is, you should be able to figure out the perfect tools and implementation for your grammar.\n",
    "\n",
    "Consider trying to figure out some kind of step-by-step for certain things.  Like examples.  Consider the follow set example.\n",
    "\n",
    "Oh, the first and follow set examples:  maybe give them the way to do it by hand?  No need to show them the computer code to do it.\n",
    "\n",
    "Shift reduce parsing takes n time without needing to do all that grammar shifting around that recursive descent needs.\n",
    "\n",
    "https://stackoverflow.com/questions/3922699/what-is-the-runtime-difference-between-different-parsing-algorithms\n",
    "\n",
    "Above link is interesting.  Very strange as well.  But ugh.  There's just so much.  I feel like conceptual simplicity matters the most.  Which means you should use a lexer and parser generator.  Bison uses shift reduce, so you should learn shift reduce.\n",
    "\n",
    "Is there a specific name for the kinds of things a regular grammar can't accomplish?  Like it can't accomplish infinitely recursive structures, or it can't do infinite nesting, or it can't do uncountably infinite sets, or it can't do non-modulo sets, or something.  Just give a name to it.  As of now, it's hard to figure out whether you can write something as a regular expression or not.  You have to use the pumping lemma, I think.  Maybe explain that simply.  Would actually be useful.  'Can I write this as a regular expression'?  If yes, fantastic, an easy win.  If not, oh well, at least you didn't waste a bunch of time trying to think of a way to do it.\n",
    "\n",
    "https://cs.stackexchange.com/questions/51189/ambiguity-vs-context-sensitivity\n",
    "https://en.wikipedia.org/wiki/Ambiguous_grammar\n",
    "https://stackoverflow.com/questions/14589346/is-c-context-free-or-context-sensitive \n",
    "\n",
    "Also want to ask:  'why is top down parser easier to debug?'  reference the above stack exchange thing.  Both top down and bottom up are written using backus nar form.\n",
    "\n",
    "look at python's full grammar specification.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
