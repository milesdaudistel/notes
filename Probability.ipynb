{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "`counting` as in counting cards, is the most basic kind of probability.  The kind of questions that counting answers are like this:  You have a set S, and you pick elements from that set for your new set T. How many different T's could you make?  \n",
    "\n",
    "There are 2 questions you should ask that determine how you solve this problem:\n",
    "\n",
    "Does `order` matter? Is T = {x, y} the same as T = {y, x}?  If the answer is yes, then order does not matter.  If the answer is no, then order matters.\n",
    "\n",
    "Is there `replacement`?  Lets say S = {x, y, z} and you want to choose 2 elements for T.  If you choose x, does S = {y, z} now, or is it still S = {x, y, z}?  If x is gone from S, then there is no replacement.  If x is still in S, then there is replacement.  If there is replacement, then you could have T = {x, x}.  If there is no replacement, then it would be impossible for you to get T = {x, x}.\n",
    "\n",
    "Let's say we have S = {a, b, c, d, e, f, g, h, i, j}, which is 10 items, and we want to find different subsets of 5.\n",
    "\n",
    "`n` is the number of items we can choose from\n",
    "\n",
    "`k` is the number of choices we get to make.\n",
    "\n",
    "`With replacememnt, order matters`\n",
    "\n",
    "Example:  You roll a die 5 times.  How many sequences of rolls could you get?\n",
    "\n",
    "Answer:  6\\*6\\*6\\*6\\*6 = 6^5\n",
    "\n",
    "On each roll, there are 6 possible outcomes.  1 followed by 2 is different from 2 followed by 1, since order matters.\n",
    "\n",
    "General formula:  n^k\n",
    "\n",
    "`Without replacement, order matters`\n",
    "\n",
    "Example:  How many ways could 20 cards be stacked up from a deck of 100 unique cards?\n",
    "\n",
    "Answer:  100\\*99\\*98\\*...\\*81 = 100! / 80!\n",
    "\n",
    "Since each card is unique, there is no replacement.  A stack is ordered, so order matters.  We have 100 choices for our first card, 99 choices for our second card, etc.\n",
    "\n",
    "General formula:  n! / (n-k)!\n",
    "\n",
    "`Without replacement, order doesn't matter`\n",
    "\n",
    "Example:  How many 20 card hands could you make from a deck of 100 unique cards?\n",
    "\n",
    "Answer:  100! / 80! / 20!\n",
    "\n",
    "First we do the calculation as if order did matter.  Now consider how many different stacks of cards you could make with a single hand of 20 cards.  The answer to that is 20!.  So for every 20! stacks of cards, there is 1 hand of cards.  So you take 100! / 80! and divide it the whole thing by 20! to convert from ordered stacks of cards to unordered hands of cards.\n",
    "\n",
    "General formula:  n! / ((n-k)!k!)\n",
    "\n",
    "This formula is so common it has a name:  choose, or `n choose k`, and it often abbreviated as $n \\choose k$.\n",
    "\n",
    "`With replacement, order doesn't matter` Warning, this one is weird.\n",
    "\n",
    "Example:  How many ways can you put 5 identical balls into 10 labeled bins?\n",
    "\n",
    "Answer:  $14 \\choose 5$\n",
    "\n",
    "First, we are choosing bins, not balls.  If that's weird, maybe think of the bin as being 'reused' instead of 'replaced'.  We can represent this problem with a binary string.  0's are balls, and 1's are dividers between bins.  So if there are 2 bins, there's 1 divider.  Since there are 10 bins, there are 9 dividers.  Here's an example ordering:  01101011100111.  There's 1 ball in the first bin, third bin, fourth bin, 2 balls in the seventh bin, and 0 balls in all the other bins.  So there are 10 - 1 + 5 digits, and we are choosing 5 of them to be 0's and the rest to be 1's.  So we end up with $14 \\choose 5$.  We could also equivalently say that we are choosing 9 of them to be 1's and the rest to be 0's.\n",
    "\n",
    "<details><summary>Prove it</summary>\n",
    "    \n",
    "$$\n",
    "\\binom{n}{k}\n",
    "\n",
    "= \\frac{n!}{(n-k)!k!}\n",
    "$$\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "$$\n",
    "\\binom{n}{n-k}\n",
    "\n",
    "= \\frac{n!}{(n-(n-k))!(n-k)!}\n",
    "\n",
    "= \\frac{n!}{k!(n-k)!}\n",
    "\n",
    "= \\frac{n!}{(n-k)!k!}\n",
    "\n",
    "$$\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/presentation/d/1Y7sdF-q27CjbGO58gpYcYS9Fgz3PSkZJcXDyOuJqiTQ/edit#slide=id.g407c66229f_1_274\n",
    "\n",
    "Compare this with what is said by the 'law of large numbers'.  I think it was something like 'if we know the distribution, then we can predict with X accuracy'.  The thing is that you dont actually know the distribution.  Like, pretty much ever.  Because you're a human, and you're bad at collecting data.\n",
    "\n",
    "Oh, if the link is broken, just look for 'Literary Digest 1936 election poll failing'.  Even though Digest got it right for a number of years beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes Nets**\n",
    "\n",
    "Reviewing these because Sequioua asked for help and it's not very often that you can provide assistance to someone.\n",
    "\n",
    "Bayes nets are for when we don't have perfect information about the world.  They let us calculate the probabilities of certain events happening.\n",
    "\n",
    "Events are connected and potentially dependent on each other in some way.  This is where the net comes in.  An arrow means that 2 events _might_ be dependent on each other.\n",
    "\n",
    "Inference by enumeration: We have a giant table of probabilities.\n",
    "Say we want P(W=sun|S=winter).\n",
    "We select the rows where S=winter.\n",
    "Say S=winter is a quarter of them, so P(S=winter)=25%\n",
    "Now the entries in our table only account for 25% of the what season it might be.\n",
    "But since we _know_ S=winter, the probability is 1, and we have to renormalize.\n",
    "Renormalizing means making the total probability of this new table 1 again.\n",
    "To do that, just sum the probabilities of each table entry, then divide each table entry by that sum.\n",
    "Now you have a new table whose name is P(W|S=winter).\n",
    "This name means 'we know the season is winter.  Put in a query for the weather W to find the probability of that weather given that the season is winter.'\n",
    "Then you sum all the P(W=sun) rows to get the total probability of P(W=sun|S=winter).\n",
    "\n",
    "So if you have a table of probabilities, you want to compute P(Q_1, ...., Q_n|e_1, ...., e_n).\n",
    "\n",
    "Each e_i is 'evidence'.  It's a random variable that we already know the assignment of, like S=winter.\n",
    "\n",
    "Each Q_i is a query variable that we don't know.  A Q_i might be W, or it might be W=sun.\n",
    "\n",
    "W means 'give me a table with all the possibilities for W'.\n",
    "\n",
    "W=sun means 'give me a table where W=sun specifically.'\n",
    "\n",
    "these are the general steps on how you compute P:\n",
    "Step 1:  select all the rows where each of the e_i's are true and put them in a new table.\n",
    "Step 2:\n",
    "for each row:\n",
    "    P(row) = P(row) / sum(all P(row)'s from 1 to n)\n",
    "Step 3:\n",
    "Select all the rows where the Q_i's have been assigned.  Do not repeat step 2.\n",
    "Step 4:\n",
    "Now for the unassigned Q_i's, assign them to whatever it is that you want to query.\n",
    "\n",
    "What's the difference between assigning Q_i vs an e_i?  The difference is just like P(X=x, Y=y|Z=z) vs P(X=x|Y=y, Z=z).  In the first case, Y is a Q_i, in the second, it's an e_i.  The first is saying 'what is the probability that X is x and Y is y given that Z is z'.  The second is saying 'what is the probability that X is x given Y is y and Z is z.'\n",
    "\n",
    "The problem is that these tables can be really big.  If each row is size d, and we have n rows, then that's d^n entries.  Too many.  A bayes net will be less space complexity.\n",
    "\n",
    "So we have the events season, temperature, and weather.\n",
    "\n",
    "We have 4 seasons, spring, summer, fall, winter.  We have, I don't know, 100 temperatures in fahrenheit, from 1 to 100.  Obviously we've had 0 or below or above 100, but lets keep this easy.  We also have 2 weathers, sun and rain.  Also snow doesn't exist, because God is punishing us.  If we wanted to calculate the joint probability table, we would need all of the following:\n",
    "\n",
    "P(Season=spring, temperature=100, weather=sun)\n",
    "P(spring, 100, rain)\n",
    "P(spring, 99, sun)\n",
    "P(spring, 99, rain)\n",
    "....\n",
    "P(winter, 2, sun)\n",
    "P(winter, 2, rain)\n",
    "P(winter, 1, sun)\n",
    "P(winter, 1, rain)\n",
    "\n",
    "Altogether, there will be 4\\*100\\*2=800 table entries.\n",
    "\n",
    "Now lets think of the general case.  We have n variables (columns), and each variable can have d values, then we will have d^n entries (rows), which is a ton, right?  In our specific case, the variables have different domains, so the formula is instead product(d_i) for all i.\n",
    "\n",
    "A bayes net combines tables with DAGs.  Instead of having 1 table that calculates all possibilities for P(Season, Temperature, Weather), we have a bunch of tables of conditional probabilities.  These conditional tables will be much smaller than the joint probability table, and will decrease our space complexity.\n",
    "\n",
    "We know that there is some dependence relationship between season, temp, and weather.  We would probably guess that temp and weather are dependent on season, not the other way around.  Maybe weather might also be directly dependent on temp, or temp on weather, but lets keep this simple for now.  So temperature and weather do not have an edge between them.  Season influences weather, and season influences temperature.\n",
    "\n",
    "So we have this DAG:\n",
    "season---->temperature\n",
    "       |\n",
    "       |-->weather\n",
    "       \n",
    "\n",
    "If we know the season, temp and weather are independent from each other.\n",
    "\n",
    "If we do not know the season, temp and weather are dependent on each other.  Well, truthfully, they _might_ be (and intuitively they should be) dependent on each other, but with bayes nets you can never guarentee dependence.\n",
    "\n",
    "Lets say we know temperature, but not season or weather.  Knowing the temperature means we can make a more educated guess about the season.  Having a more educated guess about the season means we can make a more educated guess about the weather.  So knowing the temperature means we know more about the weather.  So temperature and weather are dependent on each other.\n",
    "\n",
    "Now here's a second scenario.  Let's say we already know the season, but not the temperature or the weather.  Now we learn what the temperature is.  The temperature will let us make a more educated guess about the season .... but we already know exactly what the season is, so using temperature to guess the season is pointless.  Since we already know the season in this case, learning about the temperature won't help us make a better guess about the weather.  So temperature and weather are dependent on each other.\n",
    "\n",
    "This relationship between season, temperature, and weather is a 'common cause' relationship.  A similar line of thought can be applied to understand the other relationships: common effect and causal chain. \n",
    "\n",
    "So we just need a P(Season) table, a P(Temp|Season) table, and a P(Weather|Season) table.\n",
    "\n",
    "The P(Season) table has 4 entries.\n",
    "\n",
    "The P(Temp|Season) table has 400 entries.\n",
    "\n",
    "The P(Weather|Season) table has 8 entries.\n",
    "\n",
    "So these 3 tables have 412 entries, rather than 800 entries, not to mention the entries are smaller than in the joint probability table.  Much better in terms of space.\n",
    "\n",
    "With these 3 tables, you can calculate any query.  Here are some examples:\n",
    "\n",
    "P(T, W|S) = P(T|S)P(W|S), since T and W are independent given S.\n",
    "\n",
    "P(S|W) = P(W|S)P(S)/P(W)\n",
    "\n",
    "P(T) = sum(P(T|S=season) for all seasons)\n",
    "\n",
    "Gotcha.  So I'm looking at note 6, under Bayes Net (Inference).\n",
    "\n",
    "So you have these probability tables:\n",
    "P(T), P(C|T), P(S|T), P(E|C,S)\n",
    "\n",
    "You want the table P(T|+e)\n",
    "\n",
    "Since all you need is T and +e, all other variables are extraneous information.  So C and S are not needed, and neither is anyting in E that is not +e.  \n",
    "\n",
    "One way to do this is that you could get P(T|+e) by combining all the information you have into a single table P(T, C, S, E).  Then you would sum over the C column.  Then sum over the S column.  Then drop all rows whose E column value isn't +e, and normalize to get P(T|+e).  However, this is inefficient because the P(T, C, S, E) table is huge.  A better way is to eliminate as soon as possible so that you aren't doing calculations on variables that you don't need to be doing.\n",
    "\n",
    "Here's the 'variable elimination' way to do it.  The first thing we do is find the tables that contain E, and eliminate all rows that are not +e, then normalize the new probability table.  So P(E|C,S) will become P(+e|C,S).  Now you won't be doing any calculations on rows that involve -e, which are useless.\n",
    "\n",
    "Then we want to eliminate C or S.  We'll do C first.  We combine all the tables that involve C in any way.  Our unified table of C will be P(C,+e|T,S) = P(C|T)Â·P(+e|C,S).  Now we can sum out C to get P(+e|T,S).  \n",
    "\n",
    "Aaaand i'm bored.  Hopefully the stuff from before will be enough.\n",
    "\n",
    "\n",
    "Side note, FPGAs are currently being used for this.\n",
    "https://www.nextplatform.com/2018/08/27/xilinx-unveils-xdnn-fpga-architecture-for-ai-inference/\n",
    "Not entirely practical yet, but has a lot of potential.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
