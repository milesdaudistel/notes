{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def graph(lambda_arr, xmin, xmax):\n",
    "    xplot = range(xmin, xmax+1)\n",
    "    ymin = 0\n",
    "    ymax = 0\n",
    "    for f in lambda_arr:\n",
    "        yplot = []\n",
    "        for x in xplot:\n",
    "            y = f(x)\n",
    "            if y < ymin:\n",
    "                ymin = y\n",
    "            if y > ymax:\n",
    "                ymax = y\n",
    "            yplot.append(f(x))\n",
    "        plt.plot(xplot, yplot)\n",
    "    plt.axis([xmin, xmax, ymin, ymax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear algebra\n",
    "\n",
    "**What is linear algebra for?**\n",
    "\n",
    "Solving systems of linear equations.  Seeing what values can satisfy the constraints of these equations.  Now when you 'solve' a system of equations, you get the answers for each variable.  \n",
    "\n",
    "One way you could think about it is solving for a bunch of individual values at once.\n",
    "But it's more than that.\n",
    "Rather than thinking of x1, x2, x3 as being individual values that you're solving for at the same time, think of it as solving for a single answer, where that answers components are x1, x2, x3, etc.  Syntactically, a vector is just a list of numbers.  But semantically, that list of numbers comes together to represent the whole, something greater than its parts.  Think of 5d apple space.  What kinds of different apples are there?  You want to get information on all of these apples.  You aren't trying to get just the vitamin C content, just the calorie content, or just the size, etc.  You want to mathematically describe the object that is an apple, which can't be done with just a single number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Equation**\n",
    "\n",
    "A linear equation means you have a bunch of variables, $x_1$, $x_2$, $x_3$, etc, and they are all clumped together in 1 equation.\n",
    "\n",
    "$ax_1 + bx_2+cx_3+...=y$\n",
    "\n",
    "If $x_1$ gets increased by 1, the other x's will also be increased.  But by how much?\n",
    "\n",
    "It depends on how many other variables there are.  If there's 1 other variable, it's a line, and we know.  If there's 2 or more, then the equation decribes a plane or something else, and we don't know.  We need more than 1 equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Systems of linear equations**\n",
    "\n",
    "A bunch of linear equations.  You want the set of variable values that make all the equations true simultaneously.\n",
    "\n",
    "If you have 2 lines in 2d space, like so:\n",
    "\n",
    "$$\n",
    "x + y = 2 \\\\\n",
    "2x + y = 1\n",
    "$$\n",
    "\n",
    "The solution is (x, y) = (-1, 3).\n",
    "\n",
    "Visually, the equations are 2 lines.\n",
    "\n",
    "If the lines are parallel, there are 0 solutions.\n",
    "\n",
    "If the lines cross, there is 1 solution.\n",
    "\n",
    "If the lines are actually the same line, there are infinite solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Free variables, etc TODO**\n",
    "\n",
    "if x+y+z=d, and we know d, then we have a free variable.  this means we could write the equation as z = F(x, y), y=F(x, z), x=F(y, z), or whatever.\n",
    "\n",
    "A linear equation relates all the variables to a constant.  This is the same as saying \n",
    "\n",
    "If you're in N-space, a single linear equation makes a shape of dimensions N-1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix**\n",
    "\n",
    "How do you turn x+y+z=d into a row in a matrix?\n",
    "\n",
    "Where does the d go?\n",
    "\n",
    "A function is a transformation on points.\n",
    "\n",
    "An equation is a set of points.\n",
    "\n",
    "Perhaps a function is also an equation?\n",
    "\n",
    "Just replace the function notation with another variable.\n",
    "\n",
    "What does it look like with multiple free variables?\n",
    "\n",
    "x1+x2+x3=1\n",
    "\n",
    "How do you write this in function notation?\n",
    "\n",
    "x3 = 1 - x1 - x2\n",
    "\n",
    "F(x1, x2) = 1 - x1 - x2\n",
    "\n",
    "Is there a difference between rows and columns?\n",
    "\n",
    "If it was just columns to columns, then matrix matrix multiplication would be way easier.\n",
    "\n",
    "Matrix multiplication is function stuff, not actual multiplication.\n",
    "\n",
    "All the functions you've seen so far are basically 1d to 1d vector functions.\n",
    "\n",
    "If you've seen multivariable functions, that's Nd to 1d.\n",
    "\n",
    "For each equation, you have a bunch of input variables, and 1 output variable.\n",
    "\n",
    "The 1 output variable is the dependent variable.\n",
    "\n",
    "Now what if we wanted a function that had multiple output variables?\n",
    "What would that even mean?\n",
    "\n",
    "It would really just be 2 functions.\n",
    "F(w, x) = 3w+4x\n",
    "G(w, x) = 2w+3x\n",
    "\n",
    "Here, F and G are basically y and z.  We have 4 variables.  2 of the variables are dependent.\n",
    "\n",
    "Before, we had 1 or 2 input variables.  We basically took those variables, transformed them into some other thing, and plotted the untransformed vs the transformed variables.  Here, the input and output aren't going to be graphed at the same time.\n",
    "\n",
    "A data point _is_ a vector,  a vector _is_ a function, and vice versa.\n",
    "\n",
    "What happens when you use a data point as a function on an other data point?\n",
    "\n",
    "A data point is an N to 1 function.\n",
    "\n",
    "Wait, no.  A data point can be considered an N to 1 function.  But it means more than that.\n",
    "\n",
    "It's an item.\n",
    "\n",
    "All functions take in N points and give 1 point.\n",
    "\n",
    "If you want a function that takes in N points and gives back P points, you need P functions.\n",
    "\n",
    "A matrix of P functions.\n",
    "\n",
    "Is there ever a difference between a function and a data point?\n",
    "Yeah, definitely.\n",
    "Syntactically they're identical, but semantically they can mean different things.\n",
    "\n",
    "Maybe go from 2d to 3d. on your plane.\n",
    "\n",
    "Can you specify any hyperplane with a vector perpendicular to it?\n",
    "\n",
    "Yup.  In any space, a hyperplane cuts that space in half.  How do you get to the 2 halves it creates?  With a vector perpendicular to the hyperplane.\n",
    "\n",
    "So you say 'I want the set of all vectors perpendicular to this vector'.\n",
    "\n",
    "Which means if your vector is, say (1, 1, -2), then the hyperplane perpendicular to this that goes through 0 is 1x+1y-2x=0.\n",
    "\n",
    "So all lower dimensional things are created from higher dimensional things.\n",
    "\n",
    "In 2d, a line can be specified by a vector perpendicular to it.\n",
    "\n",
    "But in 3d, a line can only be specified by 2 planes intersecting.\n",
    "\n",
    "Each plane needs a vector perpendicular to it.\n",
    "\n",
    "Which means a line in 3d needs 2 vectors to specify it.\n",
    "\n",
    "Well, actually you could just have a vector to specify a line.\n",
    "\n",
    "But still.\n",
    "\n",
    "In 4d, you need a vector to specify a .... sphere, I would guess?  An infinite sphere?\n",
    "\n",
    "Maybe that's not the right way to think about it.\n",
    "\n",
    "But also remember that you can just specify a line with 1 vector in any number of dimensions.\n",
    "\n",
    "You can specify a plane in any number of dimensions with 2 vectors in the plane as long as they're not dependent.\n",
    "\n",
    "So you can specify a thing with vectors inside of it, or vectors outside of it.\n",
    "\n",
    "Well, not any vector outside of the plane.  If it's outside, it has to be perpendicular.\n",
    "\n",
    "Also these hyperplanes have to be through the origin.  Otherwise you also need an offset.\n",
    "\n",
    "Normally, you are used to graphs of x and y, where y is dependent on x.\n",
    "\n",
    "Now there will be graphs of x1, x2, etc, where x1 and x2 are not necessarily dependant on each other.\n",
    "\n",
    "A data point is still a function, but its value isn't necessarily meaningful.  Like for apple vectors.\n",
    "\n",
    "Although you can still use the length.\n",
    "\n",
    "With your skewed plane, you would say that the vector (1, 1, 1) has a length of root(3).  You want a 3d vector that has length 1.  That would be \n",
    "\n",
    "What about my bread and cake and blah blah example?\n",
    "Much like regular math, you can do a whole lot of stuff that doesn't make sense.\n",
    "you can do eggs * butter, but that isn't meaningful for your specific problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about this:  Each component of your vector is an input, and each row of your matrix is a function that takes the components of your vector as an input.  You want more than just a single number, so you stack a bunch of functions on top of each other and compute them all together.  Now you have a new vector in a new vector space.  This vector means something completely different than the old vector.  Unless your equations are something like a=3\\*a+b or whatever.  Then it means the same thing.  Just think of matrix multiplication as functions.  Then inverses, transposes, determinants, whatever, it all depends on context.  Eggs \\* butter doesn't _mean_ anything, but distance \\* time does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`So each row is a linear equation.  What is the thing on the right of the equals sign called?  How does it affect the line?  How is it different from the numbers attached to the x's?`\n",
    "\n",
    "Look at this equation:\n",
    "\n",
    "$3x + y = 2$\n",
    "\n",
    "This is just another form of the equation\n",
    "\n",
    "$y = -3x + 2$\n",
    "\n",
    "Without the '+2', this line would go through the origin with a '-3' slope.  With the '+2', it goes through the y-intercept at '+2', and the x-intercept at '-2'.\n",
    "\n",
    "In the latter equation, the '+2' is called the y-intercept.  It shifts the lines away from the origin.  In this case, it shifts the line up 2 spaces on the y axis.\n",
    "\n",
    "I'm just going to call it the constant term.  Constant for short.  The other things aren't constants, they're coefficients.\n",
    "\n",
    "`If solving systems of linear equations is all linear algebra is for, why do we even need more complicated stuff like matrix multiplication and determinants?`\n",
    "\n",
    "Good question.  Here's an example question that would require us to use matrix multiplication.\n",
    "\n",
    "`Why does adding 2 shift the y-axis?  Why is the x-axis reversed?`\n",
    "\n",
    "Because we're adding it to the x side of the equation.  These two equations are equivalent:\n",
    "\n",
    "$$\n",
    "y = -3x + 2\n",
    "y - 2 = -3x\n",
    "$$\n",
    "\n",
    "Shifting the line up 2 is the same as shifting the line left 2.  \n",
    "\n",
    "Now look at these 2 equations:\n",
    "\n",
    "$$\n",
    "y = -3x - 2\n",
    "y + 2 = -3x\n",
    "$$\n",
    "\n",
    "These two equations are also equivalent.  Shifting the line 2 down is the same as shifting the line 2 right.\n",
    "\n",
    "`If each row in a matrix is a linear equation, why does 3blue1brown talk about the columns all the time?`\n",
    "\n",
    "`What exactly is a vector?  Is it a point?  Is it an arrow?  A list of numbers?`\n",
    "\n",
    "`So an augmented matrix represents the coefficients of a bunch of variables, and the thing on the right of the bar is what they equal.  What does a non-augmented matrix represent?  It's just a bunch of variable coefficients without saying what they're all equal to.`\n",
    "\n",
    "`Why is matrix multiplication defined like it is?`\n",
    "\n",
    "`What is the difference between an Nx1 matrix a N-length column vector?`\n",
    "\n",
    "`What does multiplying a row vector by a matrix on the left mean?  We want to solve linear equations, and those equations are the rows of the matrix.  But you're multiplying by columns, so you multipy by a bunch of x_1's, then a bunch of x_2's, etc.  It doesn't make sense.`\n",
    "\n",
    "`Would you ever actually want to multiply a row vector by a matrix to its right?  Or is it just a consequence of how we've defined matrix multiplication?`\n",
    "\n",
    "`Why upper triangular form specifically?  Is there anything better?`\n",
    "Yes.  The more 0's the better, because that means it's an equation with fewer variables.  If you can get it to be diagonal, you have a solution for each of your variables.\n",
    "\n",
    "`Can I get a summary?`\n",
    "\n",
    "You have a bunch of different equations.  These equations represent things that are 'possible', rather than things that necessarily 'are'.  These equations are lines, planes, hyperplanes, etc.\n",
    "\n",
    "You want to know under what conditions are all of these equations (these things that are possible), possible at the same time.  What vector values (values of our variables $x_1$, $x_2$, etc) will validate all of our requirements (our equations), all at the same time?\n",
    "\n",
    "Perhaps you should think about the 3 lines on the graph in a different way.  You subtract 1 from the other.  But what if you just added that third thing to the matrix?  Rather than replacing rows, you just add new rows.  It would be the same thing.  The system would be 'overdetermined', but these new equations would be dependent on the old ones, so they wouldn't add any new information.\n",
    "\n",
    "Equations are descriptions of things.  You have a whole bunch of equations describing things.  You want to find out 'what things in the world fit the description of all of these equations?'  Think of 2 cars going at different speeds. \n",
    "\n",
    "$$\n",
    "car\\ A:\\ y=2x+1\n",
    "car\\ B:\\ y=3x\n",
    "$$\n",
    "\n",
    "Both of these equations describe cars.  They describe the relationships of the time the car has been driving (x), and their distance (y).  The 2 and 3 are the respective speeds.  You could ask 'when will these cars be at the same spot?'  In essence, what you are asking is 'when will the y values be the same?'  Since these are linear equations, there is only 1 x value for every y value, so when you find out when the cars are at the same place, you also find out the time that they are at the same place, since there's only 1 possible time they could be at the same place.\n",
    "\n",
    "`vectors` are lists of numbers.  Here's an example:  \n",
    "\n",
    "[1, 3, 5, 3.423].  \n",
    "\n",
    "In regular algebra, you used `scalar` numbers to describe things.  4 apples.  1 person.  3 slices of pie.  In linear algebra, you use vectors to describe things.  Each number in the list describes some characteristic of the thing you're describing.  We could say an apple is described by a vector where the first number describes the apples weight, the second describes its volume, the third describes its caloric content, and the fourth describes its acid content.\n",
    "\n",
    "In physics, vectors are often thought of as arrows in space, usually 2d or 3d space.  Each number in these vectors describes one of the d's (dimensions) in those spaces.  Really, it's the same either way.  You could interpret a physics vector as just a list of numbers, and you could interpret an apple vector as an arrow in 4-dimensional apple space.\n",
    "\n",
    "A scalar is really just a 1d vector.  4 apples in 1d apple space.  1 person in 1d person space.  3 slices of pie in 1d pie space.\n",
    "\n",
    "`linear combinations` are combinations of vectors.  Vectors can be stretched, squished, and added together.  \n",
    "\n",
    "You can stretch / squish a vector by multiplying it by a scalar.  \n",
    "\n",
    "You can add vectors together by adding their individual components together.\n",
    "\n",
    "The `span` of a set of vectors is all the new vectors you can make through linear combinations of the vectors in your set.\n",
    "\n",
    "`i hat, j hat, k hat` are simple vectors that are commonly used to make linear combinations.  i hat is for the x dimension, j hat for y, and k hat for z.  i hat and j hat are commonly used as the basis vectors of $R^2$, meaning that they span $R^2$.  i, j, and k are commonly used as the basis vectors of $R^3$.\n",
    "\n",
    "$$\n",
    "\\hat{i} = [1, 0, 0] \\\\\n",
    "\\hat{j} = [0, 1, 0] \\\\\n",
    "\\hat{k} = [0, 0, 1] \\\\\n",
    "$$\n",
    "\n",
    "Think of a robot that can only move by deciding to move either $\\hat{i},\\hat{j},$ or $\\hat{k}$, or some fraction of anyof them.  It could go anywhere in 3-dimensionsal space.  If you think of a set of vectors as the decisions of a robot, then all the places the robot can go is the span of that set of vectors.\n",
    "\n",
    "`basis vectors` It turns out that $\\hat{i},\\hat{j},$ and $\\hat{k}$ are special vectors, because they are `linearly independent`.  They have nothing in common in terms of where they can point.  They are all `orthogonal` to eachother.  Consider the vector $\\hat{g}$ = [0, 1, 1].  $\\hat{g}$ is independent from $\\hat{i}$, but not independent from $\\hat{j}$ or $\\hat{k}$.  The three vectors $\\hat{i}$,$\\hat{j}$,$\\hat{g}$ are a spanning set of $R^3$, but they are not a set of basis vectors for $R^3$, because they are not all linearly independent from eachother.  Basis vectors are easier to make linear combinations with.\n",
    "\n",
    "Any vector in $R^2$ can be made with $\\hat{i}$ and $\\hat{j}$.  For instance, v=[-1, 2] is the same thing as $v=-1*\\hat{i}+2*\\hat{j}$.\n",
    "\n",
    "Imagine if your robot wanted to move 1 space in the z direction.  The only way to do that would be to select $\\hat{g}$.  But then it would also have to move -1*$\\hat{j}$.  This robot is basically z-disabled.  Look at what you've done.  You've crippled your robot.\n",
    "\n",
    "<details><summary>But this robot is more efficient when it's moving in $\\hat{g}$-like directions.  How is this bad?</summary>\n",
    "We want to be equally efficient in moving in any direction, because we're equally likely to move in any direction.  Linearly independent basis vectors are equally efficient no matter which direction you're going.\n",
    "</details>\n",
    "\n",
    "`linear transformations` A transformation on a vector just means to change that vector into another vector.  A linear transformation is a transformation that only rotates/stretches/squishes the vector.  No curving.\n",
    "\n",
    "In linear algebra, all vectors are some combination of basis vectors.  In $R^2$, it's $\\hat{i},\\hat{j}$.  In $R^3$, it's $\\hat{i},\\hat{j},\\hat{k}$.\n",
    "\n",
    "To linearly transform a vector, just transform it's $\\hat{i}$ and $\\hat{j}$ components, then combine them.\n",
    "\n",
    "Say we have a vector $\\vec{p}=x*\\hat{i}+y*\\hat{j}$.  A linear transformation changes $\\hat{i}$ and $\\hat{j}$.  Now the new $\\vec{p}=x*(transformed\\ \\ \\hat{i})+y*(transformed\\ \\ \\hat{j})$.  \n",
    "\n",
    "`matrix` A matrix is a function that does linear transformations on vectors.  They look like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "i_1 & j_1 \\\\\n",
    "i_2 & j_2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first column represents the transformed $\\hat{i}$, and the second represents the transformed $\\hat{j}$.  \n",
    "\n",
    "This matrix says \"turn $\\hat{i}=[1, 0]$ into $\\hat{i}=[1, 2]$ and turns $\\hat{j}=[0, 1]$ into $\\hat{j}=[3, 4]$\":\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 \\\\\n",
    "2 & 4 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "What if you wanted to make a matrix for $R^2$ that rotates all vectors 90 degrees left?  Just rotate $\\hat{i}$ and $\\hat{j}$ by 90 degrees each.  Now you have a matrix that describes how to rotate any vector in $R^2$ 90 degrees left.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since all vectors in $R^2$ are just some combo of $\\hat{i}$ and $\\hat{j}$, by rotating these 2 vectors, you've essentially rotated all the other vectors too.  Note that you could do this equally well for some other set of basis vectors.\n",
    "\n",
    "Could you do the same thing with some other set of basis vectors?  What about a spanning set that isn't a basis set?  What does it mean to have a matrix of vectors that doesn't consist of basis vectors?  What about basis vectors that aren't $\\hat{i}$ and $\\hat{j}$?  Wouldn't that be a transformation that actually does stuff?\n",
    "\n",
    "<details><summary>Why is matrix multiplication rows to columns?</summary>\n",
    "If we look at how A*x should be formatted, the first thing we think is that x should be vertical, not horizontal.\n",
    "    \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 \\\\\n",
    "2 & 4 \n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "5 \\\\\n",
    "6 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This seems fine.  But what if x was horizontal?\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 \\\\\n",
    "2 & 4 \n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "5 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This version of matrix multiplication wastes whitespace.  Specifically the extra space above and below x.  So the quintessential vector is vertical, not horizontal.  So each item in the vector x will either be multiplied by an entire row in the matrix, or an entire column in the matrix.\n",
    "\n",
    "While it seems to me that multiplying each item in the vector by a row is more intuitive, there is something to be said about multiplying by columns.  If our x vector is a column, it is more thematically consistent to have $\\hat{i}$ and $\\hat{j}$ be columns in A, so that they are similar to x.\n",
    "\n",
    "The takeaway from this is that we could have defined it some other way.  There is no deep meaning to how we have defined matrix multiplication other than 'it looks pretty'.\n",
    "\n",
    "</details>\n",
    "\n",
    "`determinants` are how much a matrix 'expands' or 'contracts' the space that it transforms.  It doesn't really make sense to think about on a single vector.  Let's look at the below matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 0 \\\\\n",
    "0 & 2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, $\\hat{i}$ gets stretched by a factor of 3, and $\\hat{j}$ gets stretched by a factor of 2.  If you think of $\\hat{i}$ and $\\hat{j}$ as making a little 1x1 box before, now they make a 2x3 (j comes before i) box, so the determinant is 6.  Again, the determinant is how much the matrix changes space.  It is the ratio of the new area to the old area of the box made by i and j.  Since the ratio of the old area is always 1, the stretching is always (new area)/(old area) = (new area) / 1 = (new area).  So the determinant is the area of the new box.\n",
    "\n",
    "But it's more complicated than that.  With the matrix: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 0 \\\\\n",
    "0 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Our 2d space is now a 1d space.  Any time you get rid of dimensions, the determinant is 0.\n",
    "\n",
    "The determinant can also be negative.  \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 & 0 \\\\\n",
    "0 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "An area can't be negative, and that's kind of where our 'area' analogy breaks down.  Think of a negative determinant as our square flipping over onto its backside, since j is now 'on the right' of i instead of 'on the left'.\n",
    "\n",
    "Imagine you had a 2d picture of mario's face, completely symmetric except for a single mole on his right cheek.  If the determinant is negative, mario's face flips, and the mole is now on his left cheek.\n",
    "\n",
    "Only square matrices have determinants.  What would it mean to 'expand' a 2 dimensional square into 3 dimensions via a 3x2 matrix?  What is its area now?  That doesn't even make sense, because now the square is no longer a square, it's some 3d shape.  It has volume now, not area.\n",
    "\n",
    "Speaking of volume, the determinant for volume can also be negative using the same 'mario mole' example.  But it is slightly more complicated.  If any 2 of i, j, and k switch places, the determinant will be negative.  If we get 3 shifts in some way, the mole will still be on mario's right cheek and the determinant will be positive.  There's some generalization of this, but I don't care enough to think about it.\n",
    "\n",
    "Determinant formula:\n",
    "\n",
    "$$\n",
    "det(\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d \n",
    "\\end{bmatrix}) = ad-bc\n",
    "$$\n",
    "\n",
    "Consider the case where b and c are 0.  In this case, i and j are just scaled along their respective axes.  If you add a positive b and c, both i and j are now pointing up and to the right.  The closer i and j get, the more the determinant rhombus thing gets squeezed, and the less area it has.  So b and c combine to form a kind of 'negative rhombus' that subtracts from the total rhombus of the determinant.  Of course, if b\\*c turns out to be negative, the determinant will increase in value.\n",
    "\n",
    "The determinant tells us how much the vectors distances relative to eachother change.  If the determinant of a matrix is 6, all vectors that were originally 1 away from each other will now be 6 apart.  All vectors that were 2.5 away from eachother will not be 15 apart.\n",
    "\n",
    "For now, skip trying to reason out why this formula works exactly.  Also skip for 3d and onward.  Maybe come back to it.  But you can also decompose into triangular matrices for high dimension matrices.  Maybe that formula will be easier to explain.\n",
    "\n",
    "`rank` the number of dimensions in the matrix output.  A 3x2 matrix takes in 2-vectors and outputs 3-vectors, so the rank of that matrix is 3.  Unless it outputs a plane of vectors in 3-space, in which case it's rank is 2.  Or if it turns every vector into [0 0 0], in which case it's rank is 0.\n",
    "\n",
    "`column space` is the span of all the vectors that your matrix can output.  So it's the span of the columns, since your matrix outputs linear combinations of its columns.\n",
    "\n",
    "`null space` all the vectors your matrix turns into 0.  If it's any more than a single vector your matrix is squishing space.\n",
    "\n",
    "Also need inverse matrices, row space, whatever.\n",
    "\n",
    "`dot product` is a special kind of linear matrix transformation.  It's used to discover the relationship between 2 vectors v and w.  If the dot product is 0, they're perpendicular, if they're facing the same direction, it's positive.  If they're facing different directions, it's negative.  In general, the dot product is used to measure 'similarity' between 2 vectors.\n",
    "\n",
    "Remember all the vectors from 3blue1brown being folded onto a plane?  It's like that.  Notice that the arrow pointing perpendicular to the plane gets 'folded' by being squished down into nothing.  So here you're squishing a vector into another vector.  If that squished vector is 0, you know the 2 vectors are perpendicular.\n",
    "\n",
    "You take 2 vectors, say v and w, of the same dimension and matrix multiply them into a single number.  In this instance, either v or w could be considered the matrix, and the other the vector.  Really what this shows is that vectors are just matrices.  A vector is really a nx1 matrix, where the n is each individual actual dimension of whatever, and the 1 output is a single measure whatever it is.  It's kind of weird to think about for a lot of things.  I guess if you took the dot product between 2 apple vectors and they were perpendicular, they would be like, opposite apples or something?  But with an apple you can't have any negative attributes.  So that doesn't really make sense.\n",
    "\n",
    "It is kind of weird that the dot product is so 'asymmetric'.  That's covered in the video.  Maybe explain that later.\n",
    "\n",
    "Perhaps also clear up that the dot product isn't some operation between 2 lines, but an operation between 2 points.  I think you might be getting a little confused in your intuition of vectors, thinking they're lines instead of points.\n",
    "\n",
    "`What is the significance of 2 vectors being perpendicular?`\n",
    "\n",
    "If we have a set of vectors that span a space, those vectors are a basis if they are all independent of each other.  They are all independent of each other if all of their dot products with each other are 0.  Wait, that's pairwise independence.  Anyway, it's part of it.  If we have a basis of vectors, that's better than a regular span of vectors, because it's simpler.  I think it's less computationally expensive.  That's at least 1 reason.  Remember the example with the robot.\n",
    "\n",
    "`cross product` take 2 vectors v and w, and you can use the cross product to find a third vector p that is perpendicular to both v and w at the same time.  Also p's length will be equal to the rhombus created by v and w (as though v and w were i and j).  It's pretty simple to find a vector orthogonal to v, and simple to find a vector orthogonal to w, but in 3d there are infinite vectors orthogonal to v and w.  So how do you find a vector orthogonal to both v and w out of infinitely many vectors?  With the cross product.\n",
    "\n",
    "\n",
    "\n",
    "`eigenvectors` look at the 'change of basis' video maybe.  All nxn (square) matrices have eigenvectors.  A vector is an eigenvector for a matrix if that matrix stretches/squishes the vector but doesn't rotate it.\n",
    "\n",
    "`Eigenvalues` each eigenvector is stretched/squised by a certain amount; its eigenvalue.\n",
    "\n",
    "`diagnol matrix` are easy to work with.  Can easily compute things many times.  All 0's except the diagnol.  The basis vectors are all eigenvectors of a diagnol matrix.\n",
    "\n",
    "`eigenbasis` how do I compute the 100th power of this matrix?  Easiest way is to do change of basis to some eigenbasis, do the 100th power, then change of basis back.  Not all matrices can be diagnolized (translated).  Things like a rotation, which don't have any eigenvectors.  But there are other ways to do those.  Maybe break it up into it's rotational part and its scaling part.  Then you can change of basis the scaling part, then do that modulo thing for the rotation.\n",
    "\n",
    "`why is matrix multiplication defined like it is?`\n",
    "\n",
    "I think this might be your answer:\n",
    "https://math.stackexchange.com/questions/31725/intuition-behind-matrix-multiplication\n",
    "\n",
    "I don't know where to put this, but linear combinations of your equations doesn't create a new 'constraint', as you put it earlier.  I don't know where this will go, but I feel like it will go somewhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that one paper on the nearest-neighbor algorithm, and how they figured out how to make it faster by not using the euclidean definition of ‘nearest’?  That makes me wonder what other problems we could solve by discarding this notion that euclidean distance is the ‘best’ (or ‘optimal’, haha) measure of optimality.  Like, sometimes manhattan distance is best.  When is variance the best indicator of spread, and when is it not?  Hm…. I feel like there’s something here I’m not quite getting.  Can you optimize your notion of optimality?  Obviously not for simple problems like 'maximize the money you're making', but for more complicated stuff.\n",
    "\n",
    "On page 209 of the CS170 textbook, they talk about duality.  Notice that the dual for \n",
    "$$Ax \\leq b$$ \n",
    "is \n",
    "$$y^TA\\geq c^T$$\n",
    "\n",
    "This might be your answer to 'what's the point of being able to multiply by a matrix on the left?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EE127`\n",
    "\n",
    "In EE127, a `function` is a function f: $R^n \\rightarrow R$\n",
    "\n",
    "A `map` is a function g:$R^n \\rightarrow R^m$\n",
    "\n",
    "So a function has only scalar outputs, and a map has vector outputs.\n",
    "\n",
    "A `graph` for a function f is exactly what you would expect it to be.\n",
    "\n",
    "An `epigraph` for a function f is the graph, plus everything above the possible values of f.  Maybe it's to show feasible solutions?\n",
    "\n",
    "A `contour`, or contour line, is something for functions with multiple input variables.  A contour line is a squiggly oval thing (a closed curve) with a value.  If a contour has a value of 3, the contour line is saying 'all of the x, y values that I have will all output a z value of 3'.  So the contour line would be a squiggle looping around the z-axis at a height of 3.\n",
    "\n",
    "You can see an example of contour lines in the CS170 textbook on linear programming.\n",
    "\n",
    "A `level set` is a contour.\n",
    "\n",
    "A `sublevel set` is a contour plus everything above the possible values of the contour.  It's a bunch of contours with different values all on the same graph.\n",
    "\n",
    "`What does it mean if we have a bunch of constraints and no objective function?`\n",
    "\n",
    "Then we're just trying to find any feasible solution.  No need for maxing/mining.\n",
    "\n",
    "`If there are feasible solutions, is there always an optimal solution?`\n",
    "\n",
    "Yes, but sometimes you can't get to them.  Look at this problem:\n",
    "\n",
    "min $e^{-x}$\n",
    "\n",
    "It just gets smaller and smaller the larger you make x.  So you can never really get the optimal solution, because the optimal solution is $x=\\infty$\n",
    "\n",
    "`Why would we ever go for a suboptimal solution?`\n",
    "\n",
    "Calculating the exact optimal solution sometimes takes a really long time.  We sometimes instead go for the $\\epsilon-suboptimal$ set of solutions.  $\\epsilon$ is just some value that says 'if you're $\\epsilon$ away from the optimal solution, you're good enough.'\n",
    "\n",
    "`If we're going for suboptimal solutions with this epsilon thing, we have to know the optimal solution in the first place.  How is this helpful at all?`\n",
    "\n",
    "Remember you want to calculate some vector x that will give you an optimal solution.  Maybe there's some easy way to figure out the bounds on an optimal solution without actually calculating x in the first place.\n",
    "\n",
    "`What exactly is a convex function?  What makes them easy to solve?`\n",
    "\n",
    "A function is convex if you can pick any 2 points on the function's graph and draw a line between them without that line going through any other points.  So a line or a parabola are convex.  The S thing generated by $f(x)=x^3$ is not convex, because if you draw a line between, say (-1, -1) and (1, 1), that line will go through the point (0, 0), which is also on the curve of f.\n",
    "\n",
    "Convex functions are easy to solve because they have 1 global maximum.  With no local maximums to throw you off, it's much easier to find the actual maximum, and therefore the optimal answer.\n",
    "\n",
    "Concave functions have 1 global minimum and are equivalent.\n",
    "\n",
    "`Why is the book definition of convex optimization so formal and obtuse?`\n",
    "\n",
    "Because this definition works for more than just linear programming, which is what you learned before.\n",
    "\n",
    "It works for least squares, linear programming, quadratic programming, nonlinear optimization, and convex optimization and combinatorial optimization.\n",
    "\n",
    "Maybe you could take your linear programming example and convert it to this more formal notation?\n",
    "\n",
    "Maybe try to do the discussion before you do the homework?\n",
    "\n",
    "`What is an affine set?  Why are we even making a distinction?`\n",
    "\n",
    "First, look at these two vectors in $R^3$: \n",
    "$$\n",
    "v_1=[1, 0, 0]\\\\\n",
    "v_2=[0, 1, 0]\n",
    "$$\n",
    "\n",
    "They span the subspace of the x-y plane in $R^3$.  So you can describe this plane with $v_1$ and $v_2$.  It describes every vector in $R^3$ that has a z component of 0.\n",
    "\n",
    "Now try to think of two vectors in $R^3$ that together span every vector in $R^3$ with a z component of 1.  Essentially, the x-y plane lifted up 1.  It's not possible with 2 vectors.\n",
    "\n",
    "However, it is possible with 3 vectors.  To describe this boosted x-y plane, we use notation something like this:\n",
    "\n",
    "$$A=S+v_0$$\n",
    "\n",
    "S is the subspace of the x-y plane, and we'll say that $v_0$ is the vector (0, 0, 1).  So $v_0$ isn't part of the span of the other 2 vectors, its a vector that is added to every single vector in the span, boosting them all by 1 in the z direction.\n",
    "\n",
    "This boosted plane, A, is called an 'affine set'.  It is distinguised from subspaces because it can't be described by a set of vectors that 'span' a space.\n",
    "\n",
    "`What is a norm?`\n",
    "\n",
    "Here's the general formula:\n",
    "\n",
    "$$\n",
    "\\sqrt[n]{x_1^n+x_2^n+...}\n",
    "$$\n",
    "\n",
    "If n is 1, it's manhattan distance.  If n is 2, it's euclidean distance.  If we take n to it's limit of infinity, it's the biggest of x's components.\n",
    "\n",
    "As n gets bigger, the magnitude gets smaller.\n",
    "\n",
    "`Why is the infinity norm the maximum of the vectors components?`\n",
    "\n",
    "As n gets bigger and bigger, the biggest of all the $x_i$'s will start to dwarf the others.  So all the other x's become insignificant, basically nothing.  So when n is really really big, the equation might as well be:\n",
    "\n",
    "$$\n",
    "\\sqrt[\\infty]{x_{biggest}^\\infty}\n",
    "$$\n",
    "\n",
    "The infinite power and infinite root cancel out, and we are left with $x_{biggest}$.\n",
    "\n",
    "`What is the cauchy-schwartz inequality?`\n",
    "\n",
    "The dot product of 2 vectors is less than or equal to their euclidean magnitudes multiplied together.\n",
    "\n",
    "They're equal when the dot product is maximized.  The dot product is maximized when the two vectors point in the exact same or exact opposite direction.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "x^T*y \\leq \\sqrt[2]{x_1^2+x_2^2+...} * \\sqrt[2]{y_1^2+y_2^2+...}\n",
    "$$\n",
    "\n",
    "It's a little more obvious if you consider the case where x == y, since the euclidean norm is just a dot product which you take the square root of:\n",
    "\n",
    "$$\n",
    "y^T*y \\leq \\sqrt[2]{y_1^2+y_2^2+...} * \\sqrt[2]{y_1^2+y_2^2+...} \\\\\n",
    "y^T*y \\leq y_1^2+y_2^2+... \\\\\n",
    "y^T*y == y_1^2+y_2^2+...\n",
    "$$\n",
    "\n",
    "If you're still not convinced, consider taking one of the y's and stretching /shrinking it.  The equality will still hold.  Rotate the vector, and the dot product will be less than the norm-multiplying.\n",
    "\n",
    "Perhaps there's some tip to tail analogy here?  If the vectors are pointing in the exact same direction, the total distance is maximized.  But when you 'bend' or push the 2 vectors so that they're not pointing in the same direction, that can only make the total combined distance less.\n",
    "\n",
    "Cauchy-Schwartz also implies the following:\n",
    "\n",
    "$$\n",
    "\\frac{x^T*y}{\\sqrt[2]{x_1^2+x_2^2+...} * \\sqrt[2]{y_1^2+y_2^2+...}} \\leq 1 \\\\\n",
    "$$\n",
    "\n",
    "Segway to the 'Angles between vectors' thing here.\n",
    "\n",
    "Imagine a crazy bug that only wants to move in 1 specific vector direction.  If someone picked up the bug and moved it along a vector perpendicular to the direction it wanted to go, it would not care.  Imagine the perpendicular vector as drawing an infinite starting line.  If you move the bug ahead of the starting line (positive dot product) it's happy, and if you move it behind the starting line (negative dot product) it's angry.\n",
    "\n",
    "You have x, the vector you're projecting onto, and y, the vector that's getting projected.  Slide y along the line perpendicular to x until x and y are pointing in the same direction.\n",
    "\n",
    "`Ok, yeah, orthogonality is when the dot product is 0.  But then the way we explain the dot product is that when it's 0 it means the 2 vectors are orthogonal.  When do you actually use the dot product?  What is it useful for?`\n",
    "\n",
    "The projection of a vector onto a line, or a plane, or a whatever, calculates the answer to the question 'I have a vector and this other thing.  What point on this other thing is closest to this vector?'\n",
    "\n",
    "By 'closest' we mean distance, and by distance, we mean euclidean distance, usually.\n",
    "\n",
    "You woulnd't calculate a vector's projection onto another vector.  That doesn't make any sense.  'Which point on x is closest to this point y?  x is only 1 point, so of course the answer is x.  Instead, we figure out a line that goes in the same direction as x, then project y onto that.'\n",
    "\n",
    "Ok, trig functions.  An angle is measured in radians.\n",
    "Radians are some constant times pi.\n",
    "Pi is the ratio of the circle's circumference to its diameter.\n",
    "Tau is the ratio of the circle's circumference to its radius.\n",
    "A circle's area is pi*r*r\n",
    "which is the same as (C/2r)*r*r\n",
    "which is the same as Cr/2\n",
    "So using pi is good because then you don't have to measure circumference.\n",
    "Since circumference is just a function of the radius / diameter.\n",
    "But perhaps diameter is easier to measure?  Just start at any point on the circle,\n",
    "and sweep your ruler or whatever across the circle.  The point at which this sweep\n",
    "is maximized is the diameter.  But then I guess radius is just as easily gotten.\n",
    "Just 1 extra step.\n",
    "We say that an angle of 2pi is the same as 0, because why?  Why does it stop at 2pi?\n",
    "Why not pi?\n",
    "An angle is measured as some slice of a circle of radius 1, and area pi.\n",
    "Why not just measure angles directly, with some fraction of 1?\n",
    "Well, I guess it would only ever go up to 1/2.\n",
    "Once you got past 1/2 it would start going down again.\n",
    "So I guess you only need a half circle to measure an angle.\n",
    "So I guess it makes sense.  Kind of.\n",
    "But why pi/2 for a 90 degree angle?  Why not 1/2?  Just have the radius be root(pi).\n",
    "Maybe it has something to do with the triangle stuff.\n",
    "\n",
    "Why don't cos and sin always add up to 1?  Why do we square them before adding them?\n",
    "Maybe it has to do with euclidean norm.\n",
    "\n",
    "So cos measures angles equalness/oppositeness, and sin measures their perpendicularness.\n",
    "A helpful way to think about perpendicularness is that since it's a right angle,\n",
    "all you have to do is rotate it and you get vectors in the x/y direction.\n",
    "So they, as you thought of earlier, 'have nothing in common'.\n",
    "Their components are completely distinct.\n",
    "\n",
    "Any 2 vectors, when put tail to tail, form a right triangle.  \n",
    "Just put one of them on the x axis.\n",
    "Now if you actually want a right triangle the 2 vectors will have to be the same length.\n",
    "Let's shrink the vectors so they're both size 1.\n",
    "This way they'll definitely make a right triangle.\n",
    "Hm...but how exactly do we shrink the vector?\n",
    "Obviously we want it to be size 1, but what exactly is the size of a vector?\n",
    "The euclidean distance is the most natural choice.\n",
    "Wait.  But is it?\n",
    "It's the best for physical phenomena.  It's just like how we measure things with a ruler.\n",
    "But what about other stuff?  Like your 'apple vector' thing earlier?\n",
    "What if your idea of 'best' was just the total of the vitamins?\n",
    "If apple X had 30, 30, 30, and apple Y had 50, 25, 0, then X has a larger L1 norm, but Y has a larger L2 norm.\n",
    "\n",
    "What is the dot products connection with cos?\n",
    "\n",
    "Wait, let's go back to the whole 'how do we measure an angle' thing.\n",
    "You need a function that tells you 'these 2 vectors are going in the same direction'\n",
    "it also needs to say 'these 2 vectors are perpendicular'\n",
    "it also needs to say 'these 2 vectors are going opposite of each other'\n",
    "and everything in between.\n",
    "what should the range of this function be?\n",
    "well, you need the range to be symmetric.  \n",
    "xMax for same direction, 0 for perpendicular, xMin for opposite direction.\n",
    "What's the simplest xMax and xMin you can think of?  1 and -1.\n",
    "\n",
    "\n",
    "---------------\n",
    "\n",
    "a spanning set is a set of vectors that can be combined to create anything in a vector space.  It is a finite set of vectors.\n",
    "A vector space gets spanned by a spanning set.  It is an infinite set of vectors.\n",
    "A basis is a special kind of spanning set.  All of its vectors are independent.\n",
    "\n",
    "A={v0=(1, 0), v1=(0, 1), v2=(0, 2)} is a spanning set for R2\n",
    "A is not a basis of R2, since v0 and v2 are dependent on each other.\n",
    "\n",
    "B={v0=(1, 0), v1=(0, 1)} is also a spanning set for R2.\n",
    "B is a basis of R2, since v0 and v1 are independent.\n",
    "\n",
    "R2 is spanned by A.\n",
    "R2 is the span of A.\n",
    "These statements are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template\n",
    "\n",
    "`Motivation`\n",
    "\n",
    "`Formula`\n",
    "\n",
    "`Intuition`\n",
    "\n",
    "`Proof`\n",
    "\n",
    "`Questions`\n",
    "\n",
    "the dot product of 2 orthogonal / independent vectors is 0.  X and Y.\n",
    "The definition of orthogonality / independence comes from the dot product. of X and Y.\n",
    "Orthogonality / independence means X and Y are at right angles to each other.\n",
    "Right angles means X and Y form a right triangle.\n",
    "Right triangle means a whole bunch of interesting properties.\n",
    "    Like you can easily compute the length of the sum of X and Y.\n",
    "    Wait, can you do this easily even if they're not orthogonal.\n",
    "    Means the projection of X onto Y is 0, and Y onto X is 0.\n",
    "    \n",
    "Given X and Y, you can break X into any number of components.\n",
    "But what is interesting is when you break X into 2 components:\n",
    "    O, The component of X orthogonal to Y\n",
    "    P, The component of X parallel to Y\n",
    "    \n",
    "    \n",
    "X = O + P\n",
    "<X, Y> = ???\n",
    "<O, Y> = 0\n",
    "<P, Y> = |P| * <1 in the direction of Y, Y> = |P| * ????\n",
    "Maybe Y should be of size 1?\n",
    "After all, we're trying to project onto a line, not a vector, really.\n",
    "A projection has nothing to do with Y's length.\n",
    "Only its direction.\n",
    "In this case, the dot product would be P.\n",
    "\n",
    "<x=(2, 2), y=(4, 4)> = 2\\*4+2\\*4 = 16\n",
    "|x| = root(8)\n",
    "|y| = root(16)\n",
    "\n",
    "It's not the X, Y, and sine that make the triangle.  It's X, O, and P that make the triangle.\n",
    "\n",
    "proj x onto y = cY\n",
    "c = <x, y> / <y, y>\n",
    "\n",
    "So to project X onto Y:\n",
    "Find Y_1 = Y / |Y|, which is a 1 vector in the direction of Y\n",
    "proj X onto Y = <X, Y_1> * Y_1 (since it's a vector)\n",
    "\n",
    "We can get this same thing by doing:\n",
    "<X, Y> / <Y, Y> * Y, since it's also\n",
    "<X, Y_1> * |Y| / (<Y_1, Y_1> * |Y| * |Y|) = <X, Y_1> / (<Y_1, Y_1> * |Y|) = <X, Y_1> / (1 * 1 * |Y|)\n",
    "asdf = <X, Y_1> / |Y|.  The 1 * 1 thing is because the dot product with yourself is length squared, and Y_1's length is 1.\n",
    "\n",
    "<X, Y> = <P + O, Y> = <P, Y> + <O, Y> = <P, Y> + 0 = <P, Y>.\n",
    "You would never actually compute P, you would just do X.\n",
    "This is just to show that the dot product using X or P is the same.\n",
    "And P is the thing you want to get, so yeah.\n",
    "\n",
    "\n",
    "for the last part of question 2, they give you the hint y->min of x-prime (x').  Mong says the prime doesn't matter.  You can think of it as just x.\n",
    "Wait could you just use duality for this question and show that they're equivalent?  That would show something stronger than what they're asking for.\n",
    "\n",
    "\n",
    "An angle is the fraction of a circle that 2 vectors create when you join them tail to tail.  It's a slice of pie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Projection\n",
    "\n",
    "`Motivation`\n",
    "\n",
    "Say we have 2 vectors, x and y.  The projection of x onto y is kind of like the portion of x that is going in the y direction.  \n",
    "\n",
    "If you have a heavy box, and x is the direction you're pushing the box in, and y is the inclined plane you're pushing the box up, then the projection of x onto y is the part of x that 'counts', because directing the force any other way is wasted.\n",
    "\n",
    "`Formula`\n",
    "\n",
    "To understand the formula, you'll need to know that the dot product of 2 independent vectors is 0.\n",
    "\n",
    "$$\n",
    "proj(a\\ on\\ b) = \\frac{<a, b>}{<b, b>}*b\n",
    "$$\n",
    "\n",
    "`Intuition`\n",
    "\n",
    "`Proof`\n",
    "\n",
    "`Questions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot product\n",
    "\n",
    "`Motivation`\n",
    "\n",
    "\n",
    "\n",
    "In addition to being used for vector projection, you might also take the dot product between any 2 vectors to see if they're orthogonal.\n",
    "\n",
    "The dot product of $\\vec{x}$ and $\\vec{y}$ is the magnitude of x projected onto y.  Equivalently the magnitude of y projected onto x.\n",
    "\n",
    "Wait, this is wrong.  Y would need to be of size 1 in order for this to be true.\n",
    "\n",
    "The only time you would bother taking a dot product where neither vector is size 1 is when you're testing for orthogonality.\n",
    "\n",
    "`Formula`\n",
    "\n",
    "Dot notation:\n",
    "\n",
    "$\\vec{x} \\cdot \\vec{y} =  x_1 * y_1 + x_2 * y_2 + ...$\n",
    "\n",
    "Angle bracket notation:\n",
    "\n",
    "$\\langle \\vec{x}, \\vec{y} \\rangle = x_1 * y_1 + x_2 * y_2 + ...$\n",
    "\n",
    "`Intuition`\n",
    "\n",
    "`Proof`\n",
    "\n",
    "`Questions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norms\n",
    "\n",
    "`Motivation`\n",
    "\n",
    "Norms tell you the size / length of a vector, in different senses of the word 'size'.\n",
    "\n",
    "The L1-norm is Manhattan distance.\n",
    "\n",
    "The L2-norm is Euclidean distance.\n",
    "\n",
    "The L-Infinity-norm is the max component of a vector.\n",
    "\n",
    "`Formula`\n",
    "\n",
    "`Intuition`\n",
    "\n",
    "`Proof`\n",
    "\n",
    "`Questions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cauchy Schwartz Inequality\n",
    "\n",
    "`Motivation`\n",
    "\n",
    "Proves that the dot product of x and y is less than or equal to their lengths multiplied together.\n",
    "\n",
    "$$\n",
    "<a, b> \\leq root(<a, a>) * root(<b, b>)\n",
    "$$\n",
    "\n",
    "Basically says 'the dot product of a and b is less than or equal to their lengths multiplied.'\n",
    "\n",
    "It's equal if they're colinear, meaning going in the same direction.\n",
    "\n",
    "Cauchy Schwartz is equivalent to:\n",
    "\n",
    "$$\n",
    "<a, b> / (root(<a, a>) * root(<b, b>) \\leq 1\n",
    "$$\n",
    "\n",
    "It turns out that that the division is the angle between the two vectors like so:\n",
    "\n",
    "$$\n",
    "<a, b> / (root(<a, a>) * root(<b, b>) = cos(Theta)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Not sure why this is true exactly.  I think they try and explain it.  Maybe come back to it later.\n",
    "\n",
    "Let's go back to the 'dot product of a and b is less than or equal to their lengths multiplied.'  I'm internally thinking of <b, b> as a square made by 2 vectors, one being b and the other being perpendicular to b, and the same length.  And <a, b> is some kind of rhombus.  But if you normalize, wouldn't those be the same area?  Maybe you're thinking of it wrong.\n",
    "\n",
    "`Formula`\n",
    "\n",
    "`Intuition`\n",
    "\n",
    "`Proof`\n",
    "\n",
    "`Questions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthonormal basis\n",
    "\n",
    "`Motivation`\n",
    "\n",
    "An orthonormal basis is the best basis.  It's a spanning set where there are no dependent vectors, all the vectors are at right angles to each other, and all the vectors have length 1.  So for $R^3$, the typical orthonormal basis is (1, 0, 0) (0, 1, 0) (0, 0, 1).\n",
    "\n",
    "I think this is important for change of basis.\n",
    "\n",
    "Gram-Schmidt procedure: used to turn a basis into an orthonormal basis.\n",
    "\n",
    "Take first vector, make it have length 1.  Take second vector, find its projection onto first vector.  Subtract projection from second vector.  Second vector - projection of second vector onto first vector = new vector that is orthogonal to vector 1.  Now vector 1 and 2 have a 90 degree angle between them.  Now take the third vector.  Do this for _both_ of the previous vectors.  So for the third one you do 2 subtractions.  Also remember that you have to normalize all of them.\n",
    "\n",
    "Remember, subtract the projection, _then_ normalize.  If you normalize, then get the projection, then subtract the projection, it won't be length 1 anymore, and you'll have to normalize it again.\n",
    "\n",
    "If vector a and b are dependent, then when you do this procedure you'll end up with either a or b being the 0 vector, depending on which one you do first.\n",
    "\n",
    "`Formula`\n",
    "\n",
    "`Intuition`\n",
    "\n",
    "`Proof`\n",
    "\n",
    "`Questions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Duality` Thanatcha says that every problem has a dual, but that dual isn't necessarily the exact same problem.\n",
    "\n",
    "But, if it is the exact same problem, then you can turn your optimization problem into an optimization problem without constraints, which will mean you can solve it a lot quicker and easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is matrix multiplying like x * A like saying 4 + 2 * x?  Like, you can do it either way but we have a standard way to do it?\n",
    "\n",
    "Linear algebra.  If a matrix is just a function on vectors, how come we always do multiplication with vectors?  Isn't the matrix supposed to be like f?  So the matrix is like a function?  What is the difference between a dot product and a matrix multiplication?  Is a vector a matrix?  Is a matrix a bunch of vectors?  Is a vector a 'transformation', or is it representative of some 'object'?  Are those 2 things necessarily different?  Is matrix vector multiplication just a bunch of dot products?\n",
    "\n",
    "Ghaoi said that decomposition is the crowning achievement of linear algebra.  The spectral norm was invented in the context of physics.  All the important stuff was invented by application people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identity matrix:  1's on the diagonal, 0's elsewhere.\n",
    "\n",
    "Diagonal matrix:  x y z whatever on the diagonal, 0's elsewhere.\n",
    "\n",
    "Symmetric matrix: matrix[i][j] == matrix[j][i] for all i, j.  So The upper triangle is the same as the lower triangle.\n",
    "\n",
    "Upper triangular matrx:  if i > j, matrix[i][j] == 0.  So the lower triangle is all 0's.\n",
    "\n",
    "Orthogonal matrix:  A is an orthogonal matrix if A * A^T = I.  There are some pretty simple examples, but I can't really think of a pattern for them.\n",
    "\n",
    "Dyad:  A matrix that can be written as the product of 2 vectors.  A = u * v^T.\n",
    "Since u = a * u_length1, and v = b * v_length1, then A = (a * b) u_length1 * v_length1^T.\n",
    "\n",
    "QR decomposition:  We have matrix A, we want matrices QR == A.\n",
    "The first thing we do is Gram-Schmidt on the columns of A.  This will give us Q.\n",
    "\n",
    "For each column vector a_i in A, (except for the first one, i=1), there is a q_i that corresponds to it.  This q_i is the 'orthonormal' version of a_i.  The formula is like this:\n",
    "\n",
    "a_i - (a_i^T * q_1) * q_1 - ... - (a_i^T * q_i) * q_i = L2-norm(Q_i) * q_i.\n",
    "\n",
    "Each - (a_i^T * q_?) * q_? is a step in the Gram Schmidt normalization process.  So once we do all the subtractions, we'll be left with Q_i, the part of a_i that is orthonormal to q_1, ...q_(i-1).  Then we normalize Q_i to get q_i.  \n",
    "\n",
    "Q_i is q_i, but not normalized.  So L2-norm(Q_i) * q_i = Q_i.  I don't know why they have this silly notation in the livebook, but whatever.\n",
    "\n",
    "Now, what if we put all the subtraction stuff on the other side?\n",
    "\n",
    "a_i = (a_i^T * q_1) * q_1 + ... + (a_i^T * q_i) * q_i L2-norm(Q_i) * q_i.\n",
    "\n",
    "So each (a_i^T * q_?) * q_? = r_?i * q_i.  So since the first thing, a_1, has nothing before it, it only has 1 non-zero value in its column.  a_2 has 2 non-zero values in its column, and so on.  We could either define our R matrix by filling its columns up with r's to correspond with the q's, but then we would need to multiply by R^T, I think.  So instead we fill up by rows, and R goes on the right of Q.\n",
    "\n",
    "So Q is a matrix of orthonormal vectors.  If Q is square, then it's an orthogonal matrix, so Q * Q^T = I, and Q^T * Q = I as well.  I don't kno why Q * Q^T = I.  There's a proof somewhere.  Rows and columns and whatever.\n",
    "\n",
    "And R is an upper triangular matrix (because r1 has 1 non-zero, r2 has 2 non-zeros, etc).\n",
    "\n",
    "\n",
    "Frobenius norm:  A norm for a matrix.  Just square all the entries and add them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Least Squares (OLS):  choose the x vector that minimizes the length of the vector Ax-y.\n",
    "\n",
    "This is the same as minimizing L2-norm(Ax-y), since the L2-norm is just the length squared.\n",
    "\n",
    "We don't bother square rooting it because it's more work and there's no point.\n",
    "\n",
    "Think of A as a plane (or the range of A), and y as a vector not on A's range.  We want to find the vector in A's range that is closest to y.  So we want to find the projection of y onto the plane that is A's range.\n",
    "\n",
    "The formula for this is:\n",
    "\n",
    "x\\* = (A^T * A)^-1 * A^T * y.\n",
    "\n",
    "This is actually equivalent to QR Decomposition.  Why?  I don't know.  Should probably look at it later.\n",
    "\n",
    "Also, this assumes that A is 'tall', meaning m $\\geq$ n.  So it's underdetermined or something.\n",
    "\n",
    "There's also a bunch of special cases of least squares that I'm not going to go into.\n",
    "\n",
    "Least squares is for drawing a line that approximates a bunch of points.  L2-norm(Xw-y).  If the points are all in a row and can be perfectly fit to a line, then Xw=y.  The rows of X are the data points, w is the weights, and y is the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral Theorem:  If A is a symmetric matrix, (symmetric matrices are always square), and it is n by n in dimension, then A has n eigenvalues.  For each of the n eigenvalues, their eigenvectors are orthogonal.\n",
    "\n",
    "Cool thing:  For _any_ matrix A, B = A * A^T and C = A^T * A are both symmetric.\n",
    "Also, the non-zero eigenvalues of B and C will be the same.\n",
    "Say A is 500x3 in dimension.\n",
    "Then B has 500 eigenvalues, and C has 3.\n",
    "Since B and C's non-zero eigenvalues are the same, we just have to find C's 3 eigenvalues.  Then they will be the same as B's non-zeor eigenvalues.  So then we know the other 497 eigenvalues of B are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**  \n",
    "\n",
    "If you have 1000 dimension data, doing any calculations on that data will take a long time.  \n",
    "\n",
    "PCA orders your dimensions in terms of how important they are.\n",
    "\n",
    "Then you can keep the K most important dimensions and get rid of the rest.\n",
    "\n",
    "This makes your calculations take much less time.\n",
    "\n",
    "\n",
    "\n",
    "In your data matrix, each column is a dimension, and each row is a data point.\n",
    "\n",
    "The first think you do is get the 'sample mean vector', or just sample mean.  We'll call it $\\hat{u}$\n",
    "\n",
    "Make it by averaging each column.  Then it's the 'average row'.\n",
    "\n",
    "Now do ($\\hat{x}$-$\\hat{u}$)($\\hat{x}$-$\\hat{u}$)^T where each $\\hat{x}$ is a row.  Average all the matrices this creates.\n",
    "\n",
    "This is the covariance matrix.\n",
    "\n",
    "Find all the eigenvalues of the covariance matrix, and each eigenvector associated with them.\n",
    "\n",
    "Now choose the K largest eigenvalues, and each eigenvector associated with them.\n",
    "\n",
    "Remember that an eigenvector $\\hat{w}$ means X\\*$\\hat{w}$=a$\\hat{w}$, where a is a scalar.  So if a=8, then the eigenvector associated with $\\hat{w}$ is 8, and $\\hat{w}$ will get 8 times bigger whenever you multiply it with X.\n",
    "\n",
    "So you take each of the K best eigenvectors, and you normalize them.  That is, you make them all have length 1.\n",
    "\n",
    "Each eigenvector has the same number of elements as each row of X.\n",
    "\n",
    "So then for each row of X, you take the dot product with each eigenvector, starting with the one that has the largest eigenvalue, then the second largest, etc, until you have K dot products.\n",
    "\n",
    "Put all of these dot product scalars into a vector.\n",
    "\n",
    "This is your new transformed data point, which is a row vector.  It has K dimensions, instead of 1000.\n",
    "\n",
    "This is a change of basis.  For each row, you dot it with K eigenvectors to produce a new row that has only K elements.\n",
    "\n",
    "Since you're just doing a bunch of dot products, you can stack the eigenvectors into a matrix and do it all at once for each of your data points (row vectors in X).\n",
    "\n",
    "How do you know what to choose for K?\n",
    "\n",
    "With this fraction:\n",
    "\n",
    "$\\frac{e_1+....+e_K}{e_1+....e_N}$\n",
    "\n",
    "The denominator is the sum of all the eigenvalues.  Just choose eigenvalues until you have some fraction that you think is good enough.  Most people stop around 90-95% and say that's good enough.  It's basically a percentage saying 'I have x% of the original information of my data points and that's good enough.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Trick**\n",
    "\n",
    "You don't have enough dimensions in your data, and you want more.  You can make fake dimensions by doing stuff like taking 1 of your dimensions and squaring it, then appending it to each data point.  Now each data point has 2 dimensions.  \n",
    "\n",
    "A kernel function takes in 2 raw (unlifted) data points and returns the dot product of the lifted versions of those data points, without actually lifting them.  So you could do something with the gaussian distribution or whatever (which has infinite dimensions), and do the kernel trick on data points with the gaussian distribution as the kernel function.  You can't actually give data points infinite dimension, but the kernel trick can output a number that is the same as the dot product of 2 infinite dimensional vectors.  So that's what the kernel trick is.\n",
    "\n",
    "compatible with PCA.  Get rid of useless dimensions.  Oh no, you only had like 2 useful dimensions.  Now we use the kernel trick to add more dimensions that actually mean something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up of data with matrices**\n",
    "\n",
    "Say your data is a bunch of people.  For each person, you have their age, weight, and height.  We think that age is a function of their height and weight.\n",
    "\n",
    "What we do is put all the ages into the vector $\\hat{y}$.  Then we make the heights and weights the columns of the matrix X.\n",
    "\n",
    "Now we want to find $X\\hat{w} = \\hat{y}$.  $\\hat{w}$ is the linear function that let us find someone's age given their weight and height.\n",
    "\n",
    "So if we have the weight column vector, the height column vector, and the age column vector, we want to find out how to combine the weight and height vectors to create the age vector.  Some linear combination, that is.\n",
    "\n",
    "The problem is that the age vector is probably _not_ actually a function of weight and height.  It's almost certain that $X\\hat{w} = \\hat{y}$ will have 0 solutions.\n",
    "\n",
    "What we do is find a linear approximation.  There are M people, so the height, weight, and age vectors all live in M dimensional space.  The height and weight vectors together create a 2d plane of possibilities in M dimensional space.  What are the odds that the age vector is in that plane?  Pretty much 0.  If you had a 2d plane in 3d space, the odds of a random 3d vector being in that plane are pretty much 0.  So it'll be the same thing in M dimensional space.\n",
    "\n",
    "However, the age vector can be projected onto this plane.  The age projection is a linear combination of height and weight.\n",
    "\n",
    "So for any new height and weight we get, we'll say 'based on this height and weight, I'm going to guess that the persons age is this projection of the age vector that we had before.'  We know it's not perfectly accurate, but it is fast.  Maybe we could instead say it's a polynomial relationship of higher degree, but then we risk overfitting to our current data, and making our overall model _less_ accurate even though we made it fit better to our current data.\n",
    "\n",
    "So any new x=(weight, height), we can try to predict the age $\\hat{y}$ like this:\n",
    "\n",
    "$\\hat{y} = w^* x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinary Least Squares**\n",
    "\n",
    "So now that we know we want to find the projection of $\\hat{y}$ onto the columns of X, how do we actually do that?\n",
    "\n",
    "$min_w\\ \\ ||Xw-y||_2^2$\n",
    "\n",
    "If the columns of X are linearly independent, there's only 1 way to combine them to get the projection of $\\hat{y}$.  If they're dependent, there are multiple ways.  Do row reduction to find out if the columns are independent enough.  If at the end of row reduction you have at least N independent rows, and you have N columns, then the columns are independent.  So if this is true, there's a unique solution, and you can solve for $proj_y$ with a formula.\n",
    "\n",
    "There's some proof that shows that row reduction is equivalent to column reduction, so doing row reduction can show the columns are independent.  I don't know how it works.  Maybe come back to it.\n",
    "\n",
    "In data 100, they say 'you don't need to know the calculus based derivation.  You should know the geometric derivation\n",
    "\n",
    "Anyway, there some way to derive the formula.  I don't know how to derive it.  Here's the formula that will give you the minimum w:\n",
    "\n",
    "$w^{*}=(A^T A)^{-1} A^T y $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Least Squares**\n",
    "\n",
    "If your matrix doesn't have enough non-zero rows after row reduction, the formula won't work.  In that case, you add a parameter to force it to work.  This parameter should be really small and insignificant so it doesn't affect the answer too much.  Regularized least squares is also used to reduce overfitting when you are training a model.\n",
    "\n",
    "$min_w\\ \\ ||Xw-y||_2^2 + \\lambda ||w||_2^2$\n",
    "\n",
    "Here's the answer to the above formula:\n",
    "\n",
    "$w^{*}=(A^T A + \\lambda I_n)^{-1} A^T y $\n",
    "\n",
    "You find the value of $\\lambda$ by literally just guessing.  You randomly choose a $\\lambda$, and then you either make it higher or lower.  If the higher value gives lower error, you keep going higher.  If the lower value gives lower error, you keep going lower.  Do this until you get a $\\lambda$ that minimizes the _validation_ error.  The whole validation set thing is talked about in data 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix is just a bunch of vectors stacked together.\n",
    "\n",
    "So you need to understand vector vector stuff.\n",
    "\n",
    "Then matrix vector stuff should fall out of that.\n",
    "\n",
    "Then matrix matrix stuff should fall out of that.\n",
    "\n",
    "A vector is a list of numbers. Perhaps this list of numbers represents an object.\n",
    "\n",
    "Perhaps it represents a function.\n",
    "\n",
    "Maybe it's a linear function, maybe not.\n",
    "\n",
    "There is an implicit knowledge of the variables for each thing.\n",
    "We know the first thing is x_0, the second is x_1, etc.\n",
    "We only need to know the coefficients.\n",
    "But what one of them is 2^x, one of them is x^2, whatever?\n",
    "How would a single number be able to encapsulate that?\n",
    "Are all scalar functions 2d vectors?\n",
    "\n",
    "Could you represent f(x)=2x with a vector?\n",
    "\n",
    "\n",
    "what about f(x)=x^2?\n",
    "\n",
    "Also should understand row reduction.  n equations in n unknowns.\n",
    "This format: x1+x2=3, and how it's different from function notation.\n",
    "Function notation _assumes_ you're solving for a particular variable.  This doesn't.\n",
    "\n",
    "1x + 2y = 3\n",
    "4x + 5y = 6\n",
    "\n",
    "If this can be put into matrix notation, then why if we multiply by a vector, that vector is a column?\n",
    "It makes no sense.  If the vector is 2d, and v_0 is x, and v_1 is y, then it should line up.\n",
    "But it doesn't.  Why???\n",
    "\n",
    "Maybe you need to understand linear maps?\n",
    "\n",
    "We can think of measuring a vector by putting a ruler up to it and measuring it.  Why is the L-2 norm equivalent to doing this?  It's like we put a ruler up to each component of the vector.  Why does this triangle thing work?  Maybe it's because you only need to 'look' at 2 dimensions at a time.  If you look at 2 dimensions and create a vector that puts them together, now you have 1 less dimension to worry about.  Then you just do this over and over to get your answer.  What shape would even be created when adding 3 vectors together at once?  Oh, and also these vectors are all perpendicular, because you're looking at each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EigenDecomposition**\n",
    "\n",
    "This is the reason we care about determinants and eigenvectors and eigenvalues and whatever.\n",
    "\n",
    "If we have a square matrix A, it might be possible to get it in this form:\n",
    "\n",
    "$A=PDP^{-1}$\n",
    "\n",
    "Where D is the diagonal matrix of all of A's eigenvalues.\n",
    "\n",
    "This is special because it means:\n",
    "\n",
    "$A^{1000}=PD^{1000}P^{-1}$\n",
    "\n",
    "And since D is just diagonal, for each eigenvalue we do $\\lambda^{1000}$.\n",
    "\n",
    "This is much easier than doing regular matrix multiplication.\n",
    "\n",
    "You might think this is kind of a specific case.  What if the matrix isn't square?\n",
    "\n",
    "Well, if the matrix isn't square then you wouldn't be applying it 1000 times in the first place, right?\n",
    "\n",
    "This kind of thing is used a lot for time step matrices.  Speaking of, here's the inverse:\n",
    "\n",
    "$A^{-1}=(PDP^{-1})^-1=PD^{-1}P^{-1}$\n",
    "\n",
    "So with this, you can just as easily go backwards as you can go forwards.  The inverse of D is just $\\frac{1}{\\lambda}$ for everything on the diagonal.\n",
    "\n",
    "But how do you know when a square matrix has an eigendecomposition?\n",
    "\n",
    "Say A is n by n.  A will have an eigendecomposition if it has n non-degenerate eigenvalues.  This will mean A has n linearly independent eigenvectors.  Well, technically infinite, but whatever.\n",
    "\n",
    "http://mathworld.wolfram.com/EigenDecomposition.html\n",
    "\n",
    "Proof is here.  It's pretty intuitive, in my opinion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dyads**\n",
    "\n",
    "Say you have these two vectors:\n",
    "\n",
    "$\n",
    "x \\in R^m \\\\\n",
    "y \\in R^n \\\\\n",
    "$\n",
    "\n",
    "And you do this: $xy^T$.\n",
    "\n",
    "This will create the matrix A, which will look like this:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    " x_1y_1 & \\dots & x_1y_n \\\\ \n",
    " \\vdots & \\ddots & \\vdots \\\\ \n",
    " x_my_1 & \\dots &x_my_n \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Each column is a vector.  The first column vector in A is just the vector x scaled by $y_1$.  The second vector is x scaled by $y_2$.  The last vector is x scaled by $y_n$.\n",
    "\n",
    "Since every column of A is just some scaled version of the vector x, then A will send all n-vectors to some version of x.  So for all n-vectors z, Az=?x, where ? is some scalar value.\n",
    "\n",
    "This means The rank of A is 1, and its nullspace is everything orthogonal to x.\n",
    "\n",
    "Now let's say you have another dyad, $B=vw^T$.\n",
    "\n",
    "So A sends all vectors to ?x, and B sends all vectors to ?v.\n",
    "\n",
    "Now let's say we have C=A+B.\n",
    "\n",
    "Then Cz=(A+B)z=Az+Bz=?x+?v.\n",
    "\n",
    "So if C is a sum of dyads, then you know it's a rank two matrix whose column space is the 2-dimensional subspace of $R^n$ spanned by x and v.\n",
    "\n",
    "Dyads supposedly have a lot of applications, like in image and video compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# This is now a cheatsheet\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$x^TAx$\n",
    "\n",
    "Since you have $x$ and $x^T$, just say $x$ is n by 1, and 1 by n.  Then you know that $A$ is n by n.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$x$ is always a column vector.\n",
    "\n",
    "$x^T$ is always a row vector.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "All loss functions are L(y_predicted, y_actual).\n",
    "\n",
    "If you see something like L(y_actual, x, theta, blah, blah), that means the prediction function has been embedded into the loss function.  The prediction function takes in x, theta, blah, and blah, and spits out y_predicted, which is hidden from you.\n",
    "\n",
    "$\n",
    "L(\\theta, x, y) = \\frac{1}{n} \\sum_{i=1}^n (\\theta^2x_i^2-log(y_i))\n",
    "$\n",
    "\n",
    "Here's an example.  x is our input vector, y is the vector of actual values, and $\\theta$ is the singular weight we can change.  I don't really know what exactly our prediction function is, but it's in there.... somewhere.\n",
    "\n",
    "The loss function is the objective function that we are trying to minimize, like from convex optimization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can't divide by a matrix.\n",
    "\n",
    "$\n",
    "AA^{-1} = I \\\\\n",
    "A^{-1}A = I \\\\\n",
    "$\n",
    "\n",
    "You might think you could just do A / A.\n",
    "\n",
    "But consider multiplying the inverse by some other matrix B.\n",
    "\n",
    "You might think you could say $B/A=BA^{-1}$, but that won't work, because how would you say $A^{-1}B$?  You couldn't do it with a division operator.  That's why we don't use the / notation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$x^Tx$ is a 1 by 1 matrix, a scalar.\n",
    "\n",
    "$xx^T$ is an n by n matrix.\n",
    "\n",
    "Maybe exploring this further will give you some insight into the dot product, or transposes, or vector multiplication in general?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "A is a matrix.  Upper case letters are matrices.\n",
    "\n",
    "x is a vector.  Lower case letters are vectors.  Scalars are just 1 by 1 vectors.\n",
    "\n",
    "For any equation of matrices and stuff, you can assume their dimensions will match up.\n",
    "\n",
    "$w^T(X^TX-xx^T)w + y^Ty$\n",
    "\n",
    "just say that w is n by 1.\n",
    "\n",
    "Then $w^T$ has to be 1 by n.\n",
    "\n",
    "Then the thing in the middle, $X^TX-xx^T$, has to be n by n.\n",
    "\n",
    "So $X^TX$ must be n by n.\n",
    "\n",
    "So X has to be m by n, and $X^T$ has to be n by m.\n",
    "\n",
    "x and y have to be n by 1, and $x^T$ and $y^T$ have to be 1 by n.\n",
    "\n",
    "In a lot of problems, they'll say stuff like 'assuming X is square....'.  In this case, m == n, since X is square.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For a matrix, it's number of rows by number of columns, NOT size(r) by size(c).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An orthogonal basis is best because you can project onto this basis easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Data 100, discussion 6, question 2.\n",
    "\n",
    "$\\Phi^Te=0$ because e is perpendicular to the columns of $\\Phi$, so e's dot product with the columns is 0.  Maybe this example will help you with transposes, or row space, or something.\n",
    "\n",
    "If you can't find it, the question is called 'Geometry of least squares.'\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Not all square matrices have inverses.\n",
    "\n",
    "[0] is a 1 by 1 matrix.  It has no inverse, right?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Some dyad stuff:\n",
    "\n",
    "$\n",
    "p, q \\in R^n \\\\\n",
    "||p||_2=1, ||q||_2 = 1 \\\\\n",
    "A=pq^T+qp^T \\\\\n",
    "$\n",
    "\n",
    "Also, p and q are independent.\n",
    "\n",
    "So p and q are n-vectors of size 1, and A is a sum of dyads for them.  \n",
    "\n",
    "Now lets define $c=p^Tq$, which is a scalar instead of a matrix.  This will be useful in the next calculations:\n",
    "\n",
    "$\n",
    "c=p^Tq \\\\\n",
    "A(p+q)=(c+1)(p+q) \\\\\n",
    "A(p-q)=(c-1)(p-q) \\\\\n",
    "$\n",
    "\n",
    "So the eigenvectors of A are p+q and p-q, and their eigenvalues are c+1 and c-1.\n",
    "\n",
    "Ugh, I'm tired of typing this.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Row_rank(A) == Column_rank(A)\n",
    "ALWAYS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "all dimensions are equally important for all norms.  As in they all have equal weighting.  For SVD, you were thinking 'what if you dropped a dimension that you thought was more important?' But you shouldn't think like that, because if that was the case then we couldn't be doing any of the things we're doing right now.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Actually, if a matrix M is a dyad, then M \\* ? will always be a dyad, although maybe it will have a different number of columns.  The reason for this is that any multiplication will only result in a bunch of columns that are multiples of the columns of M.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you know when a matrix is symmetric?\n",
    "\n",
    "$u^TRR^TL^TLR^TRu$\n",
    "\n",
    "Between the two u's here is a symmetric matrix.  Think of a palindrome.  If it looks like a palindrome, it's probably symmetric, and you should test to see if it is by taking the transpose and seeing if it's the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ingredients:  flour eggs milk butter sugarcube\n",
    "\n",
    "recipes:  bread cake muffin crepe omelette pancake\n",
    "\n",
    "nutrition:  carbs proteins fats sugars\n",
    "\n",
    "flour = 2carbs + 0proteins + 0fats + 0sugars\n",
    "\n",
    "egg = 0carbs + 1proteins + 1fats + 0sugars\n",
    "\n",
    "milk = 0carbs + 1 protein + 2fats + 3sugars\n",
    "\n",
    "butter = 0carbs + 0proteins + 2fats + 0sugars\n",
    "\n",
    "sugarcube = 0carbs + 0proteins + 0fats + 3sugars\n",
    "\n",
    "\n",
    "\n",
    "bread = 3flour + 0eggs + 0milk + 0butter + 1sugarcube\n",
    "\n",
    "cake = 2flour + 2eggs + 2milk + 2butter + 2sugarcube\n",
    "\n",
    "muffin = 1flour + 0eggs + 0milk + 1butter + 1sugarcube\n",
    "\n",
    "crepe = 1flour + 1eggs +0milk + 0butter + 0sugarcube\n",
    "\n",
    "omelette = 0flour + 4eggs + 1milk + 1butter + 0sugarcube\n",
    "\n",
    "pancake = 2flour + 1eggs + 1milk + 1butter + 0sugarcube\n",
    "\n",
    "recipes <--- ingredients ---> nutrition\n",
    "\n",
    "$\n",
    "A= \\begin{bmatrix}\n",
    "- & carbs & proteins & fats & sugars \\\\\n",
    "flour & 2 & 0 & 0 & 0 \\\\\n",
    "egg &0 & 1 & 1 & 0 \\\\\n",
    "milk & 0 & 1 & 2 & 3 \\\\\n",
    "butter & 0 & 0 & 2 & 0 \\\\\n",
    "sugarcube & 0 & 0 & 0 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "The 'columnspace' of this matrix is the infinite number of macronutrient combinations we could make.\n",
    "\n",
    "The nullspace is ????\n",
    "\n",
    "The 'rowspace' of this matrix is the infinite number of ingredient combinations we could make.\n",
    "It's also the columnspace of the transpose.\n",
    "\n",
    "The 'left nullspace' is the nullspace of the transpose.  So rowspace and left nullspace are kind of the duals of the columnspace and nullspace.  A gets columnspace and nullspace.  A-transpose gets rowspace and left-nullspace.\n",
    "\n",
    "Left-multiplying a vector by a matrix is the same thing as right-multiplying by the transpose of the matrix, then transposing again.\n",
    "$x^TA = (A^Tx)^T$\n",
    "\n",
    "But why do we care?  Well, sometimes we don't.  In the above matrix, if you had a 4-vector of macronutrients, you could multiply by A and get .... what?  A vector, yes, but not one that really means anything.\n",
    "\n",
    "But if you left multiply by a row vector of goods that you have baked, you would get out a row vector of the total number of each macronutrient contained in your goods.  If you take the transpose of A, you will get a matrix that will do the same thing, except its output will be a column instead of a row.\n",
    "\n",
    "So the concepts of left-multiplying, rowspace, and left-nullspace are all contained within the transpose of a matrix.\n",
    "\n",
    "How about this: a scalar is a vector with only one real dimension.  A sector.  All scalars actually live in infinite dimensional space.  We're just looking at dimensions we care about.\n",
    "\n",
    "The flour sector gets transformed into 2 carb sectors.\n",
    "The egg sector gets transformed into both the protein and fat sector.\n",
    "\n",
    "So flour=2carbs + 0proteins + 0fats + 0sugars\n",
    "doesn't just mean 'flour is this vector'.  It means 'flour is a sector.  The flour sector is EQUIVALENT to 2 carb sectors.'  Kind of a subtle thing to wrap your mind around.  Maybe later this won't be interesting, but I think it's important now for your understanding of transformations.  It's the fact that flour isn't JUST a vector in macronutrient space.  It's a sector in its own space.  A sector defines a dimension.  Or something.  Every vector is its own vector space, its own dimension, its own whatever.\n",
    "\n",
    "Another way to put it is this:\n",
    "flour=2carbs + 0proteins + 0fats + 0sugars\n",
    "Alone, flour is a scalar.  \n",
    "But when you combine it with the other equations, notice that it's all the same variables.\n",
    "So now flour is instead the vector $[1, 0, 0, 0, 0]$.\n",
    "Each equation is actually a statement of translation for a unit vector whose dimensions are all 0's except 1.  \n",
    "A sector.\n",
    "Each equation describes translating a single sector into vectors of another space.\n",
    "Whether its a meaningful translation or not .... well, that's the same question as whether or not operations on scalars are meaningful.\n",
    "\n",
    "Oh, and just because the columnspace is the infinite number of macronutrient combos we could make, that doesn't mean each column is a vector corresponding to a single macronutrient.  Remember that this matrix doesn't mean anything.\n",
    "\n",
    "Instead, you should look at the transpose.\n",
    "\n",
    "$\n",
    "A^T= \\begin{bmatrix}\n",
    "- & flour & egg & milk & butter & sugarcube \\\\\n",
    "carbs & 2 & 0 & 0 & 0 & 0 \\\\\n",
    "proteins & 0 & 1 & 1 & 0 & 0 \\\\\n",
    "fats & 0 & 1 & 2 & 2 & 0 \\\\\n",
    "sugars & 0 & 0 & 3 & 0 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Here, your input vector is $[flour, egg, milk, butter, sugarcube]^T$.  If you increase the flour entry by 1, that corresponds to adding an additional first column into your sum when you matrix multiply.  So each column corresponds to the translation of an ingredient sector into macronutrients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should keep this cell preserved as it is.  This was your exploration of matrix multiplication that eventually took you to semi-groups and a bunch of other stuff.  It's an actual record of you learning something new.\n",
    "\n",
    "https://math.stackexchange.com/questions/31725/intuition-behind-matrix-multiplication\n",
    "Ok, so the answer is like 'why isn't it component wise?'\n",
    "Component wise means each element to each element.  That means only 2 matrices that have the _exact_ same dimensions could be multiplied.  If it was componentwise, you couldn't transform between dimensions.\n",
    "\n",
    "This answer isn't actually that great....\n",
    "\n",
    "Ok, so why is matrix multiplication defined like this?  Why not do something else?\n",
    "\n",
    "Matrix multiplication isn't actually 'multiplication'.  It's a composition of linear functions.  Say you had these 2 functions:\n",
    "\n",
    "$\n",
    "f(x) = x^2 \\\\\n",
    "g(x) = 4x+1 \\\\\n",
    "$\n",
    "\n",
    "You can compose these two functions like this:\n",
    "\n",
    "$\n",
    "f(g(x)) = (4x+1)^2 \\\\\n",
    "$\n",
    "\n",
    "It's the same thing with matrix multiplication.  So matrix multiplication isn't actually multiplication, it's composition of linear functions.  \n",
    "\n",
    "What does matrix multiplication do?  It composes linear functions.\n",
    "\n",
    "How does matrix multiplication compose linear functions?  By multiplying rows to columns.\n",
    "\n",
    "But rows to columns is weird.  Why do we do it like this?  Is there some easier way to define matrix multiplication?\n",
    "\n",
    "First, let's think of 'component-wise' multiplication.  What if that was matrix multiplication instead?  Let's try it:\n",
    "\n",
    "$\n",
    "C=AB \\\\\n",
    "C_{i, j}= A_{i, j} * B_{i, j} \\quad \\forall i, j \\\\\n",
    "$\n",
    "\n",
    "This is component-wise multiplication.  Here's an example:\n",
    "\n",
    "$\n",
    "A= \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "B= \\begin{bmatrix}\n",
    "5 & 6 \\\\\n",
    "7 & 8 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "C= \\begin{bmatrix}\n",
    "1*5 & 2*6 \\\\\n",
    "3*7 & 4*8 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "$\n",
    "\n",
    "This seems simpler than rows to columns, right?.  Here's the problem: under this definition, A and B always have to have the exact same dimensions.  If A is m by n, then B also has to be m by n.  \n",
    "\n",
    "So if A was m by n and B was n by p, then we couldn't put them together at all.  It would be impossible.  Matrix multiplication is supposed to compose linear functions, and component-wise can't do that.\n",
    "\n",
    "Say you had a p-vector named x, and wanted to transform it into an m vector.  Under normal matrix multiplication, you could say ABx.  AB takes in a p-vector and gives back an m-vector.  But with componentwise matrix multiplication, AB doesn't make any sense because the dimensions don't line up.  So component-wise matrix multiplication won't let you put A and B together in the same way you could put 2 scalar functions f and g together.\n",
    "\n",
    "Ok, so component-wise doesn't work.  What if instead we did rows to rows, or columns to columns?  Lining up rows with rows seems more natural than rows with columns.\n",
    "\n",
    "Let's think about row-row multiplication in comparision to row-column multiplication.\n",
    "\n",
    "It turns out that row-col-mult(A, B) is equivalent to row-row-mult(A, $B^T$).  \n",
    "\n",
    "Lets try rows to rows.  If we have A that is of dimension Here's an example:\n",
    "\n",
    "$\n",
    "A= \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6 \\\\\n",
    "7 & 8 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "B^T= \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "C= \\begin{bmatrix}\n",
    "1*1+2*2 & 1*3+2*4 & 1*5+2*6 \\\\\n",
    "3*1+4*2 & 3*3+4*4 & 3*5+4*6 \\\\\n",
    "5*1+6*2 & 5*3+6*4 & 5*5+6*6 \\\\\n",
    "7*1+8*2 & 7*3+8*4 & 7*5+8*6 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "$\n",
    "\n",
    "A is 4 by 2\n",
    "\n",
    "B is 3 by 2\n",
    "\n",
    "C is 4 by 3\n",
    "\n",
    "If you do A * B with row-column multiplication, you will also get C, the same result.\n",
    "\n",
    "It turns out that anything you can do with the rows to columns definition of matrix multiplication, you can also do with the rows to rows definition.  \n",
    "\n",
    "So rc-mult(A, B) == rr-mult(A, $B^T$).  However, there is a problem.\n",
    "\n",
    "What happens when we try to do 3 matrices?  A \\* B \\* C?\n",
    "\n",
    "Row-column multiplication:\n",
    "\n",
    "$(AB)C=A(BC)$\n",
    "\n",
    "Convert $(AB)C$ to row-row form:\n",
    "\n",
    "$\n",
    "AB \\rightarrow AB^T \\\\\n",
    "(AB)C \\rightarrow (AB^T)C^T \\\\\n",
    "$\n",
    "\n",
    "Convert $A(BC)$ to row-row form:\n",
    "\n",
    "$\n",
    "BC \\rightarrow BC^T \\\\\n",
    "A(BC) \\rightarrow A(BC^T)^T \\\\\n",
    "$\n",
    "\n",
    "If you start with A, B, and C and in row-column notation and convert them to row-row notation, then:\n",
    "\n",
    "$\n",
    "(AB)C=(AB^T)C^T=A(BC^T)^T=A(BC) \\\\\n",
    "$\n",
    "\n",
    "But this is only because you explicitly started from row-column form.  In general, under the row-row method, it is NOT true that:\n",
    "\n",
    "$\n",
    "(AB^T)C^T=A(BC^T)^T \\\\\n",
    "$\n",
    "\n",
    "You can try it out with another example.\n",
    "\n",
    "So row-column matrix multiplication has the associative property, but row-row matrix multiplication does not.\n",
    "\n",
    "For any expression in row-column form, you can find an _equivalent_ expression in row-row form.  However, associativity greatly simplifies things.  Now we're going to explore why associativity is important:\n",
    "\n",
    "Subtraction is not an associative operator, but we still use it.\n",
    "\n",
    "3 - (2 - 1) = 2\n",
    "vs\n",
    "(3 - 2) - 1 = 0\n",
    "\n",
    "You could replace each subtraction and get an equivalent expression that _is_ associative like so:\n",
    "\n",
    "3 + -(2 + -1)\n",
    "= 3 + -2 + 1\n",
    "= 2\n",
    "\n",
    "The middle expression here has all the information from before, but now it is associative.  You can put parenthesis anywhere and the expression will not change.\n",
    "\n",
    "Division is also not associative.  But I don't think you can apply the same trick.\n",
    "\n",
    "When is associativity good?  When is it bad?  Is that even a good question to ask?  Or is it like saying 'when is multiplication good, and when is multiplication bad?'\n",
    "\n",
    "I want to say that we care about associativity for the same reason that we defined PEMDAS in the first place: it's just more convenient.  It's a bookkeeping thing.\n",
    "\n",
    "Imagine we had the product abcde.  There are a bunch of ways we could insert parenthesis in here.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Associative_property\n",
    "Insert the tamari lattice image from wikipedia here.\n",
    "\n",
    "But if products weren't associative, each of these expressions might be different, and you couldn't get rid of parenthesis.  That's a lot of writing.  So associativity lets us drop parenthesis.\n",
    "\n",
    "I feel like there's a lot more to associativity than just convenience.\n",
    "\n",
    "Like with chain matrix multiplication, you want to choose the order in which to multiply the matrices together such that you do the least number of operations possible.  If matrix multiplication wasn't associative, you wouldn't have access to this algorithm that could potentially speed up your calculation by a lot.  So if we defined matrix multiplication as being rows to rows, then we would have to do some crazy weird manipulations like above with the subtraction every time we wanted to do it.  So we want associativity by default.  Not just something we can turn into another thing that is associative.\n",
    "\n",
    "So row-column is associative, row-row is not.  We can transform row-row into something that is associative whenever we need, but we should just do that by default since it will be less work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duality/KKT will not be on midterm 2, but will be on the final.\n",
    "\n",
    "Do the eta kappa nu, tau beta pi, and bcourses midterms.\n",
    "\n",
    "What is a singular value?\n",
    "\n",
    "How to get maximum singular value?\n",
    "\n",
    "How to do SVD?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVD: Singular Value Decomposition**\n",
    "\n",
    "For any invertible square matrix A, you can turn it into an eigendecomposition $PDP^{-1}$.\n",
    "\n",
    "What about if A isn't square?  Can you turn it into a form that's easier to work with?\n",
    "\n",
    "Yes, you can do SVD on A.\n",
    "\n",
    "SVD is usually in the form of a sum of dyads.  You can then drop some of these dyads to get an approximation of A.  Each dyad you drop makes the approximation less accurate, but also is more compact and takes less time to compute.  Keeping the first k dyads is a rank k approximation.  Here's what it looks like:\n",
    "\n",
    "$\n",
    "A=x_1y_1^T + .... + x_ny_n^T\n",
    "$\n",
    "\n",
    "Here, n is the rank of matrix A.\n",
    "\n",
    "Each $x_i, y_i$ is a pair of vectors that makes a matrix.  If A is not full rank, then many of the x's and y's will be 0-vectors, meaning you can get rid of them immediately, which will save space even if you don't do any approximating.\n",
    "\n",
    "However, when we write out an SVD, we usually don't have it in this form, since it's kind of hard to write out.  Instead, we have it in this form, which is easily translatable into the dyad sum form:\n",
    "\n",
    "$A=USV^T$\n",
    "\n",
    "The columns of U are the eigenvectors of $AA^T$\n",
    "\n",
    "The columns of V are the eigenvectors of $A^TA$\n",
    "\n",
    "U and V are orthogonal matrices.  Their columns are orthonormal, meaning length 1 and independent of each other.\n",
    "\n",
    "This means $U^T==U^{-1}$ and $V^T==V^{-1}$\n",
    "\n",
    "S is a diagonal matrix of singular values.  A singular value (notated by $\\sigma$) is the square root of an eigenvalue of $AA^T$ and $A^TA$. $AA^T$ and $A^TA$ have the same eigenvalues.\n",
    "\n",
    "Here's a short proof of that:\n",
    "\n",
    "$\n",
    "A^TAv=\\lambda_iv \\\\\n",
    "AA^TAv=A\\lambda_iv \\\\\n",
    "u=Av \\\\\n",
    "AA^Tu = \\lambda_iu \\\\\n",
    "$\n",
    "\n",
    "So any eigenvalue $\\lambda_i$ of $A^TA$ has an eigenvector v, and $AA^T$ will have a corresponding eigenvector u whose eigenvalue is also $\\lambda_i$.\n",
    "\n",
    "Anyway, $A=USV^T$ can be translated into $A=x_1y_1^T + .... + x_ny_n^T$.  \n",
    "\n",
    "S can be separated out into the sum of a bunch of matrices that all only have 1 non-zero entry.  Each non-zero entry is a $\\sigma_i$.  Here's an example:\n",
    "\n",
    "$\n",
    "S = \\begin{bmatrix}\n",
    "\\sigma_1 & 0 \\\\\n",
    "0 & \\sigma_2 \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & \\sigma_2 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Now let's look at $SV^T$.\n",
    "\n",
    "$\n",
    "SV^T = (VS^T)^T = (VS)^T = \n",
    "(\\begin{bmatrix}\n",
    "v_1 & v_2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "v_1 & v_2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & \\sigma_2 \\\\\n",
    "\\end{bmatrix})^T =\n",
    "(\\begin{bmatrix}\n",
    "\\sigma_1v_1 & 0 \\\\\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0 & \\sigma_2v_2 \\\\\n",
    "\\end{bmatrix})^T\n",
    "$\n",
    "\n",
    "This was kind of long-winded, but at the end you'll have the columns of V, where column i is multiplied by $\\sigma_i$.  Then the whole thing is transposed.  So the rows of $V^T$ have each been multiplied by $\\sigma_i$.  At the end you'll have a summation of matrices, where each matrix has 1 non-zero row.\n",
    "\n",
    "Then if you multiply U by each of these matrices, you'll get a summation of matrices that each have only 1 non-zero column.  Summing all these matrices, you get back A.  The i'th all-columns-are-0-exept-one is equivalent to the i'th column of U \\* $\\sigma_i$ \\* the i'th row of $V^T$.  Which means a scalar \\* a row vector \\* a column vector, which is a dyad.  So it's a summation of dyads.\n",
    "\n",
    "the difference between the approximation and the original A is ||A-A_app||_Frobenius.  \n",
    "\n",
    "Why does U come before V?  Why does $A=USV^T$ and not $A=VSU^T$?\n",
    "Remember V is an orthogonal, so the inverse is the transpose.\n",
    "\n",
    "Why does this work in the first place?  Why do these 'singular values' work at all?  Why are U and V made up of eigenvectors?\n",
    "Proof is in the David C Lay book.  It's a 'proof by construction' or something.  Look it up later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
