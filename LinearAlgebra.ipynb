{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear algebra\n",
    "`vectors` are lists of numbers.  Here's an example:  \n",
    "\n",
    "[1, 3, 5, 3.423].  \n",
    "\n",
    "In regular algebra, you used `scalar` numbers to describe things.  4 apples.  1 person.  3 slices of pie.  In linear algebra, you use vectors to describe things.  Each number in the list describes some characteristic of the thing you're describing.  We could say an apple is described by a vector where the first number describes the apples weight, the second describes its volume, the third describes its caloric content, and the fourth describes its acid content.\n",
    "\n",
    "In physics, vectors are often thought of as arrows in space, usually 2d or 3d space.  Each number in these vectors describes one of the d's (dimensions) in those spaces.  Really, it's the same either way.  You could interpret a physics vector as just a list of numbers, and you could interpret an apple vector as an arrow in 4-dimensional apple space.\n",
    "\n",
    "A scalar is really just a 1d vector.  4 apples in 1d apple space.  1 person in 1d person space.  3 slices of pie in 1d pie space.\n",
    "\n",
    "`linear combinations` are combinations of vectors.  Vectors can be stretched, squished, and added together.  \n",
    "\n",
    "You can stretch / squish a vector by multiplying it by a scalar.  \n",
    "\n",
    "You can add vectors together by adding their individual components together.\n",
    "\n",
    "The `span` of a set of vectors is all the new vectors you can make through linear combinations of the vectors in your set.\n",
    "\n",
    "`i hat, j hat, k hat` are simple vectors that are commonly used to make linear combinations.  i hat is for the x dimension, j hat for y, and k hat for z.  i hat and j hat are commonly used as the basis vectors of $R^2$, meaning that they span $R^2$.  i, j, and k are commonly used as the basis vectors of $R^3$.\n",
    "\n",
    "$$\n",
    "\\hat{i} = [1, 0, 0] \\\\\n",
    "\\hat{j} = [0, 1, 0] \\\\\n",
    "\\hat{k} = [0, 0, 1] \\\\\n",
    "$$\n",
    "\n",
    "Think of a robot that can only move by deciding to move either $\\hat{i},\\hat{j},$ or $\\hat{k}$, or some fraction of anyof them.  It could go anywhere in 3-dimensionsal space.  If you think of a set of vectors as the decisions of a robot, then all the places the robot can go is the span of that set of vectors.\n",
    "\n",
    "`basis vectors` It turns out that $\\hat{i},\\hat{j},$ and $\\hat{k}$ are special vectors, because they are `linearly independent`.  They have nothing in common in terms of where they can point.  They are all `orthogonal` to eachother.  Consider the vector $\\hat{g}$ = [0, 1, 1].  $\\hat{g}$ is independent from $\\hat{i}$, but not independent from $\\hat{j}$ or $\\hat{k}$.  The three vectors $\\hat{i}$,$\\hat{j}$,$\\hat{g}$ are a spanning set of $R^3$, but they are not a set of basis vectors for $R^3$, because they are not all linearly independent from eachother.  Basis vectors are easier to make linear combinations with.\n",
    "\n",
    "Any vector in $R^2$ can be made with $\\hat{i}$ and $\\hat{j}$.  For instance, v=[-1, 2] is the same thing as $v=-1*\\hat{i}+2*\\hat{j}$.\n",
    "\n",
    "Imagine if your robot wanted to move 1 space in the z direction.  The only way to do that would be to select $\\hat{g}$.  But then it would also have to move -1*$\\hat{j}$.  This robot is basically z-disabled.  Look at what you've done.  You've crippled your robot.\n",
    "\n",
    "<details><summary>But this robot is more efficient when it's moving in $\\hat{g}$-like directions.  How is this bad?</summary>\n",
    "We want to be equally efficient in moving in any direction, because we're equally likely to move in any direction.  Linearly independent basis vectors are equally efficient no matter which direction you're going.\n",
    "</details>\n",
    "\n",
    "`linear transformations` A transformation on a vector just means to change that vector into another vector.  A linear transformation is a transformation that only rotates/stretches/squishes the vector.  No curving.\n",
    "\n",
    "In linear algebra, all vectors are some combination of basis vectors.  In $R^2$, it's $\\hat{i},\\hat{j}$.  In $R^3$, it's $\\hat{i},\\hat{j},\\hat{k}$.\n",
    "\n",
    "To linearly transform a vector, just transform it's $\\hat{i}$ and $\\hat{j}$ components, then combine them.\n",
    "\n",
    "Say we have a vector $\\vec{p}=x*\\hat{i}+y*\\hat{j}$.  A linear transformation changes $\\hat{i}$ and $\\hat{j}$.  Now the new $\\vec{p}=x*(transformed\\ \\ \\hat{i})+y*(transformed\\ \\ \\hat{j})$.  \n",
    "\n",
    "`matrix` A matrix is a function that does linear transformations on vectors.  They look like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "i_1 & j_1 \\\\\n",
    "i_2 & j_2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first column represents the transformed $\\hat{i}$, and the second represents the transformed $\\hat{j}$.  \n",
    "\n",
    "This matrix says \"turn $\\hat{i}=[1, 0]$ into $\\hat{i}=[1, 2]$ and turns $\\hat{j}=[0, 1]$ into $\\hat{j}=[3, 4]$\":\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 \\\\\n",
    "2 & 4 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "What if you wanted to make a matrix for $R^2$ that rotates all vectors 90 degrees left?  Just rotate $\\hat{i}$ and $\\hat{j}$ by 90 degrees each.  Now you have a matrix that describes how to rotate any vector in $R^2$ 90 degrees left.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since all vectors in $R^2$ are just some combo of $\\hat{i}$ and $\\hat{j}$, by rotating these 2 vectors, you've essentially rotated all the other vectors too.  Note that you could do this equally well for some other set of basis vectors.\n",
    "\n",
    "Could you do the same thing with some other set of basis vectors?  What about a spanning set that isn't a basis set?  What does it mean to have a matrix of vectors that doesn't consist of basis vectors?  What about basis vectors that aren't $\\hat{i}$ and $\\hat{j}$?  Wouldn't that be a transformation that actually does stuff?\n",
    "\n",
    "<details><summary>Why is matrix multiplication rows to columns?</summary>\n",
    "If we look at how A*x should be formatted, the first thing we think is that x should be vertical, not horizontal.\n",
    "    \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 \\\\\n",
    "2 & 4 \n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "5 \\\\\n",
    "6 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This seems fine.  But what if x was horizontal?\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 \\\\\n",
    "2 & 4 \n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "5 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This version of matrix multiplication wastes whitespace.  Specifically the extra space above and below x.  So the quintessential vector is vertical, not horizontal.  So each item in the vector x will either be multiplied by an entire row in the matrix, or an entire column in the matrix.\n",
    "\n",
    "While it seems to me that multiplying each item in the vector by a row is more intuitive, there is something to be said about multiplying by columns.  If our x vector is a column, it is more thematically consistent to have $\\hat{i}$ and $\\hat{j}$ be columns in A, so that they are similar to x.\n",
    "\n",
    "The takeaway from this is that we could have defined it some other way.  There is no deep meaning to how we have defined matrix multiplication other than 'it looks pretty'.\n",
    "\n",
    "</details>\n",
    "\n",
    "`determinants` are how much a matrix 'expands' or 'contracts' the space that it transforms.  It doesn't really make sense to think about on a single vector.  Let's look at the below matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 0 \\\\\n",
    "0 & 2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, $\\hat{i}$ gets stretched by a factor of 3, and $\\hat{j}$ gets stretched by a factor of 2.  If you think of $\\hat{i}$ and $\\hat{j}$ as making a little 1x1 box before, now they make a 2x3 (j comes before i) box, so the determinant is 6.  Again, the determinant is how much the matrix changes space.  It is the ratio of the new area to the old area of the box made by i and j.  Since the ratio of the old area is always 1, the stretching is always (new area)/(old area) = (new area) / 1 = (new area).  So the determinant is the area of the new box.\n",
    "\n",
    "But it's more complicated than that.  With the matrix: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 0 \\\\\n",
    "0 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Our 2d space is now a 1d space.  Any time you get rid of dimensions, the determinant is 0.\n",
    "\n",
    "The determinant can also be negative.  \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 & 0 \\\\\n",
    "0 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "An area can't be negative, and that's kind of where our 'area' analogy breaks down.  Think of a negative determinant as our square flipping over onto its backside, since j is now 'on the right' of i instead of 'on the left'.\n",
    "\n",
    "Imagine you had a 2d picture of mario's face, completely symmetric except for a single mole on his right cheek.  If the determinant is negative, mario's face flips, and the mole is now on his left cheek.\n",
    "\n",
    "Only square matrices have determinants.  What would it mean to 'expand' a 2 dimensional square into 3 dimensions via a 3x2 matrix?  What is its area now?  That doesn't even make sense, because now the square is no longer a square, it's some 3d shape.  It has volume now, not area.\n",
    "\n",
    "Speaking of volume, the determinant for volume can also be negative using the same 'mario mole' example.  But it is slightly more complicated.  If any 2 of i, j, and k switch places, the determinant will be negative.  If we get 3 shifts in some way, the mole will still be on mario's right cheek and the determinant will be positive.  There's some generalization of this, but I don't care enough to think about it.\n",
    "\n",
    "Determinant formula:\n",
    "\n",
    "$$\n",
    "det(\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d \n",
    "\\end{bmatrix}) = ad-bc\n",
    "$$\n",
    "\n",
    "Consider the case where b and c are 0.  In this case, i and j are just scaled along their respective axes.  If you add a positive b and c, both i and j are now pointing up and to the right.  The closer i and j get, the more the determinant rhombus thing gets squeezed, and the less area it has.  So b and c combine to form a kind of 'negative rhombus' that subtracts from the total rhombus of the determinant.  Of course, if b\\*c turns out to be negative, the determinant will increase in value.\n",
    "\n",
    "The determinant tells us how much the vectors distances relative to eachother change.  If the determinant of a matrix is 6, all vectors that were originally 1 away from each other will now be 6 apart.  All vectors that were 2.5 away from eachother will not be 15 apart.\n",
    "\n",
    "For now, skip trying to reason out why this formula works exactly.  Also skip for 3d and onward.  Maybe come back to it.  But you can also decompose into triangular matrices for high dimension matrices.  Maybe that formula will be easier to explain.\n",
    "\n",
    "`rank` the number of dimensions in the matrix output.  A 3x2 matrix takes in 2-vectors and outputs 3-vectors, so the rank of that matrix is 3.  Unless it outputs a plane of vectors in 3-space, in which case it's rank is 2.  Or if it turns every vector into [0 0 0], in which case it's rank is 0.\n",
    "\n",
    "`column space` is the span of all the vectors that your matrix can output.  So it's the span of the columns, since your matrix outputs linear combinations of its columns.\n",
    "\n",
    "`null space` all the vectors your matrix turns into 0.  If it's any more than a single vector your matrix is squishing space.\n",
    "\n",
    "Also need inverse matrices, row space, whatever.\n",
    "\n",
    "`dot product` is a special kind of linear matrix transformation.  It's used to discover the relationship between 2 vectors v and w.  If the dot product is 0, they're perpendicular, if they're facing the same direction, it's positive.  If they're facing different directions, it's negative.  In general, the dot product is used to measure 'similarity' between 2 vectors.\n",
    "\n",
    "Remember all the vectors from 3blue1brown being folded onto a plane?  It's like that.  Notice that the arrow pointing perpendicular to the plane gets 'folded' by being squished down into nothing.  So here you're squishing a vector into another vector.  If that squished vector is 0, you know the 2 vectors are perpendicular.\n",
    "\n",
    "You take 2 vectors, say v and w, of the same dimension and matrix multiply them into a single number.  In this instance, either v or w could be considered the matrix, and the other the vector.  Really what this shows is that vectors are just matrices.  A vector is really a nx1 matrix, where the n is each individual actual dimension of whatever, and the 1 output is a single measure whatever it is.  It's kind of weird to think about for a lot of things.  I guess if you took the dot product between 2 apple vectors and they were perpendicular, they would be like, opposite apples or something?  But with an apple you can't have any negative attributes.  So that doesn't really make sense.\n",
    "\n",
    "It is kind of weird that the dot product is so 'asymmetric'.  That's covered in the video.  Maybe explain that later.\n",
    "\n",
    "`cross product` take 2 vectors v and w, and you can use the cross product to find a third vector p that is perpendicular to both v and w at the same time.  Also p's length will be equal to the rhombus created by v and w (as though v and w were i and j).  It's pretty simple to find a vector orthogonal to v, and simple to find a vector orthogonal to w, but in 3d there are infinite vectors orthogonal to v and w.  So how do you find a vector orthogonal to both v and w out of infinitely many vectors?  With the cross product.\n",
    "\n",
    "\n",
    "\n",
    "# unused\n",
    "\n",
    "This means that matrix vector multiplication will either be 'rows to rows' or 'rows to columns'.  As a side note, do NOT think of matrix vector multiplication as being 'rows to columns' or anything like that.  It is each individual part of the vector turning into a scalar and scalar vector multiplying a whole column of the matrix. \n",
    "\n",
    "Could potentially think of $\\hat{i}$ as 'the single unit of mass' and $\\hat{j}$ as 'the single unit of volume'.  Everything is just combinations of these single units that make up a whole that is an apple vector.\n",
    "\n",
    "But what about the vectors $\\hat{g}$ = [2, 1] and $\\hat{h}$ = [1, -2]?  Those actually also have nothing in common.  While the individual vectors are a little weird, this robot is just as efficient as our ijk robot.  So how do you tell if 2 vectors 'have nothing in common'?  With the dot product.\n",
    "\n",
    "`dot product` is very flexible.  One thing it can do is tell you whether 2 vectors are orthogonal.  Here is the formula:  \n",
    "\n",
    "$\\hat{g}*\\hat{h} = g_1*h_1+g_2*h_2+g_3*h_3+...$\n",
    "\n",
    "So the dot product is a function that turns 2 vectors into a scalar.  If the dot product of 2 vectors is 0, those vectors are orthogonal.\n",
    "\n",
    "Dot product is just a regular matrix transformation.  But it's the vector that is the matrix.  1 vector is the vector space that you're going to, and the other vector is the one being transformed into that space.  This works no matter which vector you pick.  Yeah, should probably hold off on this (and the 'but what about...?') question until after you explain matrix transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
