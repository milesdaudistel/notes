{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tier 0———————————————————————————\n",
    "Just skip directly to STOKE after you finish the class.  Working with search-based optimization will be the same no matter what language you’re working in.  It’ll all by LLVM bytecode.  This is the part that gets you excited.  Don’t put it off.\n",
    "\n",
    "Wait, this may be wrong.  In order to optimize things, you need to know what you’re optimizing.  You need to understand the whole thing before you go making changes to it.\n",
    "\n",
    "Tier 1———————————————————————————\n",
    "\n",
    "`What do you want to do?`\n",
    "You want a language that goes as fast as possible.\n",
    "How are you going to achieve that?\n",
    "The cloud allows anyone (with money) to easily access parallel computers.\n",
    "This means that all programs, compilers included, can be much more powerful.\n",
    "Things like stoke and klee can take advantage of this.\n",
    "Furthermore, stoke can take advantage of different hardware architectures.\n",
    "The cloud offers a lot of powerful architecture that you could not get otherwise.\n",
    "Therefore, stoke is not just optimized for parallelism, but also for varied hardware architecture.\n",
    "Compute clouds offer a lot of varied hardware architecture.\n",
    "It has never been easier to create a programming language.\n",
    "Moreover, there is no language that has been specifically optimized for the cloud.\n",
    "This is what you want to create.  A programming language, or more generally, a computing environment in which scientists are able to take full advantage of the cloud, both in terms of compilation time and in terms of the runtime code that it can produce.\n",
    "\n",
    "`Why do you need your own language?  LLVM optimizers work for any language, so why do you need your own front end too?`\n",
    "Because the smaller your language is, the easier it is to optimize.  Normal optimizers use a lot of heuristics on the language, but Stoke would basically do away with this.  All you have to do is specify your language to Stoke, and it will be able to optimize it.  All of those man-hours poured into C and the like are no longer an impossible hurdle to get over.  You can now have code as fast as C without having to use a bunch of guess-like optimization techniques.  So you’re no longer bound to the cumbersome syntax of C either.  Type inference, functions as data, easy vector operations, no need to adhere to 40 years worth of backwards compatibility.  With Stoke, making a new language doesn’t mean you have to sacrifice runtime efficiency.\n",
    "\n",
    "`What kinds of optimizations do you actually want to look into?`\n",
    "Stoke, as it currently is, as well as applied to GPU code.\n",
    "Automatic parallelization in terms of threading and multiple CPUs/GPUs\n",
    "Polly, which does data locality optimizations.\n",
    "Using SMT/SAT solvers for general optimizations.\n",
    "Using software like Klee, Cyclone, CCured, and SAFECode to automatically catch even more errors.\n",
    "Runtime optimizations.  Not really sure how the cloud can help here, but big data programs by definition take in a lot of varied data, and there is no other way to optimize for that other than at runtime.\n",
    "\n",
    "`Who is this for?`\n",
    "Scientific programmers who want to perform massive computations.\n",
    "Of particular interest is machine learning, which currently uses many heuristics, not only to get better answers but also to speed up the arrival at those answers.\n",
    "...Cryptocurrency miners.\n",
    "Perhaps it won’t be the main language that scientific programmers use, but maybe it could be used to create libraries that these languages could call.\n",
    "\n",
    "`How is this any different from Julia?`\n",
    "Julia was designed as ‘a fast scripting language’.\n",
    "This language will be designed specifically for compiling and running on massive parallel computers.  It will attempt to be the fastest language possible.\n",
    "\n",
    "Tier 2———————————————————————————\n",
    "\n",
    "`Why pay any attention to the syntax at all?  Why not stick purely to an optimizer that can take in any kind of language?`\n",
    "Because the more features your language has, the harder the search problem is.\n",
    "More features means potentially slower as well.\n",
    "Read the below link that talks about how tuples are much faster than lists in python. https://stackoverflow.com/questions/2174124/why-do-we-need-tuples-in-python-or-any-immutable-data-type\n",
    "The smaller your programming language, the smaller the surface area is that you need to optimize.\n",
    "\n",
    "\n",
    "\n",
    "https://www.reddit.com/r/rust/comments/6g8v6p/seer_symbolic_execution_engine_for_rust/\n",
    "Forget the actual question, just read some of the comments.  This is the same thing as klee.  There’s even a comment about ‘super compilation’.\n",
    "\n",
    "If you really think about types, they are pretty fundamental to a program.  The type of a variable defines what you can do to that variable.  The reason people think dynamic type systems are nice is because they don’t like writing out complicated types.  But with static, you don’t have to as long as you have type inference.  There’s another pretty big advantage to  static typing: it catches type errors at compile time rather than runtime.  This makes larger programs easier to debug.  Runtime errors are difficult.  Most people like dynamic type systems because they think they’re easier, but really static type systems are easier.  I feel like most proponents of these ‘easier’ paradigms are just swallowing the python pill whole, and assume that every feature of python is the definition of the simplest it could possibly be.\n",
    "\n",
    "There are so many different things you want to do.  How are you supposed to do all of this stuff?\n",
    "For now, it’s just about exploring.  You won’t be able to do all of this stuff.  But you do want to explore it, and that’s what matters.\n",
    "\n",
    "What tools are you going to use?\n",
    "Right now, LLVM.  More specifically, the Stoke and Klee projects.\n",
    "If we ever get to the frontend, we’ll use Antlr, probably.\n",
    "Can’t use Flex and Bison, those are GPL licensed.\n",
    "It’s important that we stick with BSD-like licenses.\n",
    "Remember the gcc vs clang thing at Apple.\n",
    "\n",
    "\n",
    "`What exactly do all the code correctness projects do for you?  Things like klee?`\n",
    "Besides making it easier to debug, it would allow your language to be based on much less safe constructs, which means they can be faster.  In other languages, they're afraid to implement 'unsafe' constructs because in the past they would lead to very difficult bugs.  But now you can have these unsafe constructs and be comfortable in the fact that they will be caught at compile time.\n",
    "\n",
    "https://mortoray.com/2018/08/07/sadly-i-must-say-goodbye-to-leaf-my-programming-language/\n",
    "This guy spent years of his life on this language called leaf, and is now abandoning it.\n",
    "\n",
    "\n",
    "http://llvm.org/\n",
    "See the OpenMP thing?  What does the word ‘runtime’ even mean?\n",
    "Oh snap there’s klee.\n",
    "\n",
    "http://www.zverovich.net/2016/05/13/giving-up-on-julia.html\n",
    "People say ‘oh, I don’t like Julia because it’s not as fast as they advertise’, but just look at how this person is benchmarking Julia.  Hello World?  Come on.  Of course Julia is going to be slow for tiny insignificant programs like this since it’s JIT compiled.\n",
    "\n",
    "Ok, so Julia is compiled just-in-time.\n",
    "https://agilescientific.com/blog/2014/9/4/julia-in-a-nutshell.html\n",
    "Also, notice that they say something like ‘metaprogramming can make julia faster than fortran’.  Faster to code, or faster to execute?  I’m pretty sure they mean faster to code, but would it have an effect on runtime performance?\n",
    "\n",
    "https://medium.com/@simplyianm/why-gos-structs-are-superior-to-class-based-inheritance-b661ba897c67\n",
    "Skip to ‘the fragile base class problem’.  I think this explanation is good.\n",
    "\n",
    "Rust used OCaml to implement its first compiler, then it was bootstrapped.\n",
    "\n",
    "Leo said when I asked about the guy at apple:  “He wasn’t on LLVM, but he was using an SMT solver, not sure which one.  But basically yeah searching for a sequence of assembly instructions than can do the same thing as another sequence but in fewer steps”.  Hm… Unrelated, Leo also said he had to use Lex and Yacc at Apple.  Why didn’t they use Antlr?  Much easier, basically the same license.  They use LLVM, which uses a pretty much identical license.\n",
    "\n",
    "https://www.ponylang.org/discover/#what-is-pony\n",
    "A language called Pony.  Ok, for real.  There are so, so many languages out there.  There IS one that does what you want.  Don’t start from scratch.  No one does that.  To paraphrase, “In Pony, performance is the most important thing besides correctness.”\n",
    "https://news.ycombinator.com/item?id=9482483\n",
    "Rust’s creator doesn’t like it.\n",
    "\n",
    "https://www.reddit.com/r/rust/comments/7qels2/i_wonder_why_graydon_hoare_the_author_of_rust/\n",
    "This is from the original creator of rust.  He burned out for a while.\n",
    "\n",
    "So STOKE takes the old program and just makes a new one that is supposedly equivalent, right?  It does the guess and check for each ‘basic block’, right?  Maybe it would be a good idea to rearrange the users program somewhat, so that the random guesser would have an easier time checking for equivalency.  Not just so that it’ll work faster, but maybe some arrangements of basic blocks will be easier to find a global optimum for.\n",
    "\n",
    "No Country For Old Men:  ‘You pick the one right tool for the job.’  Or something to that effect.\n",
    "\n",
    "I feel like the idea of ‘undefined behavior’ could potentially be very important in your language design.  Maybe it’s actually not important, but you should look into it.\n",
    "\n",
    "Important question you have to ask yourself:  how do you model a computer now?  Computers aren’t just a monitor, a cpu, caches, physical memory, buses, etc.  It’s more general than that.  It’s not just a cpu, there’s a whole range of chips that can perform computations.  And it’s not just 1 chip, it’s many chips.  How does machine code get translated in such a way that it fully utilizes the available hardware?  How do you build a system that can optimize for a machine that is constantly evolving and growing?  Kind of thinking about ASICs with this last part.  And a more general question:  how do you solve a problem, and keep it solved?  By this, I mean the fact that we tend to write code that does the same, or very similar things, over and over again.  Python is popular because it has 1 and only 1 solution for everything.  All of this research is doing that right now.  You’re solving problems that have already been solved.  How do you solve it in a way such that it will stay solved?\n",
    "\n",
    "Wait.  If a monte carlo optimizer just uses search to find the best solution, then it doesn’t need to ‘understand’ the problem, right?  As in, you don’t have to put in hours and hours of grinding on heuristics to make it work.  So you don’t need nearly as much knowledge of the architecture.  You don’t have to think about each individual case.  Here’s the thing:  wouldn’t this mean you could do the exact same with a GPU?  Get its instruction set, then you could include all the GPU instructions in all your random guessing.  Automatic GPU utilization, and you don’t even have to do much fancy stuff for it.  Damn, you’ve said this before, but all of this is going to sound really naive once you actually know what you’re doing.\n",
    "\n",
    "If your compiler just take the program and uses it as a test case or whatever, could you do Prolog stuff?  The program tells you what it wants, and how it wants you to do it, and you just throw away all the ‘how’ and start over.  Isn’t that Prolog?  Remember that Prolog is for AI stuff.  Machine learning is a kind of AI.  It might not just be cool.  It might be useful.  Yes, it’s a little out there.  But this random optimizer makes it seem feasible to have something like this.  It wouldn’t be too much more work in the back end.  Just the frontend; adding the syntax to be able to do it.  Assuming your idea about this Prolog thing actually makes sense, it reveals something deeper about monte carlo optimization:  it removes the need for control in your language, since optimization doesn’t depend on it.  Anything that isn’t pure logic is purely for conceptual clarity for the programmer.  But you’re probably completely wrong about this.\n",
    "\n",
    "It’s not necessarily to develop the fastest language, but to learn about the different compiler techniques.\n",
    "\n",
    "Riscy since it’s unsafe and goes almost as fast as native risc.\n",
    "\n",
    "You said something like this earlier, but maybe your language wouldn’t be a direct competitor to Julia.  Maybe it could potentially be a backend callee for Julia.  Remember that C, Rust, and Go are ‘general purpose’.  This is purely for performance.  Alternatively ‘code that is compiled on the cloud and targeted anywhere.’  Developers all have access to large computers, but users don’t.  This language creates programs that take advantage of the cloud during compile time (and also runtime, potentially).  It is general purpose in its syntax, big data in its semantics.  Stuff like Scala and hadoop.  Who is that for?  Data scientists don’t want to deal with that.  Why make huge projects based on these things.  That just adds to the learning curve.  You shouldn’t learn tools.  It’s not flexible at all.  Code is flexible.  ‘Now that we have access to the cloud, we can do compiler optimizations that before were infeasible due to time constraints.’\n",
    "\n",
    "https://softwareengineering.stackexchange.com/questions/23718/whats-the-most-used-programming-language-in-high-performance-computing-and-why\n",
    "Found when googling ‘best programming language for high performance computing’.  Makes me think that a runtime optimizer is definitely important.  Is it possible if you’re not using an interpreter/virtual machine?\n",
    "\n",
    "https://lmax-exchange.github.io/disruptor/\n",
    "Paul says this is a really good white paper on ‘disruptors’, whatever those are.  Something to do with inter-process communication or something.  He says disruptors have been around for a while, but this is just a good white paper on them.\n",
    "\n",
    "https://www.reddit.com/r/programming/comments/6z6fgz/how_did_python_become_a_data_science_powerhouse/\n",
    "\n",
    "‘A performance comparison of container-based technologies for the Cloud’.  \n",
    "https://www.sciencedirect.com/science/article/pii/S0167739X16303041\n",
    "\n",
    "Think about context sensitive grammars.  Does GLR mean context sensitive?  The ease and simplicity with which you can specify your language is much, much more important than the time it takes to parse the language.  So you should start by figuring out the easiest way to specify a programming language, then afterward work on how you can make the grammar faster to parse.  Someone said ‘we’ve been solving the wrong problems in parsing.’  Rather than thinking ‘this grammar will take n^2 time to parse’ think ‘3 out of these 100 grammar rules will take n^2 time to parse.  The other 97 take n time.’  The runtime of each individual grammar rule is more important.  But do not actually make anything like this.  You want a fast, practical programming language.  Not a fast, practical programming language generator.  There might be some tool out there that no one uses where the creators of the tool would love for you to use it.  Perhaps you could even work with someone who is currently working on that.  Cobble something together using known tools that do EXACTLY what you want without getting in the way.\n",
    "\n",
    "Wait...I remember a GoDoc saying something like it doesn’t allow implicit type coercion.  But then you have stuff like 120 * Time.minute.  Isn’t that coercion?\n",
    "\n",
    "Another important thing about Go:  you can’t necessarily say this for the other languages, but Go was created by people with experience.  Compare Go to a different language.  What are the differences in the decisions they make?\n",
    "\n",
    "https://en.wikipedia.org/wiki/Criticism_of_C%2B%2B\n",
    "You don’t have the experience that the Go guys have.  You need to learn from their mistakes WITHOUT repeating them.\n",
    "\n",
    "‘We currently perform classical loop transformations, especially tiling and loop fusion to improve data-locality. ‘  This is from their front page.  This seems like a bunch of heuristics.  I feel like you should understand more of the basics, and more of the theory, before you dive into these seemingly unproven (though no doubt effective) methods.\n",
    "\n",
    "\n",
    "On optional garbage collection:  You’ll have to do it if you want performant code.  Maybe users won’t turn off garbage collection, but the libraries they use should be fast.  You can’t get fast libraries without collecting all your garbage manually.  Would it really be that hard to mix the two?  You have python able to call native C.  \n",
    "\n",
    "\n",
    "What is the difference between actually understanding something vs memorizing it?  Consider regular expressions.  Most people don’t know how they are actually implemented in code.  But I bet there are people who will use regex their whole lives and understand it intuitively without knowing how it works under the hood.  What is the difference between this and someone using pytorch to do machine learning stuff, and having no idea what they’re doing?  Come to think of it, you don’t really know how a computer really adds 2 numbers together, do you?  But why don’t you feel the need to scratch that itch?  It’s because you know how to do addition by hand.  If asked to, you could match a string to a regex expression in your head using a known procedure.  If I see a *, I can match as many as I want.  If I see a |, I match one or the other.  But machine learning is complex.  If asked to do machine learning by hand, there is no way in hell you could do most of the coding stuff by hand.  So you don’t need to know how a computer actually does things.  All you need to know is how a human could do them by hand.  Then leave it up to the compiler writer to actually understand how a computer does it.  For all the nay-sayers, do you know how a calculator puts together numbers?  Do you fault every small business owner, accountant, and scientist who uses a calculator without knowing how it works?\n",
    "\n",
    "I forgot where I heard this, but a compiler should ‘do hundreds of passes’.  This makes it easier to do a number of things.\n",
    "\n",
    "https://en.wikipedia.org/wiki/History_of_Python\n",
    "This is how python was made.  How did the Guido guy get into this place so that he could make python?\n",
    "\n",
    "Sanic VS Julia:  Sanic is faster, as it is compiled separately, at the cost of being statically typed.\n",
    "Sanic VS C++:  Sanc will hopefully be faster, but if not, still has a much nicer syntax than C++.\n",
    "Sanic VS Rust:  This is one that I’m not sure about.  Sanic has nicer syntax.\n",
    "So all I have to do is make sure that Sanic is actually faster, or at least comparable, to these languages.  So what am I sacrificing in order to get this speed?  You can’t do any systems type stuff.  Rust and C++ are systems programming languages.  All Sanic does is go fast on numbers, functions, and objects.  That, and you have one advantage that they don’t have:  you have 0 technical debt.  Julia’s technical debt is its dynamic type system and lack of separate compilation (which they think of as a good thing).  Rust’s debt is that it is developed by a corporation.  I don’t think I need to say what C++’s technical debt is.  Perhaps you should think about the cloud computing angle more.\n",
    "\n",
    "I feel like you should NEED a file called main to start at.  Then you should also have a config file.  Well, not actually.  The config should go in main.  Like ‘this uses garbage collection’ or not.  Better to state it on a per-project basis rather than system wide.\n",
    "\n",
    "https://blog.golang.org/constants\n",
    "Type conversions are a problem that you are going to run into.  If you don’t make the right decision about how to handle them, it could get ugly.  But then again it seems kind of dumb that people would even use something like ‘long signed int’.  Like, what?  How many bytes do you need?  Just call it by that.  In fact, all your types should just be int.  A 64 bit thing is a dint.  A double int.  A 128 bit thing is a ddint.  A double double int.  A 16 bit thing is a hint.  A half int.  An 8 bit thing is a hhint.  A half half int.  But you should also have a special 8 bit thing for characters.  NOT 16 bit.  If people want to use the extended character set, just give them a 32-bit version.  Because I think java did 16 bit, and it ended up not being enough.  So they can either use ASCII like a normal human being, or they can use full 32 bit whatevers for their special text.  Oh, and don’t forget floats.  Floats are 32 bit.  Dfloats are 64 bit.  Hfloats are 16 bit.  The only problem is:  signed or unsigned?  I feel like unsigned shouldn’t even be a thing.  If you want to force things to be >= 0, that’s your problem.  Oh, but unsigned gives you twice the range!  Yeah, if you think you need ‘twice’ the range now, you’re going to need 4 times the range soon.  Don’t use unsigned.  Just byte the bullet and add another d to your type.  Or you could do it the rust way:  i8, i16, i32, etc.  Seems better that way.  Explicit.  No need to convert anything in your head.  It just tells you how long it is.  f16, f32, etc.  Chars, I think, are a special case.  You have ASCII, which requires 8 bits, but should you include a c16?  Java uses c16’s and now they’re hurting for c32’s.  If you choose the next character set to be 32 bits for each character code, should c16’s exist, or will that just confuse people?\n",
    "\n",
    "All this stuff about optimizations.  Does it have anything to do with ‘control’ theory?  Since programs are just logic + control?  Also seems to have a lot more to say about optimizations.\n",
    "\n",
    "Writing your own lexer and parser vs using a Flex + Bison:\n",
    "Writing your own means you have more control.  Could potentially be faster.  Will almost definitely be slower.  Will be a lot of work.  Will hopefully let you understand the inner workings better.  Could get lost in the details.\n",
    "Using Flex + Bison.  Will almost certainly be faster.  Will also be much, much easier to code.  Although C and C++ use their own lexer and parser, other languages like Ruby use a generator.  \n",
    "I feel like gcc and clang are hand written just because C and C++ are so incredibly bloated.\n",
    "A third option:  Create your own lexer / parser generator.  I feel like this could potentially be a good solution long term.  You would get flexibility as well as speed.  Considering that Flex and Bison are super old, maybe you could make a better one.  The Bison spec says “For historical reasons, Bison by default is limited by the additional restrictions of LALR(1), which is hard to explain simply. …. As an experimental feature, you can escape these additional restrictions by requesting IELR(1) or canonical LR(1) parser tables.”  So I know for a fact that Bison has some garbage in it.  Also, remember how bad the Flex documentation was?  Before you think ‘but there are already so many tools out there’ do you understand those tools?  Would they give you exactly what you needed?  You want to make a programming language.  That’s a huge project, and a programming language has a lot of nuance to it.  Right now I kind of feel like you should definitely use Flex + Bison for now, and in the future possibly consider making your own lexer + parser generator.  If you do both, I’m sure there’s some further optimizations that you could do.  As another note about making your own generator, it’s in line with the spirit of the language you want to make.  You don’t want to hand optimize things.  You want the optimizations to happen automatically.  That’s something that a generator thing could take care of.  Also Bison says it can support GLR, but it’s ‘supported’, which to me doesn’t sound like an integral feature.  Your ideal lexing + parsing tool is like this:  it would allow you to specify any shitty grammar, and it would run.  It would allow you to easily output error messages when needed.  It would have a simple syntax and instructions.  And it would give you suggestions.  It would say something like ‘Your grammar is LR(*) right now.  It will take X seconds to run on most inputs.  We can automatically make it LR(1) for you.”  So you have the option to make it faster.  This way, you understand your grammar better.  \n",
    "Here’s some literature to read: \n",
    "https://softwareengineering.stackexchange.com/questions/17824/should-i-use-a-parser-generator-or-should-i-roll-my-own-custom-lexer-and-parser\n",
    "https://mortoray.com/2012/07/20/why-i-dont-use-a-parser-generator/\n",
    "\n",
    "\n",
    "Consider all the crazy data structures that no one ever implements because it’s just too much work and an array is faster to code.  A compiler would actually want to use data structures like this, right?  It’s code that’s being run all the time.  Before you say ‘table driven is fast, dumb, and easy’, consider that memory isn’t unlimited, and on top of that tables might not necessarily be the simplest thing conceptually to apply to a problem.  Look at Cuckoo filters.  They’re new, 2014 to be exact.  They’re probabilistic.  You could use your EE126 knowledge there.  There must be some way to apply this to compiler optimization.  I feel like there’s a way to apply just about anything to compiler optimization.  Come to think of it, it seems like probability should be an important factor in future optimizations.  If your data is random, you pretty much need to use probability in order to load balance.  How do you make sure that each CPU is getting approximately equal amounts of work?  There’s also that language whose base structure is a tree instead of an array, Refal.\n",
    "\n",
    "https://www.reddit.com/r/rust/comments/55k577/rust_compilation_times_compared_to_c_d_go_pascal/\n",
    "This seems to assert that Rust can be compiled very, very fast.\n",
    "\n",
    "Thought:  Everything that isn’t pure functional programming is syntactic sugar.  For loops, variable declarations, etc.  Someone proved that you can do anything with functional programming, right?\n",
    "\n",
    "Let’s say you make your own language, and it really does do everything you want.  Now you have to sell it.  Well, not sell it, but get people to pick it up.  How do you do that?  Make something cool.  Hm...what’s cool?  Hint:  a game.\n",
    "\n",
    "Design is figuring out what you want to be a limitation, and what you want to be a maximization.  Do you want to maximize the comfort of a car while simultaneously making sure it can go at least 100mph?  Or do you want to maximize the speed of the car while also making sure it’s comfortable enough to sit in for a few hours?  Could result in similar designs, could result in very different designs.\n",
    "\n",
    "How to decide on software tools:  whichever is most popular.  If a google search doesn’t help, see if you can collect analytics yourself.  How many contributors does the project have?  How many pull requests?  How compatible is it with other stuff?  The compatibility thing can be found on wikipedia through charts.\n",
    "\n",
    "Metaprogramming and template metaprogramming.  “Moves computations from runtime to compile time”.  Definitely worth looking into.  But also computations for big data are based on the data, which isn’t given until runtime.  Also look at ‘convention over configuration’ on wikipedia.  Linked to on the metaprogramming page.\n",
    "\n",
    "Reflection:  another interesting concept.  But looking at it, it makes me think that this is the prototypical example of a feature that your language should sacrifice (as in not implement).  These types of features (of which I’m having a hard time describing their commonalities) are things that increase the linguistic power of the language; not the computational power.  These are the types of things that you do not need, it seems like.\n",
    "\n",
    "I feel like computation itself can be shown to be a tree.  Like a math computation.  If you can figure out the tree, shouldn’t it be relatively simple to figure out how to parallelize it?\n",
    "\n",
    "I know this is supposed to be about automatic parallelization, but I feel like a really easy way to implement locking would be to just do something like this:\n",
    "\n",
    "\tRegular code\n",
    "\n",
    "\tLOCK:\n",
    "\t\tSynced code\n",
    "\t\tSynced code\n",
    "\t\tSynced code\n",
    "\n",
    "\tRegular code\n",
    "\n",
    "This is better than lock.acquire() and lock.release() because you never have to search for where the lock is actually happening.  The indentation makes it obvious.  Of course, in C you could just do lock.acquire and lock.release, then indent every line between them since C doesn’t care about whitespace.  But nobody does that because it would look weird and generate wtfs from readers.\n",
    "\n",
    "“How do I minimize the number of poor design decisions I make?”  This is an incredibly broad question, but still one worth asking.  Obviously, the more time you spend on making a design decision, the less likely you are to make a poor one.  But of course, you have limited time on this earth, and so must make decisions with some amount of haste.  There’s the obvious things to do:  consult someone with experience.  But you can’t always go with what they say.  Otherwise you’ll just end up with nothing new.\n",
    "\n",
    "Another argument for optional garbage collection:  say you wanted to write a really fast machine learning library.  Kind of hard to do that with garbage collection, right?  Sure, you could write it in C, but that would defeat the purpose, wouldn’t it?\n",
    "\n",
    "The wikipedia entry for GLR parsers say that an alternative name is ‘parallel parser’.  This sounds promising.  There was also this guy on stack overflow who said GLR parsers were great.  Also, while they run in n^3 time on ambiguous grammars (the parsers you know can’t run at all on ambiguous grammars) they still run in n time on non-ambiguous grammars.  This sounds fantastic.  N time when you can, but can also take ambiguous grammars, and also seems like it’s easily parallelizable.\n",
    "\n",
    "Alternative if else:\n",
    "X ?= 4:\n",
    "\tReturn x\n",
    "\tReturn y\n",
    "Not perfect, doesn’t cover everything, bad idea by itself, but better than ternary operator because of readability.  Or maybe some easy way to combine this with switch statements?\n",
    "\n",
    "A thought on static typing:  you read somewhere that programming is logic + control, right?  And supposedly, less control is better.  A language like prolog is basically all logic and no control.  The less state, the more functional the language is, the more pretty.  I would argue that static typing falls into the logic portion more than the control portion.  Think of static typing as delineating the set that a variable belongs to in a mathematical equation.  It’s like saying f(x) = bla bla where x is an element of the natural numbers.  You could leave it off for conciseness, yes, but people mostly leave it out once it’s already been defined in some earlier mathematical equation.  In programming, you’re always creating functions that the user hasn’t seen before, in essence creating new proofs all the time, so it’s better to be statically typed.  Another argument is that you can include much better error messages, which is actually easier for the user.  Also the auto keyword is really hot right now, so it’s really not that big a deal.\n",
    "\n",
    "You should have 2 settings for debug output:  beginner, advanced.  Beginner basically talks in plain english.  Advanced is much more concise, like other debug output.  The reason you might want more concise debug output is because for big projects there’s a lot of debug output.  Instead of 80 lines, you might get 400 lines.  A lot more scrolling.  Kind of annoying.\n",
    "\n",
    "Refal - A language where the basic data structure is a tree, rather than a list.  Allows for more freedom.  That’s all I know about this, but it sounds interesting.\n",
    "\n",
    "Spark is basically for Scala.  You should look at Scala.\n",
    "\n",
    "Ask the super node guys more questions.  You need to get an advisor.  You need to find an advisor whose research lines up with what you want to do.  2 years of classes, 2 years of hard research, X years before you get a stamp from your advisor.  Your advisor determines what you work on.  Then you tell the advisor that you want to change it a little bit, and maybe the advisor says ok.  Your advisor pays you.  Your advisor gets grants.  Most advisors are professors.  There are some advisors who are at Berkeley National Lab.  “How to find a research advisor”.  Do you get into grad school, then find an advisor, or do you get an advisor, then get into grad school?  Yes.  But really, probably more likely to get in if you get an advisor first.  Ask professors working on cloud, on compilers, and on parallelism / distribution.\n",
    "\n",
    "Resource\n",
    "https://stackoverflow.com/questions/6319086/are-gcc-and-clang-parsers-really-handwritten?noredirect=1&lq=1\n",
    "Even C is ambiguous.  You should avoid operator overloading when possible.  Also avoid all the crazy shit that C allows.\n",
    "\n",
    "Goal\n",
    "Model your optimizations as constraint satisfaction problems.  I feel like most people know how to do this, but they never formally learned the actual algorithm and they just have a big switch statement.  Actually, Professor Aiken mentioned that all these optimizations can be modeled as a search problem.  Is a constraint satisfaction problem a search problem?  I think so.  Speaking of search problems, Prolog is good for that, right?  Is it at all possible for you to just mash prolog-like functionality into a regular imperative language?  Would that be at all helpful?  Would it just be confusing?  Would it be easier, or harder to optimize?\n",
    "\n",
    "Goal\n",
    "Basic principle of coding:  If the programmer isn’t forced to do it, they won’t do it.  This means code is never optimized.  Tools that aren’t mandatory aren’t used.  There are compiler options beyond -o3.  They aren’t used for fear of introducing bugs and dependencies.  So you need to make the decision for them.  This is not a general purpose language.  Look at polly, and all the bug fixes it requires.  \n",
    "\n",
    "Goal\n",
    "Write out why you think a brand new language is necessary / helpful to big data programmers.  Why does it need to be separate, what does it have that other languages don’t?\n",
    "\n",
    "Goal\n",
    "Make a generic letter to professors.  Hi professor, I’m Miles. I enjoyed your work on _, and had some questions about it.  (Ask questions here).  The reason Im interested because of this idea I had for a programming language.  (Insert research pitch here).  What do you think?  Use your letter to Nick as a reference.  Maybe include a demo showing that it ran fast.\n",
    "\n",
    "Goal feature\n",
    "Since the cloud has very limited hardware, you can optimize for that specific hardware.\n",
    "\n",
    "Tier 3———————————————————————————\n",
    "The problem with current object oriented programming is currently being fixed.  Look at how oop used to be done (Java, C++), and how it’s done now (React, Node).  The problem was that every problem used to be an ‘is a’ problem rather than a ‘has a’ problem.  But is there a runtime cost to this?  Look at how Go does it.  But the heart of the problem is that you can’t say an object ‘is’ anything other than itself.  Yes, you can make a human class, and have any number of human objects spawn from that class.  But what if they have different jobs?  Then 1 human object would have a certain method, and another would not.  So maybe you have it implement some kind of interface.  But I feel like there’s definitely still a lot of room for further simplification.  It’s just that each individual object is unique, not just in its fields, but potentially in its methods as well.  It’s because true categorization, true generalization, is impossible.  All categorization, no matter how natural and intuitive it is, is arbitrary in the end, and there will be exceptions.  You had a fuzzy idea of being able to just refer to arbitrary methods in a class or struct.  I’m pretty sure this is a mixin.  Look at React, and why they think mixins are bad.  Also if you want to figure out what a mixin is, it’s not just a react thing, so just look up ‘wikipedia mixin’.  But then the react thing says something like ‘mixins were a bad idea because javascript is a dynamic language’.  A static language would catch all the crazy stuff at compile time and tell you about it.  Still, read the react thing.  It’s a very good description of the entanglement that results from mixins.  You might think to yourself ‘why not just put some thought into how you design it?’  That requires a lot of forethought.  These websites are simple products.  You know exactly what you want the website to do before you’ve written a single line of code.  The problem is how you design the code.  And if you make your code system easily changeable, you don’t need to put any thought into how you design it.  The design process isn’t something that should even need to be accounted for.  You should be able to code exactly what you want, and then easily change it later if you need to.\n",
    "\n",
    "Professor Aiken said in Static Vs Dynamic Typing Part 1 that ‘there are a lot of fancy new type systems that haven’t been implemented yet.’  What are they?  What exactly is he talking about?\n",
    "\n",
    "If everyone spoke English and used the English alphabet, would ascii be ok for everything?  Could we constrain chars to just 8 bits?\n",
    "\n",
    "Could you potentially use your language to mine crypto?  If it really use faster than C, and easier to code, then you should be able to do it more efficiently than other people.  Maybe something to consider later down the line, when you need money to sustain yourself.  Or potentially some other big data whatever.  Just find something that requires a lot of mindless number crunching.\n",
    "\n",
    "Wait, I understand why Python is locked to 1 processor:  it was originally designed for the Amoeba operating system.  Since Amoeba presents its multi-cpu system as a single unit, a program that explicitly asks to take up multiple cores doesn’t make sense.  Of course, the Amoeba operating system is dead, and python has been gimped ever since.  Good thing numpy and other libraries exist.\n",
    "\n",
    "Best commands for running a program:\n",
    "sweater <source file> \n",
    "This is reminiscent of python <source file>.  Will compile+run the program.\n",
    "Sweater compile <source file>\n",
    "./<binary file of same name>\n",
    "This will do the 2 steps separately.  This way you’re not re-compiling every time you run the program.\n",
    "\n",
    "Ok, maybe optional garbage collection would be nice.  You know, for certain projects.  I know you don’t plan to work on that kind of stuff, but if you’re trying to make a language that’s as fast as possible, leaving out something like that would be kind of sad.  I know you’re going for big data type stuff, but if you’re trying to make a really awesome compiler that can make really small fast binaries, maybe people would want to use it for embedded stuff, or maybe games.  Except for the fact that it uses garbage collection.  It just seems like a waste to have all of that functionality, and have this 1 barrier completely cut your language off from being usable in these 2 places.  What if you decide to pivot later?  Would it really be that hard?  Garbage collection is just an algorithm.  It shouldn’t be difficult to turn it on or off.  Also, it should be just that, on or off.  No halfway measures.  You either use the garbage collector, or you don’t.  Simpler, less bug prone, easier to optimize.\n",
    "\n",
    "Most optimization you can’t make guarantees about.  But you can guarantee stuff like ‘This uses the gpu’ or ‘this will automatically distribute tasks.’.  This is what sets your language apart.  \n",
    "\n",
    "69 LangZ.  It’s a language, it has a letter (Like C or R), and its for lazy people who don’t want to worry about parallelization, or making their code fast in general.\n",
    "\n",
    "70 asdf\n",
    "\n",
    "71 asdf\n",
    "\n",
    "72 What is IEEE?  Are there standards you need to comply with for ease of use?\n",
    "\n",
    "73 Check out the Go testing/benchmark library.  Interesting concept.\n",
    "\n",
    "74 Don’t forget DE Shaw Research is making that ‘specialized hardware’ to research drugs\n",
    "\n",
    "75 The reason matrices are so important is that they represent transformations on data.  They are 2 dimensional, mapping 1 type of data to another.  The user defines how they want data mapped.  That isn’t something a machine can just do automatically\n",
    "\n",
    "76 One problem (you’ve probably said this before) is that scientists code once, and run once.  Doesn’t matter how fast your program runs if it takes forever to compile.  How does lisp compile so fast?  Can you use parallelism to compile faster?\n",
    "\n",
    "77 Users should always be assured that their program is taking full advantage of all available hardware.  Should have a simple command or function that will tell the user what hardware the program can see.\n",
    "\n",
    "83 What makes bash scripting so “powerful?”  Probably how easy it is to manipulate input / output. \n",
    "\n",
    "84 Another point reiteration:  no project forks.  No options.  Every time a project forks, one of those tines will be used 99% of the time, the other 1% of the time.  Anyone working on the 1% is wasting their time.  That polly project is nice and all, but how many people are actually using it?  Perhaps there are a few people, but it would benefit a hell of a lot more people if it was incorporated into the normal Cpp compiler.  So why don’t they do that?  My uneducated guess is that integrating polly into the standard LLVM optimizer would require too many changes, too many compromises.  It would break too many things.  So the further along the LLVM compiler gets, the more difficult it will be to integrate polly.  So I certainly hope that what they’re working on is more than just applicable to C++.  But they’re at that ETH Zurich place.  Much smarter than you are.  They know what they’re doing.\n",
    "\n",
    "87 Wikipedia ‘array programming’ and ‘programming paradigms’.\n",
    "\n",
    "88 Could use attribute notation for certain operations like inverse / transpose.  A.i and A.t\n",
    "\n",
    "89 Why does every language use something like a pow function for exponentiation?  Python uses the ** operator.  Easy.\n",
    "\n",
    "90 Lets look at the ! operator.  If its in front of a variable, its logical NOT.  But if its at the end of a variable, you could make it a factorial.  If you use ‘and ‘ or ‘not’ then ! Could just be for factorial.  But why would you do this?  Because in math, factorial is a common enough operation that they gave it a special notation.  So it’s a common operation.  You want to optimize for common operations.\n",
    "\n",
    "91 Either Haskell or Erlang “let it crash”.  You’ll have to think about reliability.  Can you put this off, or does it need to happen now?  In general, what features are hard to implement later, and what can be put off?\n",
    "\n",
    "92 asdf\n",
    "\n",
    "93 Look at Prolog.  On wikipedia, it mentions something about using it to differentiate between compiler passes or something.   Also note the quick sort implementation.  Incredibly concise, even spared to something like python.\n",
    "\n",
    "94 asdf\n",
    "\n",
    "95 The Golang regex doc contains a link comparing Go’s implementation to Perl, Python, etc.  Worth looking at.\n",
    "\n",
    "96 The cathedral and the bazaar.  Also find some other work on the same subject.  Will teach about open source software development, the ups and downs, how to make it work, etc\n",
    "\n",
    "97 Databases will teach you about languages like SQL, something very different that what you’re used to.  Seems important to understand this paradigm that is so closely tied with big data.  Actually, it must be important, maybe even pivotal, to your language.  Less so than concepts like ‘cloud’, ‘distribution’, etc, but more so than most of the optimization stuff you’ve been thinking of.\n",
    "\n",
    "98 asdf\n",
    "\n",
    "99 Your parser generates an AST?  Our parser generates a BST:  blood, sweat, and tears.\n",
    "\n",
    "100 asdf\n",
    "\n",
    "101 How can a compiler be built in such a way that it is ‘self-optimizing’?  Is this super-compilation?  Boot-strapping?\n",
    "\n",
    "102 Code can sit there optimizing for an arbitrary amount of time.  Maybe make it so that it just continually optimizes until you tell it to stop.  Is there some kind of discipline or methodology to this?  The setting for this idea is “computer, work on this problem until I tell you to stop at an arbitrary time”.  How can the computer work on the problem in such a way that whatever time it is told to stop, it can yield some result that is ‘beneficial’?  Something to do with streams of data?  Or just the idea of streams in general?  How can you make an algorithm that, when it is told to stop, will have to throw away a minimal amount of work?  Using the factorial thing again, lets say we told the computer “calculate factorial(100000000) until I tell you to stop”.  We’ll say the computer will never finish this.  If the computer used recursion and you stopped it at an arbitrary point, it would have nothing to show for all of its computation, since it didn’t save anything.  But if it used a dynamic programming implementation, it could at least say something like “I couldn’t get to 100000000, but I did manage to calculate all the factorials up to 40000”.  \n",
    "\n",
    "103 asdf\n",
    "\n",
    "104 There’s a lot for you to learn about runtime optimization.  If you make a program that takes in arbitrary data, you’ll need to make some runtime optimizations.  It can’t be done at compile time because you haven’t gotten the full picture yet.\n",
    "\n",
    "105 Syntactic sugar:  pretty, also restricting.  If it restricts what you can write, its a smaller domain space, and therefore easier to optimize.  Then again, users might get confused.  “This syntactically sugared expression should be identical to this unsugared expression, but for some reason it runs faster.  What gives?”  But maybe you’re conceptualizing it wrong.  If the sugared expression really is identical, then the compiler should see no difference, right?\n",
    "\n",
    "106 How do you make up for a bad design decision?  If you keep it, you have to put up with it forever.  If you change it, you break a lot of code.  What do?\n",
    "\n",
    "Lex and Flex, Bison and Yacc.  Which ones will restrict you (compare their licenses?).  Find out which ones documentation is better.  Also note that lex documentation applies to flex.  I think.  Mostly.\n",
    "\n",
    "107 Consider the latex mathematical format.  Could that be a form of ‘syntactical sugar’ that you could include in a language?\n",
    "\n",
    "108 Flex and Bison are basically just Cpp files with the single edition of a special syntax denoted by %’s at the beginning and end.  What if you made a language where you could just define your own mini-languages?  Is this somehow different from a macro?  Actually, could result in a lot of code completely incomprehensible to anyone else reading it.  Still, a fun and interesting concept.\n",
    "\n",
    "109 Why does constexpr exist in Cpp?  If we have a factorial function f(n), why do we need to say constexpr f(n) in order for Cpp to evaluate it at compile time?  If the compiler sees an f(5) somewhere in the program, it could easily just try to execute that function now.  If it works out, great, if it doesn’t, oh well, at least you’ve partially evaluated it.  I mean, there are certain implications.  Like what if the user wants to time the function.  Then it takes a long time to compile, and when they go to test the runtime it comes back as constant time.  That’s not exactly an accurate measurement.  Still, it seems like a really big waste to not do that automatically.  Perhaps another argument in favor of compilation + runtime together.\n",
    "\n",
    "110 An argument for making an optimizer backend for an already-existing language:  you get an enormous suite of test cases for free.  Just rip anyone’s code off GitHub, run it through your optimizer, and see if the overall behavior is the same.  Another option is to create a parser thing that turns language X into your language.  I mean, you compiled Python into C, so it should be possible.  But it seems like it would be difficult and bug-prone.  But it’s an interesting concept.  Language-agnostic test cases.\n",
    "\n",
    "111 Every dynamic programming algorithm has a recursive equivalent, right?  Does that mean you could auto-dynamize recursive functions?  Looking at the factorial function again, it seems like it would be really easy.\n",
    "\n",
    "112 Should find an in-browser equivalent of sublime and atom for ease of use with the cloud.\n",
    "\n",
    "The whole online-compilation thing.  There must be a way to save the work you do during compilation such that you won’t have to repeat it once you run it again.  Of course when you run a program it could output an optimized file, then on subsequent runs could check for the optimized file, but really that seems like it could get complicated and bug-prone very fast.  For instance, if a user timed multiple runs of their code, the first run would be much slower than the others.  Wait, never mind all of this.  By default, your code should not run with any optimizations.  If users want to test the speed of their algorithm, they should test it without applying any optimizations.\n",
    "\n",
    "You expect everyone using your language to use it on the cloud with a lot of processors.  All of that computing power shouldn’t just be taken advantage of at runtime.  You need to also seriously think about how you, the compiler writer, can take advantage of it at compile time.  How can you make optimizing faster, given all of these parallel cpu’s and gpu’s?  If you’re going to make a whole bunch of insane optimizations, that could end up being really slow.  Don’t let that happen.  One idea is pipelining the different stages of compilation.  One thread is lexing a part of the program, and it passes what it lexes to the parsing thread not as one large chunk, but as a stream.  I’m almost certain they already do this.  Right?  But it must take on a little more nuance in a many-core computational environment.  Ok, yeah, you are dumb.  If you have 4 cores, and 5 files to compile, then your first task is to lex 4 files.  Then whichever one gets done lexes the next file.  Pipelining is only useful if you have cores that aren’t being used.  So I guess it would be faster for single file scripts.  But by their very nature they already compile fast.  Wait, but if you’re working on a giant parallel cluster, maybe you do have more cores than you have files.  So maybe pipelining would be useful.\n",
    "\n",
    "Here’s an idea for your language:  crowdsource super-compilation.  When a user isn’t actually running anything on their cloud, (or maybe while they’re running things on their cloud), you could ask them if they could run a script that would search through the compiler source code using 3SAT, looking for optimizations.  Then any optimizations found would be sent back to you.  A super easy way for users to contribute to the project without actually having to put in any work.\n",
    "\n",
    "TCP vs Unix sockets.  Makes me think that learning 168 might be more useful than previously thought.  Answers the question “How do (or should) nodes in a cluster communicate?”\n",
    "\n",
    "A problem with separate compilation:  Let’s say your scientist has a big data problem.  If they ran a script in python, it would take a week of number crunching.  With your new language, it would take 2 hours of compilation, and 4 hours of number crunching.  That sounds way better, but then your scientist has to sit there for 2 hours doing nothing, waiting for the program to compile, before they hit run and head home.  You’re trying to let the scientist have more free time than if they were using python, but in this scenario they actually have less free time.  So perhaps that’s a point in favor of non-separate compilation.  Of course, the scientist could just type a command like “cool_compiler my_script.cl -o my_script && ./my_script”, but that’s complicated.  Hm… maybe a simple solution is to just have a command that’s something like “build+run” or something like that.  Then again, compiling takes like 0 time unless you have a really really big project, and code-wise, scientific computing doesn’t use large codebases, right?\n",
    "\n",
    "Feature\n",
    "Don’t actually include this, as it’s kind of related to your thought about generative compilers.  You know how regex can only count ‘modulo’ some number?  If you want to count nested parens, couldn’t you have some expression like “#parens % 2 == 0 | #parens % 3 == 0 | #parens % 5 == 0 | ….”  This way, your regex can count all sets of parens so long as the total nesting isn’t some product of primes that you haven’t taken into account.  So just make the number of prime mods significantly large.\n",
    "\n",
    "Feature\n",
    "This isn’t really a feature, but you should have lots of examples in your documentation that do a ‘using namespace math’ so you can show how concise math operations can be.  Look at how latex formats everything.  You could make it similar to that, possibly.\n",
    "\n",
    "Feature\n",
    "Whitespace is good not just because it’s cleaner, but because it frees up other punctuation.  Think about all the different punctuation, and how it’s wasted.  #, $, {}, etc.  Why do we use # in front of an include?  What is the point.  # and the others should be reserved for operations.  An operation is something you use all the time.  We don’t have to have to add two numbers together by doing add(x, y), right?  As often as possible, we want to use operators, which are much cleaner.  Things like definitions shouldn’t have their own special single characters.  Funcs should be declared by saying ‘func’, macros should be declared by saying ‘macro’, etc.  Then again, you can essentially create infinite operators just by repeating 1 of these symbols.  You could make it more compact by mixing up the symbols.  You’re using ** for taking powers of stuff.  You could use $, $$, $$$, etc, and each would mean a different thing and be easy to lex.  You could also take $%, $%#, etc.  Maybe you should make it easy for users to construct their own operators.  Well, it probably already is easy for them to construct their own operators, but just encourage it or something.\n",
    "\n",
    "Goal\n",
    "A lot of these features are about usability.  Can be summed up like so:  a user wonders ‘what library should I use?’  They should 99% of the time be able to just grab a library that has your stamp of approval.\n",
    "\n",
    "Feature\n",
    "There should be a config file for each project, like a gitignore.  These features should be things like spacing.  Things that don’t matter to the compiler but matter to the user.\n",
    "\n",
    "Feature\n",
    "‘Go get’ is just a fancy git clone.  Remember that comic?  But also interesting.  An easy way to download stuff.  But it also gives users 2 options.  Hm….  Then again, you could make it just for ‘extended’ libraries that you write yourself.\n",
    "\n",
    "Feature\n",
    "Should be able to compile and get a single binary without having to do any Makefile stuff.  Why do big projects always have some crazy makefile?\n",
    "\n",
    "Resource\n",
    "Julia supposedly has ‘powerful shell like abilities to manage other processes’.  You were talking about the conciseness of shell scripts.  This sounds interesting.  Another thing is that Julia isn’t ‘separately compiled’.  Wtf.  It takes literally 1 command to compile stuff.  Maybe its just-in-time compiled.\n",
    "\n",
    "Goal\n",
    "Distributed systems currently needs some library like OpenMPI to understand that there are other systems connected.  Need to figure out a way to put this on the back end so that the user doesn’t need to worry about it.\n",
    "\n",
    "Goal, Blob\n",
    "About cloud computing and why its a huge factor.  The cloud has virtualization of hardware.  It also had highly homogenoeus hardware, meaning you need to build for much fewer targets.  Also means easier access to hardware since since we have virtualization to abstract it.\n",
    "\n",
    "Goal, Blob\n",
    "Another possible principle.  Maybe there’s no single best way to accomplish every task, but there should be one obvious way.  I think this is like python.  There should be minimal libraries.  If you have an idea to make the language better, it should make the language better in all cases, which means its a decision the coder shouldn’t have to make, which means it should be written directly into the compiler.\n",
    "\n",
    "Standards\n",
    "Something else that must be looked into, but much later, is how will the licensing work?  Rememberr that gcc has a license that caused apple to switch to Clang, which had a less restrictive license.\n",
    "\n",
    "Goal\n",
    "Use specialized hardware as much as possible.\n",
    "\n",
    "Blob\n",
    "Remember what you said about the cloud and how hardware would become more and more specialized?  This is another big one to consider.  How can you optimize as efficiently and painlessly as possible for hardware that hasn’t come out yet?  Is the concept of FPGA’s related to this at all?\n",
    "\n",
    "Goal Blob\n",
    "Another strong consideration is ‘environment’.  Is it barebones like C/Go, or is it very contained, like java, or python, which even has its own installer for everything, pip.  \n",
    "\n",
    "Resource\n",
    "One component of the success of python is that the creator, Guido Van Possum, is the ‘benevolent dictator for life’.  I would assume this meant he had the last say on all design decisions.  This is counter intuitive.  If different people have different preferences, how did one guys decisions lead to such unanimous satisfaction with the language?  \n",
    "\n",
    "Goal\n",
    "If you ever forget why you’re doing this, remember that Jay let his comp run overnight to get pictures of fly neurons.\n",
    "\n",
    "Blob\n",
    "Remember that malloc sucks, and it isn’t something that the coder should have to think about, but is necessary for the sake of performance.  Don’t call it malloc.  Try to make it as intuitive as possible.  Malloc could be ‘save’ and free could be ‘trash’.  Give pointers the same treatment.  Forget convention.  This isn’t for computer scientists, it’s for scientists.  Just have to tell the scientists ‘at the end of your function, everything gets trashed unless you explicitly save it’.  \n",
    "\n",
    "Resource\n",
    "If you need an example problem to use, consider your CS170 project.  You know that project inside out, and you know its really computationally intensive.  If that doesn’t work, consider a generic CS170 project.  Maybe ask a professor.\n",
    "\n",
    "Goal\n",
    "To reiterate, this is not a general purpose language.  Since it has fewer concerns than C/C++, it can be faster, and with fewer bugs.\n",
    "\n",
    "Goal\n",
    "Your language maximizes runtime with the constraint of being syntactically as simple as Python.  Your language does not maximize simplicity with the constraint of being faster than C.\n",
    "\n",
    "Feature\n",
    "Its not part of syntax, but you also don’t want a bunch of library calls like you would have to make in C to parallelize.  Nor do you want a bunch of conflicting compiler options.  All you want is 1 cohesive compiler option.  The MacOS of compilers.\n",
    "\n",
    "Feature\n",
    "If this language is used for big data, it should have good integration with database languages like SQL, right?  How would that work?\n",
    "\n",
    "Blob\n",
    "“Im waiting on my data to finish” is the new “Im waiting on my code to compile”\n",
    "\n",
    "Goal Feature\n",
    "This is about automatic optimization.  No threads, no processes, no message passing is every seen by the user.  But do consider alternative constructs to make it easier. Remember that hacker news article able thread “nurseries”?\n",
    "\n",
    "Feature\n",
    "Look into ‘options’ garbage collection.  I know there’s at least one language that does it.  Think fo the Malcolms and Jays of the world.  They don’t want to pick up trash.  If they see a language that forces them to do it, they’ll just drop the language.  Then they don’t get any speedup.\n",
    "\n",
    "Feature\n",
    "Also should have an ‘auto’ keyword or := definition.  They’re really really lazy.\n",
    "\n",
    "Blob\n",
    "In fact, the fundamental spirit of coding is lazyness.  Rather than doing work yourself, you are telling a computer to do work for you.  Your job is to automate yourself out of a job.\n",
    "\n",
    "Resource feature\n",
    "Read “Go’s Declarative Syntax”. Has a good explanation of why C pointers are how they are, and why Go is different.  Maybe instead of ‘pointers’ you could call them ‘boxes’, since you ‘open’ them to get the actual data.  Or maybe that’s too close to packages.  Maybe ‘portal’ is a better word?  But if you’re a scientist, do you even need pointers?  Actually, you know the MapReduce model takes in functions as arguments.  So scientists need functions as args.  I think that counts as needing pointers.\n",
    "\n",
    "Feature\n",
    "What about arrays / lists?  Variable length or fixed?  Obviously fixed is faster, but variable length is easier / more intuitive.  Go has fixed length and lets you make ‘slices’ but what if you want to go over capacity?  Annoying.  Maybe have both.  Use [] for fixed length arrays, and <> for extendable arrays/vectors.  Your gut feeling about vectors being bad is just because of video games.  For games you need consistent performance.  But for data you just need amortized performance.  Look into Pythons weird way of doing lists too.   I like the <> because it looks like an arrow, so its extensible.\n",
    "\n",
    "Blob\n",
    "Choice paralysis is a real thing and it plagues software.  Make it easy on the users by giving them official packages.  A user doesn’t need limited choices in order to make a decision.  They can have a bunch of choices.  What paralyzes them is not having a default choice.  Having a bunch of default packages will act as a default choice for the users.  Think of all the default libraries available in java.\n",
    "\n",
    "Resource feature\n",
    "Why did gcc choose handwritten recursive descent?  I know this isn’t the reason, but it seems easier to parallelize.\n",
    "\n",
    "Goal\n",
    "Other languages consider all cases, you only consider the average case (within reason).  In fact, all of the rules should have an asterisk.  There should be exceptions if you can provide a good enough reason.\n",
    "\n",
    "Resource\n",
    "Remember machinebox?  It basically just did all the machine learning algorithms and kept the best ones.  Kind of like your cs170 project.  Also seems like something easily optimizable if you know the whole language it s implemented in.  Don’t you just give it the data and labels and it spits out an answer?  Seems suspiciously like a black box (duh) but the temptation to implement it is too much.  Using machine box is almost like using a declarative language in a way.  You feed it the facts (data, like pictures or whatever) and then give it a bunch of successful queries (labels).  Then you give it new queries and it guesses at the correct response.  The bag of tricks that is machine learning sounds like you could easily just run all the different tricks in parallel, much in the same way as you did with your CS170 project.\n",
    "\n",
    "Feature\n",
    "About your ‘environment’.  Does it use a container?  How many runtime library calls will it make?  Should you use something like docker?  What about he java ‘virtual environment’?  Is that a container?  Virtualization would make it easier, but I think you can’t really do hardware optimization if you’re running a virtual environment.  If the whole point of a virtual machine is to abstract hardware, then interacting with hardware defeats the purpose of the virtual machine. But I think containers are different.  They just contain all the dependences/settings needed to wrong your program.  All of these things seem to fall under the ‘operating systems’ umbrella.\n",
    "\n",
    "Goal\n",
    "Code can be 2 of 3 things:  Simple, Fast, Flexible.  C++ is fast and flexible.  Python is Simple and Flexible.  C4 will be simple and fast.\n",
    "\n",
    "Goal\n",
    "Julia:  maximize simplicity, constraint of being C-like in speed.\n",
    "C4:  Maximize speed, constraint of being pythonic in complexity.  Another way to phrase it is C4 is Julia but specifically for the cloud.  And pythonic in complexity means that the user never sees any more than 1 thread.\n",
    "\n",
    "\n",
    "Blob\n",
    "An operating system is used to manage resources.  There are many different kinds of users, so the machines ou sell need an OS that is flexible.  But when you go on the cloud, you can have a master running an OS, and slaves running some absolutely minimal stripped down OS.  Your calculator is used only for number crunching, so it should be faster, right?  The OS is always performing checks to make sure things go smoothly.  But what if you’re a slave node?  All you do is crunch numbers.  So you shouldn’t need to run those checks.  Would this yield more speed up?  Do GPU’s / TPU’s already do this?\n",
    "\n",
    "Goal\n",
    "Rephrasing of the GPU / TPU thing:  we have automatic parallelization, automatic memory optimization, we need automatic decision on what hardware to use.  Shouldn’t need to make an API  call in your code.  The GPU / TPU has a small target, so the compiler should be able to recognize code that it can handle best. \n",
    "\n",
    "50 What can you do at installation time?  Are there any optimizations you can make there?  Would it mean that people couldn’t share binaries, since their compilers would be different, which would Ean any binaries would only work on that architecture?  When I say ‘installation time’ I mean that literally.  What if the compiler could optimize itself (so that compile time is shorter) based on what hardware its installed on?  Getting binaries vs source code:  forcing source might actually be a good idea.  Perhaps simpler for the user to understand.  But what if its an FPGA?  What if the hardware itself changes?  There should be an easy nuke button that checks for any differences.  Like an apt-get update.  Wait, being forced to share source code would be terrible.  Then it would be impossible to build anything that wasn’t open source on it.  Wait, maybe you could just encrypt the source code?  Nah, that’s dumb.\n",
    "\n",
    "51 Actually, I would be wary of including FPGA’S as a ‘target’.  The whole point seems to be to test out new hardware.  An FPGA is essentially infinite hardware, which you obviously can’t optimize for.\n",
    "\n",
    "54 I just realized something.  You don’t want this language to be converted into C, which means your compiler back end will need to convert it to assembly.  Is that what you want?  Seems like an incredibly difficult task.  You say you want to avoid the ‘bloat and bureaucracy’ of C, but isn’t that C++ that’s bloated?  Also, optimizing for GPU’s will mean you’ll have to interface with CUDA.  So it seems kind of unavoidable.  Mostly due to targeting specific hardware.  Actually, wait.  CUDA IS JUST LIKE PRAGMA OMP PARALLEL.  ON THE WEBSITE IT SAYS ‘YOU CAN PROGRAM IN POPULAR LANGUAGES LIKE C, FORTRAN, Matlab, and Python’.  So I think you just hook into it, with any language as long as you build the API for it.  But still, imagine the time investment.  You would be spending so much time on this.  Perhaps you could start with C on the backend, and faze it out as needed?\n",
    "\n",
    "55 Don’t forget this is about automatic parallelism.  Sure, you have the option for automatic parallelism in other languages, but here its expected.  Users will know for a fact that they can write their code in this language, and they will never ever think about parallelism.  \n",
    "\n",
    "56 Very important that you do MIT OpenCourseware.  You need to get very familiar with Julia, so that you can actually convince yourself this is a good idea.\n",
    "\n",
    "57 Having second thoughts about garbage collection.  Should really look at the runtime consequences.  Im sure theres plenty of research on it.  Still pretty sure about strict type checking, just remember Go if you’re unsure.\n",
    "\n",
    "58 Again about Julia.  If you work on Julia, probably won’t be able to get the easy ‘cloud workflow’ you want\n",
    "\n",
    "59 Maybe there shouldn’t be any support for manual parallelization.  If you find a solution that is better than what the compiler gives you, you should contribute that solution to the compiler to make it more efficient.\n",
    "\n",
    "60 You talked earlier about how matrices are an ‘explicit’ form of parallelization.  But actually helpful to programmers.  I feel like this idea about matrices should be given a lot more thought.  It seems the whole basis of ‘automatic’ parallelization could be based off matrices.\n",
    "\n",
    "61 Seriously consider staying in 176.  Comp bio is the application that you fantasize about. All those bio people who don’t understand the joy of mathematics.  Is 168 really going to help you?  Hmmm…. What will help the most with cloud stuff?  Not applications of the cloud, but cloud directly.  Should research each class to find out what you would be learning.  Then again Super compilers use SAT, which is CS170, so maybe 176 would be helpful.  Hmm…. It does sound a lot more fun that 168.\n",
    "\n",
    "Remember the crazy guy who made Temple OS\n",
    "\n",
    "63 Your vision in 1 sentence:  Users log into their cloud X account, write 100% serial python code into the terminal, and get insane performance when they run it\n",
    "\n",
    "64 Why isn’t there a global ‘settings’ file?  For each project, not the entire language, that is.  Compile with these tags.  Format this way, use these globals, etc.\n",
    "\n",
    "Just had a thought about channels.  Remember the critical section when we did pragma omp parallel?  It was just a sum.  They could have put it into a channel, then continued what they were doing before.  Channels seem cool.\n",
    "\n",
    "Lets say you decide to make everything from scratch.  Would this actually take longer?  After all, you make it from scratch, you know everything about it.  Fewer bugs.  Isn’t Lisp supposed to be good for this kind of thing?  Also, what exactly does ‘from scratch’ mean?  Straight conversion to assembly?  You said this before, but if you don’t start from scratch, how hard would it be to replace dependencies if you don’t want them anymore?  How can you make it easy?\n",
    "\n",
    "How do you mathematically prove how fast an optimization will be?  This is necessary so you aren’t just doing things ‘heuristically’.  \n",
    "\n",
    "CS270\n",
    "\n",
    "What is IEEE?  What kind of standards should you comply with?\n",
    "\n",
    "Check out the Go testing benchmark thing.  Might come in handy\n",
    "\n",
    "\n",
    "\n",
    "—————\n",
    "\n",
    "An ASIC is an application specific integrated circuit.  A GPU is an ASIC for computing graphics.  A CPU is an ASIC for general stuff, I guess.  An FPGA is a general piece of hardware that you can change.  It’s useful when you want to do a specific set of operations that is not available as an ASIC.  Once you start using 100’s or 1000’s of FPGA’s to do these specific computations, it’s time to consider building a dedicated ASIC.  So FPGA’s aren’t things that you need to optimize for.\n",
    "\n",
    "Super compilers use SAT.  That’s pretty cool.\n",
    "\n",
    "Ok, seriously, seriously, reconsider garbage collection.  It flies in the face of the whole spirit of this language.  How much speed up would you even be getting?  No idea.  Maybe there’s some alternative to the garbage / no garbage paradigm.  Maybe if they explicitly save it, you assume they free it, but if they return it and don’t save it, it’s auto-saved.\n",
    "\n",
    "Something to consider that you haven’t heard of in too many books:  restrict certain data structures to make them more easily parallelizable.  Is there any way to do this?\n",
    "\n",
    "Csq because its speed is C^2.  C4 because it’s 4x faster than C and also explodes.  Sweater, because it’s made of a lot of threads and makes your computer run really hot.\n",
    "\n",
    "From hacker news, this guy Jguegant made a ‘compile time game’.  He says ‘most of the computations are done during the compiler phase.  Interesting.  How does this work?  Supposedly it’s very computationally efficient.  \n",
    "\n",
    "Must figure out a good ‘data pipeline’.  How do people get data into your language?  More complicated than it sounds.  Need to scrub data, get it into a recognizable format, etc.  Should have very fast, simple, intuitive facilities for getting data in and out of your program.\n",
    "\n",
    "Everyone ignores warnings.  Why?  Because they’re tired of coding.  In what instance would you want to warn someone, but not just through an error?\n",
    "\n",
    "Everything should have very explicit documentation.  Again, look at the java docs.  But even then, there’s room for improvement.  Each method has a parameters and returns section.  But what about side effects?  Is the function re-entrant?  What about runtime?  Any other properties to note?  Also, documentation is possibly too verbose.  Just list things out simply.  Simple documentation doesn’t just help users, it helps developers as well.  Remember at Scale there were 2 documents:  the README, and the wiki.  He said nobody ever wanted to change the README because you had to go through bureaucracy to change it.  It had to be spotless before it could be added.  It was much easier to change the wiki.  You need to make documentation easy to change.  You should want to update it.\n",
    "\n",
    "Gofmt is an interesting simple tool.  Makes me think about how tools aren’t really a standard thing.  There isn’t just a compiler, and a makefile, or an interpreter.  There isn’t just a java environment or whatever.  It’s whatever you want it to be.  I feel like tools can definitely be simpler, more helpful.  Remember that lisp debugger thing that let you rewind code?  That was pretty interesting.  Why don’t other languages do that?\n",
    "\n",
    "Cpp is a general language.  Have to make design decisions that trade average run time for good upper bound to make it consistent.  Cpp is concerned with everything.  C4 is concerned with getting a numeric answer in as fast a time as possible.  Only have to consider the average case.  Concepts from CS270.  Also, elsewhere concepts from CS151.  \n",
    "\n",
    "Find something to read on the train.\n",
    "\n",
    "Mallocing sucks, right?  Why not just malloc an object, and any object contained within that object also gets malloced.  Now you don’t have to think about mallocing a whole bunch of shit, just 1 thing.  Then freeing is easy too, and you’re less likely to leak memory.  The comp knows that if this object is malloced, everything else inside it is also malloced.\n",
    "\n",
    "What exactly is so bad about non-regular grammars?  Ok, so you want arbitrary nested parens, and a finite automata can’t ‘count’ the parens.  So what?  Just don’t use a finite automata.  Count the parens.  Why is counting the parens so bad?  Alternative solution:  Have rules up to, say, 10 parens.  If you go to 11 parens, have your compiler generate a new rule that goes up to 11 parens.  Why not generalize this?  You could have a ‘generator’ rule that can go arbitrarily high.  Obviously, you can’t literally have infinite rules, but when you encounter something that you don’t have a rule for, but you know you can generate a rule for it, go ahead and do so.  Obviously, the compiler would have to change itself permanently.  There’s no point in generating a rule if you’re just going to throw it away after the compilation is done.  So you get your compiler out of the box, it has rules up to 10 nested parens.  Then you compile something with 11 nested parens.  The compiler generates a new rule that goes up to 11 parens, and keeps it for the future.  Would that make the compiler a Quine, since it changes its own code?  Just had a thought about why this wouldn’t be that great.  Let’s say you compile, and the compiler finds something that it doesn’t have a rule for.  So it has to go through all of its rule generating schemes to figure out if it can make a new rule that will fit this syntax.  If it does, great, you just made compilation a little faster.  But what if your code is actually wrong?  Then the compiler still has to look through all of its generator rules, then realize none of them fit, then get back to you and tell you your code is wrong.  So compiling code that doesn’t work is slow to tell you that it doesn’t work.  And 99.999% of the time your code is not going to compile.  Another thought I had was that maybe when you compile, it doesn’t automatically generate new rules.  It only generates new rules when you call something like compile+generate.  But then you have this problem:  every time a user compiles and they get an error, they have to ask themselves “is this actually an error, or can I just generate a new rule?”  And its much simpler to just type compile+generate than to actually think about your code.  So even if you give users the option to leave out rule generation, they’re still going to use it every time rather than ask themselves “hmmm…is my code bad, or can I generate a new rule?”  I also remember that the gcc compiler uses a hand-written recursive descent parser.  Really slow, but easy to maintain, and of the 5 stages (flexing, parsing, …decorating?, optimization, code generation) the parser doesn’t take nearly as long as the other stuff.  Even if you got the parser down to 1/1000000 the time of what it used to be, that only shaves off like 5-10% of your total compile time.  Maybe rather than using rule generation to make parsing faster, maybe you could use it to make writing new rules easier?  Then again, a user can just as easily write new rules in terms of old rules.  Remember lambda calculus?  Everything in programming can be generated from it.  Don’t need a compiler to do it for you.\n",
    "\n",
    "\n",
    "Compiling takes a long time.  It’s possible to parallelize it, right?  If so, what about cloud compiling?  Supposedly a compiler can spend hours and hours fine-tuning a program if you really want to let it do that.  But now that we have cloud computing, we basically have unlimited parallelization power.  Perhaps what used to take hours and hours will now take just a few minutes.  Give compiling the same treatment as machine learning.  I guess this is why everyone says cloud computing is so great.  You can apply it to just about anything.  It basically removes constraints like ‘how many cores do I have?’  Infinite.  ‘How large is my cache’?  Infinite.  I wonder if Amazon AWS has an easy way to let you parallelize calculations.  Look up ‘super compilers’.  From the 3 sentences I read off some guys blog, a super compiler is really dumb, but makes good optimizations because it just does a search over all possibilities rather than use any algorithms to optimize code.  What if you could do that?  ….What if you created a programming language that was easy to make optimizations to?  A programming language that you could easily compile with cloud computing for maximum performance?  The complaints of cpp are that it’s bloated.  They made Go, but that’s garbage collected.  Jonathan Blow is making Jai, but who knows how that will turn out?\n",
    "\n",
    "Loop unrolling:  why does the unrolling need to explicitly state each instruction?  If we unroll a loop into 100000 instructions, why must the compiler explicitly type out those 100000 instructions?  Why not just say instruction * 100000?  Now you get all the speed of unrolling without the bloat in file size.  I guess then you would need to tell the program counter not to move on, to just stay where it is.  Might cause problems, might not.  I don’t have enough experience to say.\n",
    "\n",
    "There are some ways that you can do ‘compile-time parallelization’.  How is this done?  It totally seems like something that a programmer shouldn’t have to worry about, since it’s ‘hardware specific’ or whatever the term is.  That start up ‘Big Stream’ was doing a whole bunch of big data compiler stuff.  Leo worked on Machine Learning Compiler stuff.\n",
    "\n",
    "There’s so many different levels of parallelism.  It just seems like something that could be done by a compiler.  They say that Go routines are really good for parallelism, but Go wasn’t designed with parallelism in mind explicitly.  I wonder what programming languages there are whose central idea is parallelism.  Rust?  But all of these languages still have you explicitly mention parallelism.  Would it be possible to do most of it implicitly?  But what exactly makes one language more ‘concurrent’ than another?  Can ‘easy concurrency’ be made a property of the grammar itself?  By the way, Rust seems interesting.  Was voted ‘most loved programming language’ on stack overflow in 2016, 2017, and 2018.  Also has an interesting not-garbage-collector thing.  I think I’ve mentioned this before, but Bigstream was modifying compilers to make them better for big data or something.  But for real, so many levels of parallelization.  SIMD and MIMD and bit level and instruction level and task level.  Multi-core and symmetric and distributed.  So so much.  The ‘automatic parallelization’ section of the wikipedia entry on parallel computing has some stuff to say about this.  Also don’t forget that if you make a language you would want to run it ‘on the cloud’ which has Fukken huge caches.  You would basically be working in an ‘ideal’ environment with unlimited cache or registers or whatever.  Rather than doing stuff on your local machine and pushing it to the cloud for the cloud to number crunch, then push the data back to you, you should just have the data on the cloud, and you ssh into the cloud to write code and stuff.  Then you don’t have to worry about the latency of piping huge data sets back and forth.  All you need to worry about is the data that shows up on the users screen.  C has all different kinds of optimization levels for different types of constraints.  Size constraints are the simplest that come to mind.  But there’s also more complicated constraints.  Just look at all the different tags you can have.  Given no hardware constraints, what would a fully optimized binary look like?  If you make a language, you should call it ‘data rapist’, like date rape.  I’m sorry, that was over the line.  Don’t call it that.  Data Defiler?  \n",
    "\n",
    "\n",
    "Most of our code is 3rd generation.  We don’t program in assembly anymore. So why do we need to convert C to assembly that references virtual addresses?  Why not convert it to C that directly references physical addresses?  Would that be faster?  Or maybe they already do that…\n",
    "\n",
    "Ugh, having to think about how to split up jobs in a parallel process thing is hard.  It would be simpler on my brain to just think of it as having infinite threads, and they all get processed.  Like virtual threads or something.  And those virtual threads get translated into real threads.\n",
    "\n",
    "Something on parallelism:  If we have a task that we can make parallel, the best way to do it is to parallelize at the ‘highest’ possible level that we can.  Lets say we have a big list of matrices.  Each line in our list is a row of matrices.  Lets say each row is 100 matrices, and there are 10000 rows.  You want to multiply all 100 matrices together for each row, so your task is to spit out a list of 10000 matrices.  We’ll also say you have 4 cores to do this with.  You could split it up in a number of ways.  You could have each core multiply 25 matrices together for each row, then do 4 more multiplications to get the final product.  This involves 3 splits for each row, so 30000 splits, and 3 merges for each row, so 30000 merges.  Or you could just split the rows into 4 chunks and have each core work on 2500 rows.  This is just 3 splits, and 3 merges.  Much better.  So when jobs >> workers, splitting up your workload at the highest level possible is probably the best idea.  But what about if we have the same number of workers as jobs, or workers >> jobs?  The current ‘fastest’ supercomputer in the world is Taihu Light, which has 10,649,600 total cores, or workers.  Then how would you split up the work?\n",
    "\n",
    "Another separate thought on parallel processing:  splitting up tasks takes time, any you want all of your parallel processes to finish at the same time, so maybe you should give your earlier processes a little more work, since they start earlier.  Would that make much of a difference?  Is that part of load balancing?\n",
    "\n",
    "Consider taking CS152, Computer Architecture.  Good for ASPIRE, which has a super computer.  They do a lot of super computer research.\n",
    "\n",
    "MITOpenCourseware parallel computing with Julia.\n",
    "\n",
    "Learn about google cloud, and how it works.  Compare and contrast running your code on the cloud vs running your code on your laptop.\n",
    "\n",
    "Cloud runs just like a regular computer.  You buy cpu’s that you can use.  Then you run stuff on those cpu’s.  There’s stuff like burst, load balancing, using TPUs, etc.  A lot of them even tell you what CPU’s they use.  So you just need to figure out how to optimize for that specific CPU, and how much latency there is between those CPU’s.\n",
    "\n",
    "———————\n",
    "If poly makes C++ perform 100x better on big data sets, how come we’re not hearing about it?  How come libraries like numpy aren’t suddenly 100x faster?  Isn’t Python compiled into C++?\n",
    "\n",
    "Maybe it’s a separate project because it’s so hard to merge with the regular optimizations present in the normal optimizer.  What if you made a compiler thing that only worked on TPU’s?  Since this thing works on CPU’s, perhaps its optimizations clash with the more classical optimizations?\n",
    "\n",
    "Make your own toy language and compare its runtime with that of the NASA study.  Optimize performance with the constraint of simple pythonic syntax.\n",
    "\n",
    "OR, much less work, you could hack on a compiler that already exists.  Like LLVM.  I mean, what is a programming language?  It’s the front end to a machine language.  Creating a programming language can be done later.  Now, it’s important to understand what you can do to actually make the spirit of your idea real, and that means making code faster.\n",
    "\n",
    "Basic premise of your compiler is this:  hog resources.  Parallelize as much as possible.  Distribute as much as possible.  Use the GPU/TPU whenever possible.\n",
    "\n",
    "You want ‘automatic’ parallelization.  What is automatic parallelization?  If I use numpy, is that ‘automatic’?  Why is numpy a library and not a built in thing?  I think ‘automatic’ parallelization means it’s parallelized without you having to use any libraries or fancy classes.  Just primitives.  Maybe classes too.  But still not sure.  The program must be written in a completely sequential manner.  Then the compiler automatically makes it parallel.  What does it mean for the program to be ‘sequential’?  Are matrices a special exception?  They are a concept that allows us to ‘parallelize’ something in our mind.  While it is explicit parallelization, it is simpler for us to conceptualize it in the form of a matrix than 1 by 1.  So matrices should be allowed to be in your code.  Things like writing pragma amp parallel or calling go func are definitely explicit parallelism that you shouldn’t need.  So you should compare based only on being able to use matrices, and perhaps setting compiler options.\n",
    "———————\n",
    "\n",
    "Email every professor you can, just like with your jobs.\n",
    "Read a paper, do something with it, email the writer of the paper.\n",
    "\n",
    "Remember that Leo made compiler optimizations for Apple with regards to machine learning.  Perhaps you could do the same, but for the public domain.\n",
    "\n",
    "Ok, should clarify what a data manipulator type of person would get out of this language.  They already have numpy for parallelism, but they don’t have anything simple for distributed systems.  At least I don’t think they do.\n",
    "Separation of concerns.  Parallelism has nothing to do with the logic of the program.  It has to do with hardware.  Programmers should not have to worry about it.  It is possible that separating parallelism from the algorithms themselves \n",
    "Matrices are a fundamental part of data.  One dimension to specify all aspects of a single object, another dimension to specify all objects.  If those objects can come together to form the ‘aspects’ of an even bigger object, you don’t need another dimension, you just need a matrix transformation.  Matrices and matrix operations should be defined in a primitive way, not through classes.\n",
    "About the data manipulator people again.  They are a group of people that basically just use matrices.  This is a very small subset of the possible algorithms a computer scientist could use.  If this is the case, a hand-optimized library for matrix manipulation will always beat one optimized by a compiler.  So trying to improve a data manipulators output by optimizing their matrix library is not going to work.\n",
    "More generally, a scientific field that only needs to rely on a very small set of algorithms will not need a compiler to optimize and parallelize their algorithms because they have already hand-optimized those algorithms.  I don’t know anything about finance, but I’m pretty sure they just do matrix multiplication and statistics.  Very small, easily hand-optimizable subset.  So the people that would benefit the most from this kind of language are people who use a wide variety of algorithms.  Or more accurately they use a giant bag of tricks.  Machine learning.\n",
    "Something else to consider, perhaps more people need to rewrite algorithms than you think.  Remember CS170, where we had the modified knapsack algorithm?  That required us to write our own code.  No easy way to call a library on that.\n",
    "Also don’t forget the ‘many core’ processors that will probably become a thing.  Right now its easy because there’s only like 4 cores on a chip, but if the number of cores exceeds the number of jobs it becomes a much more involved optimization task.\n",
    "\n",
    "“Everyone should learn how to code”.  This does not mean learning how to code like in CS61A, where you learn\n",
    "\n",
    "Everyone coding actually means everyone scripting.  Need to think of a name for these people.  Scripters?  Data manipulators?  \n",
    "I agree with this, everyone should learn how to script and manipulate data\n",
    "We can see some of that now with BIDS.  People who are psychology students doing stuff with data.\n",
    "But right now, everyone scripts on their computers.\n",
    "This is slow.  Big data sets can take hours to run.\n",
    "In the future, they will script on the cloud.\n",
    "In much the same way that Jupyter notebooks made python more accessible to these people, cloud computing will be made more accessible.  People will write code on the cloud in the same way that they write code on their local machines.  If you can write and run code in a browser in the exact same way you write it on a local machine, the same can be applied to the cloud.\n",
    "The cloud will be faster.  It will also be more expensive.\n",
    "Scripters write code that is not well optimized.  The better optimized the code is, the less money it will cost them to run code on the cloud.\n",
    "There should be a language that is designed for scripters who use the cloud.  They write their simple code, and the compiler for that code optimizes it for performance on the cloud.  So the scripter doesn’t have to learn anything new, they get easy access to the cloud, and it’s much faster.\n",
    "\n",
    "My general idea of this language is that a scripter simply goes to their browser, logs into the X cloud account, and gets an environment much like a Jupyter notebook or repl.it.  They can type code into their browser, hit run, and it runs on the cloud.\n",
    "\n",
    "This language will not be garbage collected, and will be statically typed.  If you think static typing and no garbage collection is too hard for a scripter, read what I wrote below about python and the real reason why it’s easy to learn.  Reading from files should also be extremely fast, intuitive, and easy to code.  Also might want to put the thing about matrices down here too.\n",
    "\n",
    "Would cloud companies necessarily want their customers running code that is as highly optimized as possible?  Wouldn’t less efficiency mean you get more money?  But if it’s more efficient, then customers are more attracted to your cloud than competitors.\n",
    "\n",
    "A takeaway for making any language:  Python is not easier than Java and C.  Python is less intimidating than Java and C.  This is because it is less verbose, has less boilerplate code, and has more fancy keywords that make many code constructs more intuitive for new readers.  If you start a programmer out using C or Java, they will see a bunch of include statements, a main method, all in a class if it’s Java.  This is boilerplate code.  On top of that, you need to download some crazy IDE for Java, or a compiler for C, and to actually make your program run you have to go through a bunch of steps like typing some commands in your terminal that make no sense.  \n",
    "New coders are told to just not think about any of this and just do it.  When new coders are told this, they immediately shut down.  Internally, instinctually, it’s just like when they were in school.  Do not attempt to understand this, just memorize it.  But python doesn’t have any of that.  No boiler plate, no nothing.  In repl, just ‘print(“Hello world”)’ then hit run and you’re done.  Nothing crazy.  Everything is immediately straightforward.  And actually, Go kind of does the same thing.  Anyway, newbies fair better with python not because it’s weakly typed, or because it’s interpreted, it’s because its syntax is simple and clear.  CS164 proved that you can write Python code that is just as fast as C++, because it IS C++.  The takeaway from this is that it is entirely possible to write a ’low-level’ language like Java or C++ that is not intimidating to newcomers.  The reason is because newcomers like Python for its syntax, and syntax can easily be lifted and reused.  \n",
    "\n",
    "When doing machine learning, why are we the ones separating pictures into learn and test sets?   I feel like it would be easier if you just give the program all the pictures and labels, then the machine decides what goes into the learning and testing sets.  The idea would be that you want the highest possible accuracy with the smallest possible learning set.  This seems like it would take exponentially more time, though.  But you want your algorithm to learn the ‘essence’ of whatever you give it.  The smaller the learning set, the more you have condensed the essence of the thing it’s trying to learn.\n",
    "\n",
    "Notes from PhD Grind———————————————————————————\n",
    "Page ??? (The entirety of year 1):  What you might end up like if you pursue a Ph.D\n",
    "\n",
    "Page 22:  Send cold emails.\n",
    "\n",
    "Whats the problem?\n",
    "What’s my proposed solution?\n",
    "What compelling experiments can I run to demonstrate the effectiveness of my solution?\n",
    "\n",
    "Page 23:\n",
    "\n",
    "Professors are motivated by having their names appear on published papers, and computer science conference papers usually need strong experiments to get accepted for publication. Thus, it’s crucial to think about experiment design at project inception time.\n",
    "\n",
    "Page 30:\n",
    "\n",
    "I cannot re-emphasize this point enough times: Properly calibrating your pitch to the academic sub-community you’re targeting is crucial for getting a paper accepted.\n",
    "\n",
    "Page 35-36:\n",
    "\n",
    "Make sure you and your advisor actually have similar goals in mind.\n",
    "\n",
    "Page 37-38:\n",
    "\n",
    "The horror.  Years and years for no results.  Also, remember that this is Stanford.  The place you want to go.  Also note that he’s making ‘incremental’ improvements to the already existing Klee stuff.  That’s basically what you want to do.  Just assemble a whole bunch of stuff that already exists.  You’re not really doing ground-breaking work, just implementing other people’s work.  Potentially with modifications, but still.  You can’t make a paper out of this.  Well, maybe you could, but it would be difficult.  And you would be on a timer.  And you would have to capitulate to the professor.  Ugh.  There’s no time for that.  You don’t need to publish a paper, you need to publish a language.  But wasn’t Julia someone’s research project?  Hm… doesn’t seem like it.  But you want control.  You need to be able to make all of the decisions, right now.  What you want to make is a product.  It’s not truly an exploration of new ideas, it’s a synthesis of ideas into something real.  And do those people go to grad school?  No, they just make it.  You can’t be wasting your time with some feet-on-desk professors half-assed ideas.\n",
    "\n",
    "Page 43:\n",
    "Marketing buzzwords.  Required to publish papers.\n",
    "\n",
    "Page 47:\n",
    "Online submission forms are black holes. Anything is better than blindly submitting online.\n",
    "\n",
    "53:  Search the web for related work.  Very important.\n",
    "\n",
    "58:  It turns out that ignoring your instructors and doing whatever you want is what makes you the most productive.  As you already knew.  “Those three years—my latter half of grad school—were the most creative and productive of my life thus far, in stark contrast to my meandering first half of grad school”\n",
    "\n",
    "63:  If you are a computer science researcher, your work will never be directly used in practice.  You simply don’t have the time to get things up to the production-level standards required for people to actually want to use your thing.  “To convince someone to try IncPy, I would need to guarantee that it works better than regular Python in all possible scenarios.”  Well, that’s your language, right?  All you have to do is put colons in front of all of your variable assignments, then instead of doing python <myfile>, you do sanic <myfile>.  Of course, gross oversimplification, but still.\n",
    "\n",
    "65:  Dependency conflicts are annoying.  The more integrated your stuff is with code that already exists, the more screwed you are.  What kinds of things will you have to depend on?\n",
    "\n",
    "69:  Your scholarships are on a timeline so you’re on your own if you don’t publish a bunch of papers right away.  So they make you waste your time with useless classes and bad research, then tell you that you can’t graduate unless you shit out a bunch of papers.\n",
    "\n",
    "70:  This incPy thing turned out to be one of the components of a jupyter notebook, in a way.  You run some code, you see the output, you can refer to that same code later.  This makes me think that your language needs something like that.  That might be difficult considering your language is compiled.  Or would it be difficult?  It looks like jupyter works with Go, so it shouldn’t be too bad.\n",
    "\n",
    "73:  Jupyter isn’t just really convenient, in a lot of ways it’s essential.  You need to be able to reproduce your research, not just present it.  How can you run programs really really fast when you’re in a Jupyter notebook?\n",
    "\n",
    "74:  ‘Why not make it a general application?’  Because the larger your audience is, the less you will be able to deliver to them.\n",
    "\n",
    "74-75:  ‘Why hadn’t anyone thought of this yet?’  He says he googles it and doesn’t get much back, but this seems like Containers to me.  It’s just containers.  \n",
    "\n",
    "76:  “I’ll always miss those purer times. In my current job, there’s no way I can block off three weeks just to code non-stop!”  HMMMMMM….  Maybe get a different job?  Well, he’s probably happy where he is, but you wouldn’t be.\n",
    "77:  “This is a very important point. It’s not our job as academics to ship polished products; that’s the role of companies.”  You want to ship a polished product.\n",
    "\n",
    "80-81:  He got an offer to just work on his open source project at Google.  Just get paid money to do what he had been previously doing.\n",
    "\n",
    "87:  This isn’t the first place this is mentioned, but “And without motivated students to grind through the tough manual labor, it’s impossible to get respectable publications.”  Your thing will consist of a lot of tough manual labor.  \n",
    "\n",
    "88:  Creative freedom is not to be found anywhere.  Freedom to simply work on what interests you is also not to be found anywhere.\n",
    "\n",
    "90:  Living arrangement matters for your productivity.\n",
    "\n",
    "92-93:  An example of the constant state you want to achieve.  Also note that this guy made that one environment diagram program that is useful.  Getting used to new things is a difficult, frustrating process.  You have to push through that to get to the good part.  Of particular importance:  “After years of grinding on uncertain and failed projects earlier in grad school, I now felt invigorated working intensely towards a target that I knew I could feasibly achieve.”  You’re motivated by this compiler thing because it seems so obvious, so simply achievable to you.  Maybe not an easy task, but definitely a task that is possible.\n",
    "\n",
    "100:  Produce results.  Not just for other, but for yourself, in order to stay motivated.  Also read the rest of the lessons, they seem very insightful.  Especially point 20.  Sad to say that effort alone isn’t enough.  You need to apply the right kind of effort.\n",
    "\n",
    "Last Page:  He says that it was fulfilling, etc, etc.  But he makes a compelling argument for you not to pursue a graduate degree.  Working towards a graduate degree will force you to work hard.  Do you need someone to force you to work hard?  No.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
